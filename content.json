{"pages":[{"title":"about","text":"自我介绍： 某大厂高级开发工程师、CSDN博客专家、匠心Java公众号运营人、图数据库爱好者； 联系我邮件：1591992570@qq.com微信：yangyangisme6688 公众号： 所有文章首发在公众号：【 匠心Java】 公众号分享工作中涉及到的技术知识，主要分享数据库相关和Java技术干货（JVM+并发+全链路优化）；涉及计算机网络、数据结构与算法、linux等编程知识 微信扫描下述二维码关注；或微信搜索“匠心Java”关注，期待与大家的交流 其他： csdn-洋仔聊编程：https://liyangyang.blog.csdn.net/ github-yoylee：https://github.com/yoylee 公司内推： 待补充…","link":"/about/index.html"},{"title":"分类","text":"","link":"/categories/index.html"},{"title":"标签","text":"","link":"/tags/index.html"}],"posts":[{"title":"MySql-正在筹备中...","text":"文章正在筹备中，敬请期待！","link":"/article/MySql-%E6%AD%A3%E5%9C%A8%E7%AD%B9%E5%A4%87%E4%B8%AD.html"},{"title":"Spring系列-正在筹备中...","text":"文章正在筹备中，敬请期待！","link":"/article/Spring%E7%B3%BB%E5%88%97-%E6%AD%A3%E5%9C%A8%E7%AD%B9%E5%A4%87%E4%B8%AD.html"},{"title":"Git提交错误，回退的三种方式！","text":"你知道Git版本控制系统中都有哪些“后悔药”吗？ 本文通过案例讲解git reset 、 git revert 、 git checkout在版本控制中的作用； 场景小明同学作为新人加入到一个新的项目组中做开发，在项目的迭代开发中，小明勤勤恳恳的写代码，直到有一次… 小明：完了，完蛋了 洋仔：怎么了，一惊一乍的 小明：我把错误代码用git commit后还push到线上代码库了！ 这可怎么办！ 洋仔：莫慌，git有“后悔药”！ 洋仔：容我给你慢慢道来。 但是我们要先知道一些git的基础知识，你才能更好的理解git命令的作用 预备知识git将项目的存储分为4部分，每部分有自己作用， 见下图: Workspace：工作区（当前用户操作修改的区域） Index / Stage：暂存区 （add后的区域） Repository：仓库区或本地仓库（commit后的区域） Remote：远程仓库（push后的区域） 整体过程可以简述为： 工作区–&gt;add–&gt;暂存区–&gt;commit–&gt;本地仓库区–&gt;push–&gt;远程仓库区 远程仓库区–&gt;fetch–&gt;使用refs\\remotes下对应分支文件记录远程分支末端commit_id 和 本地仓库区 –&gt;merge–&gt;工作区 远程仓库区–&gt;pull–&gt;使用refs\\remotes下对应分支文件记录远程分支末端commit_id and 本地仓库区 and 工作区 具体的git的组成部分和概念命令，请移步下述两个博客： Git技术干货！工作中Git的使用实践和常用命令合集！ Git - 使用git不知道内部实现机制怎么行 假设项目存在这么一个提交记录： 1234567891011121314151617181920$ git logcommit commit_id4 (HEAD -&gt; master)Author: testDate: Thu Aug 20 16:28:45 2020 +0800 第三次修改README文件commit commit_id3 (HEAD -&gt; master)Author: testDate: Thu Aug 20 16:28:45 2020 +0800 第二次修改README文件commit commit_id2Author: testDate: Thu Aug 20 16:28:19 2020 +0800 第一次修改README文件commit commit_id1Author: testDate: Thu Aug 20 16:26:59 2020 +080 初始化项目 提交顺序为：commit_id1 –&gt; commit_id2 –&gt; commit_id3 –&gt; commit_id4 注意：在git中每次的commit都有一个commit id唯一标识当前的提交！ 下面，我们先来解决小明的这个问题，使用git reset即可完美解决~ 问题解决 洋仔：小明，你的这个就可以用git reset 这个命令来完美的搞定，下面我们看一下如何解决 1、获取当前提交的commit id 命令：git log 获取到当前项目分支下的所有commit记录； 假设上述小明提交错误的commit id为commit id：commit_id4这一次提交； 他的上一次提交就是commit id：commit_id3 ，我们要将修改回滚到commit_id3的时刻！ 小明：我想要把我刚才 commit的修改保留下来，我修改的代码不能给我删除掉呀！ 洋仔：没问题 2、将某个commit id前的commit清除，并保留修改的代码 命令：git reset &lt;commit_id&gt; 当前场景下就是：git reset commit_id3 将指定commit_id后的所有提交，都去除，并保留修改的代码在本地的区域，也就是Workspace中 小明：啊哈，这样的话我就可以把错误代码修改后再提交了； 但是我已经push到线上仓库的数据怎么办呢？ 洋仔：别急，有办法~ 3、修改代码完成后，将修改好的代码add到暂存区，并提交到本地仓库中 命令：git add &lt;file_name&gt; and git commit 当前场景下：git add . and git commit 将最新修改后的代码commit 则提交后的提交记录假设如下： 可以看到，我们错误提交的commit_id4提交记录消失，取而代之的是我们更新代码后提交的记录commit_id5； 这样就完成了本地的代码修改和更新 1234567891011121314151617181920$ git logcommit commit_id5 (HEAD -&gt; master)Author: testDate: Thu Aug 20 16:28:45 2020 +0800 第三次修改README文件-更新错误后提交commit commit_id3 (HEAD -&gt; master)Author: testDate: Thu Aug 20 16:28:45 2020 +0800 第二次修改README文件commit commit_id2Author: testDate: Thu Aug 20 16:28:19 2020 +0800 第一次修改README文件commit commit_id1Author: testDate: Thu Aug 20 16:26:59 2020 +080 初始化项目 4、将本地修改同步到远程仓库 命令：git push origin HEAD --force 将本地修改强行同步到远程仓库，使得远程仓库和本地仓库保持一致！ 整体流程如下： 123456git loggit reset commit_id3修改代码git add .git commit -m '第三次修改README文件-更新错误后提交'git push origin HEAD --force 洋仔：好了，小明，你的问题完美解决了 小明：哦吼，但是我还有一个问题： 如果我想要不保留回滚commit的修改，直接删除掉修改！该怎么处理呢？ 洋仔：简单~ 我们整体看一下 git reset 命令 后悔药-git reset在进行下面的讲解是，还是先假设有这么一个提交链： commit_id1 --&gt; commit_id2 --&gt; commit_id3 --&gt; commit_id4 git reset commit_id2： reset是将HEAD重新定位到commit_id2上，对于commit_id3 和 commit_id4 和本地当前的修改，对于不同的参数param，会有不同的处理； reset命令有三种处理模式： –soft：保留commit修改，将修改存储到index中；也就是说git add后的区域 –mixed：保留commit修改，将修改存储到本地工作区域中；也就是说git add前的区域 –hard：删除commit修改，慎用！ git reset –soft回滚commit_id前的所有提交，不删除修改： git reset --soft commit_id 重设head，不动index，所以效果是commit_id之后的commit修改全部在index中将id3 和 id4的修改放到index区（暂存区），也就是add后文件存放的区域，本地当前的修改保留 git reset –mixed回滚commit_id前的所有提交，不删除修改：git reset commit_id 等同于 git reset --mixed commit_id 与 下述的 git reset –hard commit_id效果不同 重设head 和 index，不重设work tree，效果就是commit_id之前的修改，全部在work tree中，为还未add的状态将id3 和 id4 的所有修改放到本地工作区中，本地当前的修改保留 git reset –hard回滚commit_id前的所有提交，将修改全部删除：git reset --hard commit_id 重设head、index、work tree，也就是说将当前项目的状态恢复到commit_id的状态，其余的全部删除（包含commit_id后的提交和本地还未提交的修改）慎用！！ 后悔药-git revert 小明：原来git reset这么强大呀！ 但是我这还有个问题： 如果想要只操作修改中间的一个commit，不对其他的commit产生影响； 也就是类似于我们只修改commit_id2，而对commit_id3 和 commit_id4无影响，该怎么处理呢？ 洋仔：（这么多问题，幸亏我懂，要不这次就丢大了。。） 简单！ git revert 命令！ 适用场景： 在项目开发中，突然发现在前几次的提交中，有一次提交中包含一个bug； 当然我们可以进行一个新的修改，然后再提交一次； 但是，不优雅哈哈； 我们可以直接重做有bug的commit~ 为什么不直接去再添加一个commit呢？ git revert是用于“反做”某一个版本，以达到撤销该版本的修改的目的。 比如，我们commit了三个版本（版本一、版本二、 版本三），突然发现版本二不行（如：有bug），想要撤销版本二，但又不想影响撤销版本三的提交，就可以用 git revert 命令来反做版本二，生成新的版本四，这个版本四里会保留版本三的东西，但撤销了版本二的东西； 在revert命令中常用的就两个： git revert -e ：重做指定commit的提交信息 git revert -n ：重做执行commit的代码修改 git revert -e重做commit_id的提交信息，生成为一个新的new_commit_idgit revert -e commit_id git revert -n重做commit_id的提交git revert -n commit_id将commit_id中修改，放到index区，我们可以对他重新做修改并重新提交 revert vs reset git revert是用一次新的commit来回滚之前的commit，此次提交之前的commit都会被保留不动； git reset是回到某次提交，提交及之前的commit都会被保留，但是此commit id之后的修改都会被删除或放回工作区等待下一次提交； 小明：还有这种操作，可以直接单独操作提交过程中的某一个commit！ 太棒了！ 后悔药-git checkout 小明：还有最后一个问题： 如果我在一次开发中，发现某个文件修改错误了，想要将文件恢复到刚pull代码时的状态怎么办呢？ 洋仔：简单！ 看git checkout解决这个问题！ 我们知道使用git checkout可以 git checkout &lt;branch_name&gt;切换分支 git checkout -b &lt;branch_bame&gt;创建分支等操作 它还有回滚指定文件的修改的功能 命令：git checkout -- &lt;file_name&gt; 上述语句的作用，就是将file_name的本地工作区的修改全部撤销，有两种情况： 如果file_name在commit后没有add过这个文件，则撤销到版本库中的状态 如果file_name在commit后add过这个文件，则撤销到暂存区的状态，也就是add后的状态 总之，就是让指定的文件回滚到最近的一次git add 或者 git commit时的状态！ 小明：太棒了，以后再也不怕提错代码了！ 总结上述，我们介绍了git reset \\ git revert \\ git checkout 在版本回滚、重做、撤销修改方面的作用； 可以应用到工作中对 误操作、不满足要求、不满足意愿的commit记录的重做和修改；","link":"/article/git-revert.html"},{"title":"中间件-正在筹备中...","text":"文章正在筹备中，敬请期待！","link":"/article/%E4%B8%AD%E9%97%B4%E4%BB%B6-%E6%AD%A3%E5%9C%A8%E7%AD%B9%E5%A4%87%E4%B8%AD.html"},{"title":"分布式-正在筹备中...","text":"文章正在筹备中，敬请期待！","link":"/article/%E5%88%86%E5%B8%83%E5%BC%8F-%E6%AD%A3%E5%9C%A8%E7%AD%B9%E5%A4%87%E4%B8%AD.html"},{"title":"图解JanusGraph系列-生成Hbase file离线批量导入方案","text":"源码分析github地址，包含图数据库序列化逻辑分析，下述介绍结合源码分析，应该可以减少大家对这种导入方式花费的时间; 大家好，我是洋仔，JanusGraph图解系列文章，实时更新~ 图数据库文章总目录： 整理所有图相关文章，请移步(超链)：图数据库系列-文章总目录 地址：https://liyangyang.blog.csdn.net/article/details/111031257 **源码分析相关可查看github（码文不易，求个star~）**： https://github.com/YYDreamer/janusgraph 版本：JanusGraph-0.5.2 转载文章请保留以下声明： 作者：洋仔聊编程 微信公众号：匠心Java 原文地址：https://liyangyang.blog.csdn.net/ 正文源码分析github地址，包含图数据库序列化逻辑分析，下述介绍结合源码分析，应该可以减少大家对这种导入方式花费的时间，github地址见上述； 本文涉及以下部分： 离线导入的理论前提 离线导入的流程 离线导入数据的验证 离线导入的现状 一：离线导入的前提1、数据相关 大规模导入的情况下使用，过亿级别 针对要离线导入的数据要做充分数据探查 导入时，尽量导入相关联的数据；例如：user节点 + phone节点 + user_login_phone_number边 在同一批次导入；从而尽量保证相关联数据在同一分区 2、技术点 id block的占用机制 ： 保证分布式id唯一性，具体可以看博客：图解Janusgraph系列-分布式id生成策略分析 schema创建完成 二：离线导入流程 项目初始化阶段 数据准备阶段 序列化阶段 验证阶段 项目初始化阶段 调用API接口获取JanusGraph的图实例和当前图实例的Transaction 调用janusgraph对应的API获取图库中所有的label节点的id，包括 节点label、边label、属性label、索引label； 并保存到内存中，等待使用 1QueryUtil.getVertices(consistentTx, BaseKey.SchemaName, typeName) 保存属性和对应的索引之间的关系 占用id block；需要多少，占用多少，block size可以自己配置（数据库+号段模式） 1234// partition 分区// idNamespace id生成器的命名空间，包含多种，此处默认是NORMAL_VERTEX// 超时时间public IDBlock getIDBlock(int partition, int idNamespace, Duration timeout); 上述，我们拿到了： schema所有label的节点值 属性和索引的对应关系 占用了一批id生成范围（避免离线导入和线上导入自动生成的id重复） 获取该批次要导入Vertex的总数量vertex_num； 将相应id block缓存到本地vertexBlocks中； 获取该批次要导入Vertex中的property总量 ( 所有属性的的总数量 + vertex节点总数量*2) = property_num； 获取满足数量的统一partition中的连续的block； 获取该批次要导入Edge的总数量num； 将相应block缓存到本地edgeBlocks中； 打包成jar包上传到指定服务器，作为id生成器 和 序列化工具包 生成需要的vertex id数量，备用 生成需要的edge id数量，备用 数据准备阶段 将节点数据和唯一vertex id整合； 对于节点，需要添加index值，index为从0开始； 获取导入批次数据的最大属性个数num+2，作为step 将vertex id和 index+=step值和原始数据整合（注意： 此处的index需要对一批次数据所有节点统一赋值！ 统一赋值index后可分批序列化；） 1234567891011{ &quot;index&quot;:16878090, &quot;label&quot;:&quot;user&quot;, &quot;propertyMap&quot;:{ &quot;create_time&quot;:&quot;2016-12-09 02:29:26&quot;, &quot;productid&quot;:&quot;2&quot;, &quot;real_name&quot;:&quot;张三&quot;, &quot;user_id&quot;:&quot;4882374234234&quot; }, &quot;vertexId&quot;:197596753968} 边数据和唯一edge id整合； 123456789{ &quot;edgeId&quot;:17514510, &quot;label&quot;:&quot;user_login_phone_number&quot;, &quot;propertyMap&quot;:{ &quot;productid&quot;:&quot;2&quot; }, &quot;sourceId&quot;:197596753968, &quot;targetId&quot;:40964208} 序列化阶段 主要是抽取出源码中的序列化逻辑； 在序列化逻辑中使用到了上述的：schema所有label的节点值、属性和索引的对应关系、唯一id等 序列化节点 12345678910jsonResult = {ArrayList@1231} size = 8 0 = &quot;{&quot;family&quot;:&quot;101&quot;,&quot;qualifier&quot;:&quot;2&quot;,&quot;rowKey&quot;:&quot;48 0 0 1 112 13 121 -128&quot;,&quot;timestamp&quot;:&quot;1595490235427&quot;,&quot;value&quot;:&quot;0 1 9 24 77 0 44 -122&quot;}&quot; 1 = &quot;{&quot;family&quot;:&quot;101&quot;,&quot;qualifier&quot;:&quot;36&quot;,&quot;rowKey&quot;:&quot;48 0 0 1 112 13 121 -128&quot;,&quot;timestamp&quot;:&quot;1595490235427&quot;,&quot;value&quot;:&quot;60 -115 9 24 77 0 48 -122 -1&quot;}&quot; 2 = &quot;{&quot;family&quot;:&quot;101&quot;,&quot;qualifier&quot;:&quot;80 -64&quot;,&quot;rowKey&quot;:&quot;48 0 0 1 112 13 121 -128&quot;,&quot;timestamp&quot;:&quot;1595490235427&quot;,&quot;value&quot;:&quot;-96 50 48 49 54 45 49 50 45 48 57 32 48 50 58 50 57 58 50 -74 9 24 77 0 52 -122&quot;}&quot; 3 = &quot;{&quot;family&quot;:&quot;101&quot;,&quot;qualifier&quot;:&quot;80 -96&quot;,&quot;rowKey&quot;:&quot;48 0 0 1 112 13 121 -128&quot;,&quot;timestamp&quot;:&quot;1595490235427&quot;,&quot;value&quot;:&quot;-96 -78 9 24 77 0 56 -122&quot;}&quot; 4 = &quot;{&quot;family&quot;:&quot;101&quot;,&quot;qualifier&quot;:&quot;81 -128&quot;,&quot;rowKey&quot;:&quot;48 0 0 1 112 13 121 -128&quot;,&quot;timestamp&quot;:&quot;1595490235427&quot;,&quot;value&quot;:&quot;-96 85 95 51 53 50 49 57 48 -71 9 24 77 0 60 -122&quot;}&quot; 5 = &quot;{&quot;family&quot;:&quot;101&quot;,&quot;qualifier&quot;:&quot;83 -64&quot;,&quot;rowKey&quot;:&quot;48 0 0 1 112 13 121 -128&quot;,&quot;timestamp&quot;:&quot;1595490235427&quot;,&quot;value&quot;:&quot;-88 -27 -68 -96 -23 -94 -106 9 24 77 0 64 -122&quot;}&quot; 6 = &quot;{&quot;family&quot;:&quot;103&quot;,&quot;qualifier&quot;:&quot;0&quot;,&quot;rowKey&quot;:&quot;111 -103 32 103 48 -119 -96 85 95 51 53 50 49 57 48 -71&quot;,&quot;timestamp&quot;:&quot;1595490235427&quot;,&quot;value&quot;:&quot;5 96 13 60 96 -80&quot;}&quot; 7 = &quot;{&quot;family&quot;:&quot;103&quot;,&quot;qualifier&quot;:&quot;0 5 96 13 60 96 -80&quot;,&quot;rowKey&quot;:&quot;-73 -16 125 -9 68 -119 -88 -27 -68 -96 -23 -94 -106&quot;,&quot;timestamp&quot;:&quot;1595490235427&quot;,&quot;value&quot;:&quot;5 96 13 60 96 -80&quot;}&quot; 101（边类型）：每个自定义属性、节点存在属性、节点和label的边 103（索引类型）：属性对应的索引 序列化边 123jsonResult = {ArrayList@1237} size = 2 0 = &quot;{&quot;family&quot;:&quot;101&quot;,&quot;qualifier&quot;:&quot;121 -64 -96 19 68 32 112&quot;,&quot;rowKey&quot;:&quot;112 0 0 0 0 0 0 -128&quot;,&quot;timestamp&quot;:&quot;1595490669289&quot;,&quot;value&quot;:&quot;8 45 0 -114 -64 -96 -78&quot;}&quot; 1 = &quot;{&quot;family&quot;:&quot;101&quot;,&quot;qualifier&quot;:&quot;121 -63 -128 32 112&quot;,&quot;rowKey&quot;:&quot;112 0 0 0 0 19 -120 -128&quot;,&quot;timestamp&quot;:&quot;1595490669289&quot;,&quot;value&quot;:&quot;8 45 0 -114 -64 -96 -78&quot;}&quot; 101（边类型）：出边、入边 103（索引类型）：属性对应的索引 将序列化后的数据根据rowkey，family，column 进行三级排序 为什么要进行排序？ sorted by id： rowkey sorted by type： family（101 103） sorted by sort key： 将序列化排序后的数据生成hfile 将生成的hfile导入到hbse中 离线导入验证阶段 节点、边数量验证 保留导入前的数量：节点、边的总数量；和导入数据同类型的边、节点总数量； 导入数据后查询：节点、边的总量；和导入数据同类型的边、节点总数量； 前后两次查询的相关数量和导入数据的数量，前后进行比对 节点、边内容抽样抽样验证 导入数据后，从导入的源数据中抽取部分数据，使用gremlin语句进行查询相对应的节点，并比对图中节点、边内容和源数据内容是否相同 节点和边的对应关系验证 抽样查询相应节点，并通过gremlin获取节点对应的边数据，和源数据进行比较 抽样查询相应的边，并通过gremlin获取边对应的节点数据，和源数据进行比较 节点、边索引抽样验证 使用节点对应的唯一索引查询数据，确保可以查询出对应数据并且数据内容和源数据相同 使用节点对应的唯一索引查询对应数量和内容，确保可以查询出对应数据并且数据内容和源数据相同 使用边对应的索引查询数据，确保可以查询出对应数据并且数据内容和源数据相同 三：离线导入的验证 前提： 两个schema一致的图 图中通过janusgraph api 或者 离线导入相同的数据 离线导入数据 对 线上图中已有数据是否有影响验证 通过janusgraph api导入数据 和 离线导入数据一致性验证 离线导入的节点的数量、内容、索引验证 离线导入的边的数量、内容、索引验证 离线导入的数据的 修改、删除验证 是否影响schema的相关操作验证 四：现状 只支持数据插入，不支持数据删除和修改 需要在图T+1更新时间段之外或者停止图线上的T+1数据更新之后，再离线的批量数据导入 当前不支持外部索引的添加 可以通过导入数据后，调用janusgraph的索引重建语句进行重建 获取es的索引的相关逻辑，直接调用es rest api插入","link":"/article/%E5%9B%BE%E8%A7%A3JanusGraph%E7%B3%BB%E5%88%97-%E7%94%9F%E6%88%90Hbase_file%E7%A6%BB%E7%BA%BF%E6%89%B9%E9%87%8F%E5%AF%BC%E5%85%A5%E6%96%B9%E6%A1%88.html"},{"title":"图解Janusgraph系列-图数据底层序列化源码分析（Data Serialize）","text":"JanusGraph的数据导入过程主要分为三阶段：prepare（准备）、serialize（序列化）、commit（提交）；不同阶段有不同的作用 大家好，我是洋仔，JanusGraph图解系列文章，实时更新~ 图数据库文章总目录： 整理所有图相关文章，请移步(超链)：图数据库系列-文章总目录 地址：https://liyangyang.blog.csdn.net/article/details/111031257 **源码分析相关可查看github（码文不易，求个star~）**： https://github.com/YYDreamer/janusgraph 下述流程高清大图地址：https://www.processon.com/view/link/5f471b2e7d9c086b9903b629 版本：JanusGraph-0.5.2 转载文章请保留以下声明： 作者：洋仔聊编程微信公众号：匠心Java原文地址：https://liyangyang.blog.csdn.net/ 正文JanusGraph的数据导入过程主要分为三阶段：prepare（准备）、serialize（序列化）、commit（提交）；不同阶段有不同的作用，如下： 下面我们分别从导入vertex节点和edge边两部分来分析写流程 建议依据源码同步看本文章，便于理解！ 一：vertex数据写流程下面vertex节点数据的导入， prepare阶段主要是依据当前给定的参数，组装出对应的vertex 或者 edge 对象；对象中包含对应的id、索引信息、属性信息和锁信息等； 过程中包含以下几种作用： 默认添加vertex exist属性，值为true，标识当前节点是否存在 默认添加label edge边，标识当前的节点 或者 边是什么label 生成vertex、edge、property的全局分布式唯一id 自定义属性验证是否满足唯一性约束 主要流程如下图（建议依照源码一块查看，上述github地址已给出）： serialize阶段主要是对上述prepare阶段准备好的数据进行序列化为二进制数据，为存储二进制数据到backend storage做准备； 另外获取本地锁 + 分布式锁数据插入（此处只是将数据插入到Hbase，插入成功并不代表获取成功） 过程中包含以下几种作用： 序列化所有relation数据并存储，包含属性、label edge、normal edge 获取属性对应index需要更新的数据，并序列化存储； 包含组合索引和mixed index的处理 获取基于图实例的本地锁 获取了本地锁的前提前，获取edge lock 和 index lock分布式锁（此处的获取锁只是将对应的KLV存储到Hbase中！存储成功并不代表获取锁成功，在commit阶段才会去检查是不是获取分布式锁成功！） 主要流程如下图： commit阶段主要是获取本地锁+分布式锁成功后，将对应序列化后的数据添加到对应的backend storage中；完成图数据插入过程！ 在此阶段才会对图库中的真实数据开始影响，才会涉及到事务的回滚机制； 过程中包含以下几种作用： 判断分布式锁的状态，获取成功则进行数据持久化；不成功则失败 持久化relation数据 持久化index数据，包含组合索引存储到第三方存储；mixed index存储到第三方索引库中 删除对应的本地锁 和 分布式锁的占用 主要流程如下图： 二：edge数据写流程针对于edge的写数据流程，整体的流程和vertex节点的数据写入相同，有几点不同，下面一一列出： 1、生成分布式唯一id的过程 导入Edge数据在生成edge的唯一id时，partition id的获取不再是随机获取，而是尝试获取边对应的out vertex的partition id； id的组成部分也不同，没有idPadding部分； 具体解释请看：《JanusGraph-分布式id生成策略》文章 2、在edge的导入中，没有同vertex数据导入，添加默认的节点是否存在属性和节点和节点对应label的边 3、获取edge对应的属性的index update时不同 在导入vertex数据时，将节点对应的属性作为relation存放在addRelation中，然后收集所有的属性relation循环获取index uodate；如下伪代码： 12345678for (InternalRelation add : Iterables.filter(addedRelations,filter)) { if (add.isProperty()) mutatedProperties.put(vertex,add); // 此处只操作属性类型的 mutations.put(vertex.longId(), add);}// 此处，收集节点对应属性对应的索引需要更新的数据、增加或删除节点时才有作用； 针对于插入edge的操作，不涉及此处for (InternalVertex v : mutatedProperties.keySet()) { indexUpdates.addAll(indexSerializer.getIndexUpdates(v,mutatedProperties.get(v)));} 而在edge数据导入中，只将edge这条边作为relation插入到addRelation中，所以无法获取属性relation，转而通过收集过程中，对每个edge对应的所有属性进行分别获取；如下伪代码： 123456for (InternalRelation add : Iterables.filter(addedRelations,filter)) { if (add.isProperty()) mutatedProperties.put(vertex,add); // 此处只操作属性类型的 mutations.put(vertex.longId(), add); // 获取边包含的属性；在节点插入时没有作用，插入边数据时，获取边上的属性对应的索引； 只有edge操作中包含边属性，并且包含索引！ indexUpdates.addAll(indexSerializer.getIndexUpdates(add));} 4、edge对应的relation数据，也就是当前插入的这个边，需要被序列化两次 一次是源节点+边关系，一次是目标节点+边关系（因为jansugraph是通过edge cut方式存储图数据的） 5、edge的数据插入过程中，edge的序列化组成部分不同于vertex的序列化组成部分； 不同点请看《Janusgraph-存储结构》文章 6、edge的数据插入中，edge的property和vertex的property组成不同！ edge中针对于sort key和signature key配置的属性，只将property value存储在对应位置。其他未被配置的属性值包含proeprty key label id + property value； 不同于vertex数据中的属性组成包含：proeprty key label id + property 唯一id +property value 三：源码分析源码分析已经push到github：https://github.com/YYDreamer/janusgraph 数据写入的流程源码过多，就不在文章中给出分析了，具体请看github中源码分析注释吧 四：应用基于数据序列化导入的源码博主将图数据的序列化逻辑抽取出来，生成一个工具包； 主要用于图数据的迁移和图数据库的初始化，适用于大数据量的导入，主要流程如下： 生成schema到图中 获取schema信息，缓存到内存中 调用api占用对应的id blocker，用于离线数据的分布式唯一id生成 调用抽取的序列化逻辑序列化节点和边数据 生成Hfile 将hfile导入到Hbase中 上述流程已经经过严格的验证并在生产环境中使用，具体之后会再出一篇文章介绍一下详细的设计与流程 五：总结对于JanusGraph图数据的写入，主要分为3部分： schema的创建 vertex节点数据的导入 edge边数据的导入 上述主要分析了vertex和edge的数据导入，大致流程相似；也分析了两部分导入数据的差异； 其中涉及的分布式唯一id的生成逻辑 和 锁机制获取的逻辑，请看《图解Janusgraph系列-Lock锁机制(本地锁+分布式锁)分析》和《图解Janusgraph系列-分布式id生成策略分析》两篇文章！ 针对于第三方索引的序列化存储逻辑，逻辑相对简单，此处没有给出，具体读者可以自主分析一下源码 码字不易，求个赞和star~","link":"/article/%E5%9B%BE%E8%A7%A3Janusgraph%E7%B3%BB%E5%88%97-%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%95%E5%B1%82%E5%BA%8F%E5%88%97%E5%8C%96%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%EF%BC%88Data_Serialize%EF%BC%89.html"},{"title":"图解Janusgraph系列-官方测试图：诸神之图（Graph of the gods）分析","text":"在Janusgraph中提供了一个用于测试的图，美名其曰“诸神之图”！英文名：Graph of the gods； 响当当的名字哈哈~~ 大家好，我是洋仔，JanusGraph图解系列文章，实时更新~ 图数据库文章总目录： 整理所有图相关文章，请移步(超链)：图数据库系列-文章总目录 地址：https://liyangyang.blog.csdn.net/article/details/111031257 **源码分析相关可查看github（码文不易，求个star~）**： https://github.com/YYDreamer/janusgraph 下述流程高清大图地址：https://www.processon.com/view/link/5f471b2e7d9c086b9903b629 版本：JanusGraph-0.5.2 转载文章请保留以下声明： 作者：洋仔聊编程微信公众号：匠心Java原文地址：https://liyangyang.blog.csdn.net/ 诸神之图在Janusgraph中提供了一个用于测试的图，美名其曰“诸神之图”！英文名：Graph of the gods； 响当当的名字哈哈~~ 图如下图： 图中，对应的类型解释如下：| 符号类型 | 含义 || —————- | ———————————————- || 粗体key | 图索引键 || 加粗 加星号的key | 图唯一索引键 || 下划线key | 顶点为中心的索引键，vertex-centric index || 空心箭头edge | 不可重复边，两个节点之间最多只能有一个该类型边 || 实心箭头edge | 单向边，只能A–&gt;B，不可以B–&gt;A | 包含类型主要包含6种节点类型： location：位置（sky：天空，sea：海，tartarus：塔耳塔洛斯） titan：巨人（saturn：罗马神话中的农神） god：神（jupiter，neptune，pluto） demigod：半神（hercules） human：人类（alcmene） monster：怪物（nemean，hydra，cerberus） 主要包含6中边类型： father：父亲 mother：母亲 brother：兄弟 battled：战斗 lives：生活在 pet：宠物 源码分析源码在GraphOfTheGodsFactory类的下述方法中： 1public static void load(final JanusGraph graph, String mixedIndexName, boolean uniqueNameCompositeIndex) {} 我们接下来分析一下，创建语句源码，具体分析一下图中的组成： 详细解释，已经在代码中注释 1、获取图管理对象实例12345678if (graph instanceof StandardJanusGraph) { Preconditions.checkState(mixedIndexNullOrExists((StandardJanusGraph)graph, mixedIndexName), ERR_NO_INDEXING_BACKEND, mixedIndexName);}// Create SchemaJanusGraphManagement management = graph.openManagement(); 2、创建属性和对应索引1234567891011121314151617181920212223// ===创建name属性； String、唯一CompositeIndex、锁机制保证name的强一致性final PropertyKey name = management.makePropertyKey(&quot;name&quot;).dataType(String.class).make();JanusGraphManagement.IndexBuilder nameIndexBuilder = management.buildIndex(&quot;name&quot;, Vertex.class).addKey(name);if (uniqueNameCompositeIndex) nameIndexBuilder.unique();JanusGraphIndex nameIndex = nameIndexBuilder.buildCompositeIndex();// 此处的LOCK，在name索引上添加了LOCK标识，标识这在并发修改相同的name属性时，必须通过锁机制（本地锁+分布式锁）保证并发修改；management.setConsistency(nameIndex, ConsistencyModifier.LOCK);// ===创建age属性；Integer、mixed indexfinal PropertyKey age = management.makePropertyKey(&quot;age&quot;).dataType(Integer.class).make();if (null != mixedIndexName) management.buildIndex(&quot;vertices&quot;, Vertex.class).addKey(age).buildMixedIndex(mixedIndexName);// ===创建time属性final PropertyKey time = management.makePropertyKey(&quot;time&quot;).dataType(Integer.class).make();// ===创建reason属性final PropertyKey reason = management.makePropertyKey(&quot;reason&quot;).dataType(String.class).make();// ===创建place属性final PropertyKey place = management.makePropertyKey(&quot;place&quot;).dataType(Geoshape.class).make();// 为reason 和 place属性创建mixed index索引edgesif (null != mixedIndexName) management.buildIndex(&quot;edges&quot;, Edge.class).addKey(reason).addKey(place).buildMixedIndex(mixedIndexName); 3、创建edge类型和对应索引12345678910111213141516// 创建边类型：father， many to onemanagement.makeEdgeLabel(&quot;father&quot;).multiplicity(Multiplicity.MANY2ONE).make();// 创建边类型：mother， many to onemanagement.makeEdgeLabel(&quot;mother&quot;).multiplicity(Multiplicity.MANY2ONE).make();// 创建边类型：battled， 签名密匙为time:争斗次数，EdgeLabel battled = management.makeEdgeLabel(&quot;battled&quot;).signature(time).make();// 为battled边创建一个以顶点为中心的 中心索引（vertex-centric index），索引属性time； 双向索引，可以从 神-&gt;怪物 也可以 怪物-&gt;神// 将查询节点对应的 battled 边时，可以使用这个vertex-centric索引，索引属性为 time；// vertex-centric index为了解决大节点问题，一个节点存在过多的边！management.buildEdgeIndex(battled, &quot;battlesByTime&quot;, Direction.BOTH, Order.desc, time);// 创建边类型：lives，签名密匙为reasonmanagement.makeEdgeLabel(&quot;lives&quot;).signature(reason).make();// 创建边类型：petmanagement.makeEdgeLabel(&quot;pet&quot;).make();// 创建边类型：brothermanagement.makeEdgeLabel(&quot;brother&quot;).make(); 4、创建vertex类型1234567// 创建节点labelmanagement.makeVertexLabel(&quot;titan&quot;).make();management.makeVertexLabel(&quot;location&quot;).make();management.makeVertexLabel(&quot;god&quot;).make();management.makeVertexLabel(&quot;demigod&quot;).make();management.makeVertexLabel(&quot;human&quot;).make();management.makeVertexLabel(&quot;monster&quot;).make(); 5、提交创建的schema数据1management.commit(); 6、插入数据获取图事务对象： 1JanusGraphTransaction tx = graph.newTransaction(); 插入节点数据： 12345678910111213// 插入节点Vertex saturn = tx.addVertex(T.label, &quot;titan&quot;, &quot;name&quot;, &quot;saturn&quot;, &quot;age&quot;, 10000);Vertex sky = tx.addVertex(T.label, &quot;location&quot;, &quot;name&quot;, &quot;sky&quot;);Vertex sea = tx.addVertex(T.label, &quot;location&quot;, &quot;name&quot;, &quot;sea&quot;);Vertex jupiter = tx.addVertex(T.label, &quot;god&quot;, &quot;name&quot;, &quot;jupiter&quot;, &quot;age&quot;, 5000);Vertex neptune = tx.addVertex(T.label, &quot;god&quot;, &quot;name&quot;, &quot;neptune&quot;, &quot;age&quot;, 4500);Vertex hercules = tx.addVertex(T.label, &quot;demigod&quot;, &quot;name&quot;, &quot;hercules&quot;, &quot;age&quot;, 30);Vertex alcmene = tx.addVertex(T.label, &quot;human&quot;, &quot;name&quot;, &quot;alcmene&quot;, &quot;age&quot;, 45);Vertex pluto = tx.addVertex(T.label, &quot;god&quot;, &quot;name&quot;, &quot;pluto&quot;, &quot;age&quot;, 4000);Vertex nemean = tx.addVertex(T.label, &quot;monster&quot;, &quot;name&quot;, &quot;nemean&quot;);Vertex hydra = tx.addVertex(T.label, &quot;monster&quot;, &quot;name&quot;, &quot;hydra&quot;);Vertex cerberus = tx.addVertex(T.label, &quot;monster&quot;, &quot;name&quot;, &quot;cerberus&quot;);Vertex tartarus = tx.addVertex(T.label, &quot;location&quot;, &quot;name&quot;, &quot;tartarus&quot;); 插入边数据： 12345678910111213141516171819202122// 插入边数据jupiter.addEdge(&quot;father&quot;, saturn);jupiter.addEdge(&quot;lives&quot;, sky, &quot;reason&quot;, &quot;loves fresh breezes&quot;);jupiter.addEdge(&quot;brother&quot;, neptune);jupiter.addEdge(&quot;brother&quot;, pluto);neptune.addEdge(&quot;lives&quot;, sea).property(&quot;reason&quot;, &quot;loves waves&quot;);neptune.addEdge(&quot;brother&quot;, jupiter);neptune.addEdge(&quot;brother&quot;, pluto);hercules.addEdge(&quot;father&quot;, jupiter);hercules.addEdge(&quot;mother&quot;, alcmene);hercules.addEdge(&quot;battled&quot;, nemean, &quot;time&quot;, 1, &quot;place&quot;, Geoshape.point(38.1f, 23.7f));hercules.addEdge(&quot;battled&quot;, hydra, &quot;time&quot;, 2, &quot;place&quot;, Geoshape.point(37.7f, 23.9f));hercules.addEdge(&quot;battled&quot;, cerberus, &quot;time&quot;, 12, &quot;place&quot;, Geoshape.point(39f, 22f));pluto.addEdge(&quot;brother&quot;, jupiter);pluto.addEdge(&quot;brother&quot;, neptune);pluto.addEdge(&quot;lives&quot;, tartarus, &quot;reason&quot;, &quot;no fear of death&quot;);pluto.addEdge(&quot;pet&quot;, cerberus);cerberus.addEdge(&quot;lives&quot;, tartarus); 提交事务，持久化数据： 12// 提交事务，持久化提交的数据到磁盘tx.commit(); 码字不易，求个赞和star~","link":"/article/%E5%9B%BE%E8%A7%A3Janusgraph%E7%B3%BB%E5%88%97-%E5%AE%98%E6%96%B9%E6%B5%8B%E8%AF%95%E5%9B%BE%EF%BC%9A%E8%AF%B8%E7%A5%9E%E4%B9%8B%E5%9B%BE%EF%BC%88Graph_of_the_gods%EF%BC%89%E5%88%86%E6%9E%90.html"},{"title":"图解Janusgraph系列-查询图数据过程源码分析","text":"查询流程可以大致分为三部分：组装查询语句、优化查询语句、执行算子链 大家好，我是洋仔，JanusGraph图解系列文章，实时更新~ 图数据库文章总目录： 整理所有图相关文章，请移步(超链)：图数据库系列-文章总目录 地址：https://liyangyang.blog.csdn.net/article/details/111031257 **源码分析相关可查看github（求star~~）**： https://github.com/YYDreamer/janusgraph 下述流程高清大图地址：https://www.processon.com/view/link/5f471b2e7d9c086b9903b629 版本：JanusGraph-0.5.2 转载文章请保留以下声明： 作者：洋仔聊编程微信公众号：匠心Java原文地址：https://liyangyang.blog.csdn.net/ 一：查询场景1.1 图数据使用JanusGraph官方提供的测试图诸神之图来测试，如下图： 具体的诸神之图的创建分析，请看《JanusGraph-官方测试图：诸神之图分析》文章 1.2 查询语句查询语句如下： 1234567891011121314@Testpublic void searchTest(){ // 获取name为hercules节点 GraphTraversal&lt;Vertex, Vertex&gt; herculesVertex = graph.traversal().V().has(&quot;name&quot;, &quot;hercules&quot;); // hercules节点战斗（battled）过12次的怪兽（monster） GraphTraversal&lt;Vertex, Vertex&gt; monsterVertices = herculesVertex.outE().has(T.label, &quot;battled&quot;).dedup().has(&quot;time&quot;, &quot;12&quot;).inV(); // 获取monsterVertices节点对应的主人 GraphTraversal&lt;Vertex, Vertex&gt; plutoVertex = monsterVertices.inE(&quot;pet&quot;).outV().has(&quot;age&quot;, 4000); if (plutoVertex.hasNext()){ Vertex next = plutoVertex.next(); // 输出主人的姓名 System.out.println(next.property(&quot;name&quot;)); }} 执行结果： 1vp[name-&gt;pluto] 我们查询就是诸神之图的左下角部分，包含了 vertex的查找 edge的查找 property的过滤 signature key的过滤（time） composite index的使用（name） 第三方索引支持的mixed index使用（age） 二： 查询流程分析查询流程可以大致分为三部分：组装查询语句、优化查询语句、执行算子链 预备知识gremlin语句官网：http://kelvinlawrence.net/book/Gremlin-Graph-Guide.html 在gremlin语句中，查询语句的类型其中包含三种哦：初始语句、中间语句、最终语句（触发执行查询语句） 初始语句：代表开始gremlin的语句，类似于V()、E()两种 中间语句：不会触发查询语句执行的语句，类似于has()、outE()、inV()等，大部分都是中间语句 最终语句：也叫作触发语句，这些语句会触发查询语句的执行！ 访问图库并进行图数据查询；类似于hasNext()、toList()等语句 只有在执行到最终语句部分时，gremlin语句才会真正的执行！ 2.1 组装查询语句在Janusgrahp的查询中，首先做的就是组装查询对象； 根据我们写的Gremlin语句，将gremlin语句拆分成不同的算子；不同的语句组装成对应的算子步骤step对象保存，生成一个查询执行算子step链； 在组装查询语句时，所有的查询Gremlin语句，通常都是通过.V()或者.E()开头，在这两部分中会创建一个查询对象，用于存放之后的每一个查询step； 例如上述执行查询语句，当执行完成以下语句时，便是进行组装查询对象，不没有真正的去查询底层存储的数据！ 123456// 获取name为hercules节点 GraphTraversal&lt;Vertex, Vertex&gt; herculesVertex = graph.traversal().V().has(&quot;name&quot;, &quot;hercules&quot;); // hercules节点战斗（battled）过12次的怪兽（monster） GraphTraversal&lt;Vertex, Vertex&gt; monsterVertices = herculesVertex.outE().has(T.label, &quot;battled&quot;).dedup().has(&quot;time&quot;, &quot;12&quot;).inV(); // 获取monsterVertices节点对应的主人 GraphTraversal&lt;Vertex, Vertex&gt; plutoVertex = monsterVertices.inE(&quot;pet&quot;).outV().has(&quot;age&quot;, 4000); 执行完成后，查看组装后的查询对象： 其中包含：图实例对象、事务对象、查询步骤链等，我们看下最终的查询步骤链，在对象中steps的属性： 1234567891011steps = {ArrayList@5271} size = 10 0 = {GraphStep@5350} &quot;GraphStep(vertex,[])&quot; 1 = {HasStep@5351} &quot;HasStep([name.eq(hercules)])&quot; 2 = {VertexStep@5352} &quot;VertexStep(OUT,edge)&quot; 3 = {HasStep@5353} &quot;HasStep([~label.eq(battled)])&quot; 4 = {DedupGlobalStep@5354} &quot;DedupGlobalStep&quot; 5 = {HasStep@5355} &quot;HasStep([time.eq(12)])&quot; 6 = {EdgeVertexStep@5356} &quot;EdgeVertexStep(IN)&quot; 7 = {VertexStep@5357} &quot;VertexStep(IN,[pet],edge)&quot; 8 = {EdgeVertexStep@5358} &quot;EdgeVertexStep(OUT)&quot; 9 = {HasStep@5359} &quot;HasStep([age.eq(4000)])&quot; 上述的gremlin查询语句主要被分为10个算子step执行； 2.2 优化查询语句在Gremlin的查询优化中，存在由策略模式设计实现的19个执行策略类，包含： Connective Strategy：连接策略 Match Predicate Strategy：匹配谓词策略 Filter Ranking Strategy：过滤排名策略 Inline Filter Strategy：内联过滤策略 Incident To Adjacent Strategy：相邻策略事件 Adjacent To Incident Strategy：邻近事件策略 Early Limit Strategy：早期限制策略 Count Strategy：计数策略 Repeat Unroll Strategy：重复展开策略 Path Retraction Strategy：路径收回策略 Lazy Barrier Strategy：惰性屏障策略 Adjacent Vertex Has Id Optimizer Strategy：相邻顶点有Id优化器策略 Adjacent Vertex Is Optimizer Strategy：相邻顶点是优化器策略 Adjacent Vertex Filter Optimizer Strategy：相邻顶点过滤器优化器策略 JanusGraph Local Query Optimizer Strategy：JanusGraph局部查询优化器策略 JanusGraph Step Strategy：JanusGraph步骤策略 JanusGraphIo Registration Strategy：JanusGraphIo注册策略 Profile Strategy：配置文件策略 Standard Verification Strategy：标准验证策略 将上述的查询算子链循环执行所有的19个策略，针对不同的策略进行不同的处理； 我们主要收一下上述的Match Predicate Strategy、： Filter Ranking Strategy依据优先级调整算子链的算子执行顺序！ 采用while+for循环的方式，将所有的算子调整到对应的位置！ 12345678while (modified) { modified = false; final List&lt;Step&gt; steps = traversal.getSteps(); for (int i = 0; i &lt; steps.size() - 1; i++) { // do somthing // update modified！ }} 优先级如下，数字越大，优先级越低，执行的时机越靠后： 12345678910111213141516171819202122if (!(step instanceof FilterStep || step instanceof OrderGlobalStep)) return 0;else if (step instanceof IsStep || step instanceof ClassFilterStep) rank = 1;else if (step instanceof HasStep) rank = 2;else if (step instanceof WherePredicateStep &amp;&amp; ((WherePredicateStep) step).getLocalChildren().isEmpty()) rank = 3;else if (step instanceof TraversalFilterStep || step instanceof NotStep) rank = 4;else if (step instanceof WhereTraversalStep) rank = 5;else if (step instanceof OrStep) rank = 6;else if (step instanceof AndStep) rank = 7;else if (step instanceof WherePredicateStep) // has by()-modulation rank = 8;else if (step instanceof DedupGlobalStep) rank = 9;else if (step instanceof OrderGlobalStep) rank = 10; 调整过后的算子链： 1234567891011steps = {ArrayList@5271} size = 10 0 = {GraphStep@5350} &quot;GraphStep(vertex,[])&quot; 1 = {HasStep@5351} &quot;HasStep([name.eq(hercules)])&quot; 2 = {VertexStep@5352} &quot;VertexStep(OUT,edge)&quot; 3 = {HasStep@5353} &quot;HasStep([~label.eq(battled)])&quot; 4 = {HasStep@5355} &quot;HasStep([time.eq(12)])&quot; 5 = {DedupGlobalStep@5354} &quot;DedupGlobalStep&quot; 6 = {EdgeVertexStep@5356} &quot;EdgeVertexStep(IN)&quot; 7 = {VertexStep@5357} &quot;VertexStep(IN,[pet],edge)&quot; 8 = {EdgeVertexStep@5358} &quot;EdgeVertexStep(OUT)&quot; 9 = {HasStep@5359} &quot;HasStep([age.eq(4000)])&quot; Inline Filter Strategy将同作用域的多个算子内联为一个一个算子； 相同作用域下，相同算子的内联整合： 例如，下述语句的两个has会在第一部分的步骤生成两个has算子： 1has(T.label,&quot;battled&quot;).has(&quot;time&quot;, &quot;12&quot;) 两个has同时作用于V()在同一作用域，又是相同类型，所以在Inline Filter Strategy策略中，会将两个has内联为一个has算子； 相同作用域下，不同算子的内联整合： 包含好多种情况，也是举一个例子，如果一个has算子一个vertex算子如下： 1outE().has(label,&quot;battled&quot;) 满足以下三个条件： has算子的前置算子为vertex算子（也就是上述out()产生的算子类型） 前置的vertex算子返回的类型时edge类型，也就是outE、inE、bothE这三种 前置的vertex算子，也就是outE、inE、bothE这三种没有指定edge label 的前提下，has算子的内容为过滤Edge的label；则可以将上述的两种组合成一个vertex算子，可以用以下语句表示： 1outE().has(label,&quot;battled&quot;) ==内联为==&gt; outE(&quot;battled&quot;) 调整过后的算子链： 12345678910steps = {ArrayList@5271} size = 9 0 = {GraphStep@5350} &quot;GraphStep(vertex,[])&quot; 1 = {HasStep@5351} &quot;HasStep([name.eq(hercules)])&quot; 2 = {VertexStep@5463} &quot;VertexStep(OUT,[battled],edge)&quot; 3 = {HasStep@5355} &quot;HasStep([time.eq(12)])&quot; 4 = {DedupGlobalStep@5354} &quot;DedupGlobalStep&quot; 5 = {EdgeVertexStep@5356} &quot;EdgeVertexStep(IN)&quot; 6 = {VertexStep@5357} &quot;VertexStep(IN,[pet],edge)&quot; 7 = {EdgeVertexStep@5358} &quot;EdgeVertexStep(OUT)&quot; 8 = {HasStep@5359} &quot;HasStep([age.eq(4000)])&quot; Incident To Adjacent Strategy作用也是合并算子，但是合并的是vertex算子（outE、inE、bothE）和edge算子（outV、inV、bothV）； 举个例子，如下语句，包含一个vertex算子（inE）和一个edge算子(outV) 1inE(&quot;pet&quot;).outV() 这个策略的作用就是将这两个算子合成一个in(&quot;pet&quot;)算子，如下： 1inE(&quot;pet&quot;).outV() ==合并==&gt; in(&quot;pet&quot;) 调整后的算子链： 123456789steps = {ArrayList@5271} size = 8 0 = {GraphStep@5350} &quot;GraphStep(vertex,[])&quot; 1 = {HasStep@5351} &quot;HasStep([name.eq(hercules)])&quot; 2 = {VertexStep@5463} &quot;VertexStep(OUT,[battled],edge)&quot; 3 = {HasStep@5355} &quot;HasStep([time.eq(12)])&quot; 4 = {DedupGlobalStep@5354} &quot;DedupGlobalStep&quot; 5 = {EdgeVertexStep@5356} &quot;EdgeVertexStep(IN)&quot; 6 = {VertexStep@5514} &quot;VertexStep(IN,[pet],vertex)&quot; 7 = {HasStep@5359} &quot;HasStep([age.eq(4000)])&quot; JanusGraph Step Strategy获取GraphStep算子，也就是V()、E()等对应产生的算子； 将Gremlin的GraphStep算子转换为图库自身的JanusGraphStep算子对象； JanusGraphStep算子对象中包含查询图库获取数据的lambda语句；在下一部分执行查询中通过调用get()来进行图库数据查询！ 调整后的算子链： 12345678steps = {ArrayList@5291} size = 7 0 = {JanusGraphStep@5630} &quot;JanusGraphStep([],[name.eq(hercules)])&quot; 1 = {JanusGraphVertexStep@5610} &quot;JanusGraphVertexStep([time.eq(12)])&quot; 2 = {DedupGlobalStep@5341} &quot;DedupGlobalStep&quot; 3 = {EdgeVertexStep@5343} &quot;EdgeVertexStep(IN)&quot; 4 = {JanusGraphVertexStep@5611} &quot;JanusGraphVertexStep(IN,[pet],vertex)&quot; 5 = {NoOpBarrierStep@5513} &quot;NoOpBarrierStep(2500)&quot; 6 = {HasStep@5346} &quot;HasStep([age.eq(4000)])&quot; 其他策略共19种，每一种都有自己的作用；优化算子链、优化调整index的使用、count语句的优化、repeat多度路径查询的优化等等 此查询语句优化部分，对用户自定义的gremlin语句进行正确性验证和优化查询过程的算子链两个作用； 最终，执行到第优化后的算子链为： 12345678steps = {ArrayList@5271} size = 7 0 = {JanusGraphStep@6302} &quot;JanusGraphStep([],[name.eq(hercules)])&quot; 1 = {JanusGraphVertexStep@6124} &quot;JanusGraphVertexStep([time.eq(12)])&quot; 2 = {DedupGlobalStep@5354} &quot;DedupGlobalStep&quot; 3 = {EdgeVertexStep@5356} &quot;EdgeVertexStep(IN)&quot; 4 = {JanusGraphVertexStep@6125} &quot;JanusGraphVertexStep(IN,[pet],vertex)&quot; 5 = {NoOpBarrierStep@5857} &quot;NoOpBarrierStep(2500)&quot; 6 = {HasStep@5359} &quot;HasStep([age.eq(4000)])&quot; 扩展： Gremlin查询语句执行过程，把执行语句转为由多个step组成对应的算子链，经过优化策略优化调整转化为确定最终可执行算子链。 Gremlin语言执行过程： [D] ecoration-应用级的迭代逻辑上迭代策略 [O]ptimization在ThinkPop图架构级别上高效迭代策略 [P]rovider optimization 从系统，语言，驱动级别优化 [F]inalization 对以上策略做调整确定最终执行策略 [V]erification:迭代策略做迭代引擎的验证如下图所示： 2.3 执行算子链执行算子链的触发时机在最终语句时进行触发；也就是我们示例语句中下述语句的hasNext()： 1plutoVertex.hasNext() 对于hasNext的源码伪代码如下： 123456789101112131415161718@Overridepublic boolean hasNext() { if (null != this.nextEnd) return true; else { try { while (true) { this.nextEnd = this.processNextStart(); if (null != this.nextEnd.get() &amp;&amp; 0 != this.nextEnd.bulk()) return true; else this.nextEnd = null; } } catch (final NoSuchElementException e) { return false; } }} 对于数据执行调用get()触发lambda表达式内容； 针对不同的算子针对中间结果进行顺序遍历； 三：源码分析源码分析已经push到github：https://github.com/YYDreamer/janusgraph 四：总结整体流程如下：","link":"/article/%E5%9B%BE%E8%A7%A3Janusgraph%E7%B3%BB%E5%88%97-%E6%9F%A5%E8%AF%A2%E5%9B%BE%E6%95%B0%E6%8D%AE%E8%BF%87%E7%A8%8B%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90.html"},{"title":"图解图库JanusGraph系列-解惑图数据库！你知道什么是图数据库吗？","text":"为什么需要图数据库？ 大家好，我是洋仔，JanusGraph图解系列文章，实时更新~ 图数据库文章总目录： 整理所有图相关文章，请移步(超链)：图数据库系列-文章总目录 地址：https://liyangyang.blog.csdn.net/article/details/111031257 **源码分析相关可查看github（求star~~）**： https://github.com/YYDreamer/janusgraph 下述流程高清大图地址：https://www.processon.com/view/link/5f471b2e7d9c086b9903b629 版本：JanusGraph-0.5.2 转载文章请保留以下声明： 作者：洋仔聊编程微信公众号：匠心Java原文地址：https://liyangyang.blog.csdn.net/ 为什么需要图数据库？设想一个场景：在金融的反欺诈场景下，当一个用户小李 请求订单，我们可以设定一个规则： 获取该用户的身份证号、注册手机号、银行预留手机号、银行卡号、紧急联系人等信息 通过这些信息去关联包含这些信息的用户集合小王、小张、小天 通过对关联出的小王、小张、小天判断黑名单用户、逾期用户、授信拒绝等信息综合判定一个分数 然后根据这个分数对小李 判定是否授信通过； 为了更加有效果我们可以小王、小张、小天作为源用户列表再获取这些用户的2度关联用户小小、小大，获取一个综合评分，和一度关联的用户综合评分共同参考，来判定这个userA到底该不该授信通过，借钱给他，如下图： 基于上述场景，我们首先先考虑使用关系型数据库： 一个用户表存储用户详情，上述过程我们需要 从接口入参获取小李的各种信息 通过小李的各种信息去表中查询出对应数据 再根据查出的一度用户去表中查询二度用户，那如果要查多度呢，如果想要获取用户的其他信息呢，就要join，多表join、多次join想想就刺激~ 那么，基于图论的图数据库就诞生了，详细的我们下面再介绍，先基于将数据存储到图库中，用户做为节点、用户与用户之间的关系作为边、用户的其他属性作为节点的属性，类似于下图； 那么用图库该怎么查呢？我们就以一种图库查询语言gremlin来实现： 1g.V().has('user_name',&quot;小李&quot;).both(&quot;user_friend&quot;).both(&quot;user_friend&quot;).both(&quot;user_friend&quot;).bothV().has('sex','男') 一句话搞定，不用多次查询、图库帮你搞定~ 多度查询轻松拈来 ps ： 具体图库底层数如何存储、查询逻辑、图库架构等 欢迎关注我~ 后续系列文章会出~ 总结一下，图库在特定场景下的优点: 高性能：随着数据量的增多和关联深度的增加，传统关系型数据库受制于检索时需要多个表之间连接操作，数据写入时也需考虑外键约束，从而导致较大的额外开销，产生严重的性能问题。而图模型固有的数据索引结构，使得它的数据查询与分析速度更快。 灵活：图数据库有非常灵活的数据模型，使用者可以根据业务变化随时调整数据模型，比如任意添加或删除顶点、边，扩充或者缩小图模型这些都可以轻松实现，这种频繁的 Schema 更改在关系型数据库上不能到很好的支持。 敏捷：图数据库的图模型非常直观，支持测试驱动开发模式，每次构建时可进行功能测试和性能测试，符合当今最流行的敏捷开发需求，对于提高生产和交付效率也有一定帮助。 图数据库图形数据库是NoSQL数据库的一种类型，起源于欧拉理论和图理论，也可称为面向/基于图的数据库，对应的英文是Graph Database。 它应用图形理论存储实体之间的关系信息；图数据库的基本含义是以“图”这种数据结构做为逻辑结构存储和查询数据。 我们知道一个图包含节点和边，如下图： 在图数据库中图将实体表现为节点，实体与其他实体连接的方式表现为联系（边）。我们可以用这个通用的、富有表现力的结构来建模各种场景，从宇宙火箭的建造到道路系统，从食物的供应链及原产地追踪到人们的病历，甚至更多其他的场景。 例如，实体：类似于用户、用户的亲属等作为一个节点存在于图中，边：用户和用户亲属之间关联的关系，小李—&gt;小李的父亲，这两个节点之间的边可以设定为“用户父母”的边； 主流图数据库目前主流的图数据库有：Neo4j，Janusgraph，Dgraph，Giraph，TigerGraph等。 ps : 这里我们只看 database model专用支持graph类型的图库 受欢迎程度如下，时间是：2020-5月 简单介绍一下Neo4j 和 Janusgraph区别： Neo4j： Neo4J使用原生的图存储，以高度自由且规范的方式管理和存储数据。对比非原生图解决方案中，随着信息量的增加，使用面向对象的数据库存储数据库使数据操作变得越来越慢。 Neo4J可以以每秒一百万条的惊人速度提供结果，因为数据中的链接部分或实体在物理上是已经相互连接的。 Neo4J的另一个特点是ACID事务，它确保实时显示数据的合法性和准确性，这是企业级应用的重要特性。 单击不收费，集群收费，所以对于不想要花大价钱买的话，这个不推荐；如果不差钱，强烈推荐使用，社群活跃，服务稳定，功能强大 Janusgraph： 开源的分布式图数据库，采用第三方存储作为底层存储，如：HBase、Cassandra等 使用第三方框架支持全文匹配、范围匹配等，如Es等 集群节点可以线性扩展，以支持更大的图和更多的并发访问用户。 数据分布式存储，并且每一份数据都有多个副本，因此，有更好的计算性能和容错性。 原生集成Apache TinkerPop图技术栈，包括Gremlin graph query language、Gremlin graph server、Gremin applications。 免费开源，我们现在正在使用的就是这个 下面就以JanusGraph为例来初探图数据库的设计 JanusGraph可以看下官网上的解释： JanusGraph is a scalable graph database optimized for storing and querying graphs containing hundreds of billions of vertices and edges distributed across a multi-machine cluster. JanusGraph is a transactional database that can support thousands of concurrent users executing complex graph traversals in real time. JanusGraph是一个可扩展的图形数据库，专门用于存储和查询分析分布在多机集群中的数千亿个顶点和关系边的图形。 JanusGraph是一个事务数据库，可以支持数千个并发用户实时执行复杂的图遍历。 历史 JanusGraph是2016年12月27日从Titan fork出来的一个分支，之后TiTan的开发团队在2017年陆续发了0.1.0rc1、0.1.0rc2、0.1.1、0.2.0等四个版本，最新的版本是2017年10月12日。 titan是从2012年开始开发，到2016年停止维护的一个分布式图数据库。最初在2012年启动titan项目的公司是Aurelius，2015年此公司被 DataStax（DataStax是开发apache Cassandra 的公司）收购，DataStax公司吸收了TiTan的图存储能力，形成了自己的商业产品DataStax Enterprise Graph。 TiTan开发者们希望把TitTan放到Apache Software Foundation下，不过，DataStax不愿意这样做，而且自从2015年9月DataStax收购了Titan的母公司后，TiTan一直处于停滞状态,鉴于此，2016年6月，TiTan的开发者们fork了一个TiTan的分支，重命名为JanusGraph，并将其置于Linux Software Foundation下。 2017年4月6日发布了第一个版本0.1.0-rc1，目前最新版本是2020年05月27日发布的0.6版 JanusGraph项目启动的初衷是“通过为其增加新功能、改善性能和扩展性、增加后端存储系统来增强分布式图系统的功能，从而振兴分布式图系统的开发” JanusGraph从Apahce TinkerPop中吸收了对属性图模型（Property Graph Model）的支持和对属性图模型进行遍历的Gremlin遍历语言。 基本概念同大多数图数据库一样，JanusGraph采用 属性图 进行建模。基于属性图的模型，JanusGraph有如下基本概念： Vertex Label：节点的类型，用于表示现实世界中的实体类型，比如”人”，“车”。在JanusGraph中，每一个节点有且只有一个Vertex Label。当不显式指定Vertex Label时，采用默认的Vertex Label。 Vertex：节点/顶点，用于表示现实世界中的实体对象。 Edge Label：边的类型，用于表示现实世界中的关系类型，比如“通话关系”，“转账关系”，“微博关注关系”等； Edge: 边，用于表示一个个具体的联系。JanusGraph的边都是单向边。如果需要双向边，则通过两条相反方向的单向边组成。JanusGraph不存在无向边。 Property Key：属性的类型，比如“姓名”，“年龄”，“时间”等。Property Key有Cardinality的概念。Cardinality有SINGLE、LIST和SET三种选项。这三种选项分别用于表示一个Property中，对于同一个Property Key是只允许有一个值、允许多个可重复的值，还是多个不可重复的值。 Property：属性，用于表示一个个具体的附加信息，采用Key-Value结构。Key就是Property Key，Value就是具体的值。 类似于下面这种图，包含节点和边，节点包含多个属性： 关键点 弹性和线性可扩展性，适用于不断增长的数据和用户群。 用于性能和容错的数据分发和复制。 多数据中心高可用性和热备份。 支持ACID和 最终的一致性。 支持各种存储后端： Apache Cassandra\\Apache HBase \\ Google Cloud Bigtable \\ Oracle BerkeleyDB 通过与大数据平台集成，支持全局图形数据分析，报告和ETL： Apache Spark\\Apache Giraph\\ApacheHadoop 支持以下方式进行geo、数据范围搜索和全文搜索： ElasticSearch \\ Apache Solr \\Apache Lucene 与Apache TinkerPop图形堆栈本机集成： Gremlin图查询语言 \\ Gremlin图服务器 \\ Gremlin应用程序 Apache 2许可下的开源 工具可视化存储在JanusGraph中的图形：Cytoscape \\Apache TinkerPop 的 Gephi插件\\ Graphexp \\ Cambridge Intelligence 的 KeyLines\\Linkurious 整体架构 JanusGraph是一个图形数据库引擎，本身专注于紧凑图形序列化，丰富的图形数据建模和高效的查询。利用Hadoop进行图形分析和批处理图处理。 JanusGraph为数据持久性、数据索引和客户端访问实现了强大的模块化接口。其模块化架构使其能够与各种存储、索引和客户端技术进行互操作；模块化架构还简化了支持新的一个 模块的流程。 架构图如下： ps：避免篇幅过大，架构相关的信息会在后续的博文详细说明 如何使用 作为一个数据库系统，它是要用来为应用程序存储数据用的，那么应用程序应该如何使用JanusGraph来为自己存储数据呢？ 一般来说，应用程序可以通过两种不同的方式来使用JanusGraph： 第一种方式：可以把JanusGraph嵌入到应用程序中去，JanusGraph和应用程序处在同一个JVM中。应用程序中的客户代码（相对JanusGraph来说是客户）直接调用Gremlin去查询JanusGraph中存储的图，这种情况下外部存储系统可以是本地的，也可以处在远程 第二种方式：应用程序和Janus Graph处在两个不同JVM中，应用通过给JanusGraph提交Gremlin查询给GremlinServer，来使用JanusGraph，因为JanusGraph原生是支持Gremlin Server的。 Gremlin Server是Apache Tinkerpop中的一个组件 JanusGraph集群包含一个、或者多个JanusGraph实例。每次启动一个JanusGraph实例的时候，都必须指定JanusGraph的配置。 在配置中，可以指定JanusGraph要用的组件，可以控制JanusGraph运行的各个方面，还可以指定一些JanusGraph集群的调优选项: 最小的JanusGraph配置只需要指定一下JanusGraph的后端存储系统，也就是它的持久化引擎。 如果要JanusGraph支持高级的图查询，就需要为JanusGraph指定一个索引后端。 若果要提升JanusGraph的查询性能，就必须为JanusGraph指定缓存，指定性能调优的选项。 以上提到的后端存储系统、索引后端、缓存、调优选项等都可以在JanusGraph的配置文件中进行指定。默认情况下它的配置文件存放在JanusGraph_home/conf目录下。 123456storage.backend=cassandrastorage.hostname=localhostindex.search.backend=elasticsearchindex.search.hostname=index.search.elasticsearch.client-only=true 也可以在写测试用例时代码控制： 123456789101112131415161718192021222324/*** 创建一个JanusGraph实例* @return JanusGraph的一个实例*/private static JanusGraph create() { try { return JanusGraphFactory.build() .set(&quot;storage.backend&quot;, &quot;hbase&quot;) .set(&quot;storage.hostname&quot;, &quot;&quot;) .set(&quot;storage.port&quot;, &quot;&quot;) .set(&quot;storage.hbase.table&quot;, &quot;&quot;) .set(&quot;cache.db-cache&quot;, &quot;true&quot;) .set(&quot;cache.db-cache-clean-wait&quot;, &quot;20&quot;) .set(&quot;cache.db-cache-time&quot;, &quot;180000&quot;) .set(&quot;cache.db-cache-size&quot;, &quot;0.5&quot;) .set(&quot;index.relationalNetwork.backend&quot;, &quot;elasticsearch&quot;) .set(&quot;index.relationalNetwork.hostname&quot;, &quot;&quot;) .set(&quot;index.relationalNetwork.port&quot;, 9000) .open(); } catch (Exception e) { e.printStackTrace(); return null; }} 通过上述代码，就可以生成一个janusgraph图实例，通过操作该图实例来对图数据库进行操作 总结本文介绍了，为什么需要图数据库，图数据库的基础理论，市场上存在的流行的图数据库并依照janusgraph图数据库来展开讲解一下图数据库相关知识等。","link":"/article/%E5%9B%BE%E8%A7%A3%E5%9B%BE%E5%BA%93JanusGraph%E7%B3%BB%E5%88%97-%E8%A7%A3%E6%83%91%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%81%E4%BD%A0%E7%9F%A5%E9%81%93%E4%BB%80%E4%B9%88%E6%98%AF%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93%E5%90%97%EF%BC%9F.html"},{"title":"如何解决内存泄漏问题？","text":"项目上线了一个接口，先灰度一台机器观察调用情况；接口不断的调用，过了一段时间，发现机器上的接口调用开始报OOM异常 ！当天就是上线deadline了，刺激。。 发现问题第一步，使用jps命令获取出问题jvm进程的进程ID使用jps -l -m获取到当前jvm进程的pid，通过上述命令获取到了服务的进程号：427726 （此处假设为这个）jps命令 jps(JVM Process Status Tool)：显示指定系统内所有的HotSpot虚拟机进程jps -l -m ： 参数-l列出机器上所有jvm进程，-m显示出JVM启动时传递给main()的参数 第二步，使用jstat观察jvm状态，发现问题 因为是OOM异常，所以我们首先重启机器观察了JVM的运行情况； 我们使用jstat -gc pid time命令观察GC，发现GC在YGC后，GC掉的内存并不多，每次YGC后都有一部分内存未回收，导致在多次YGC后回收不掉的内存被挪到堆的old区，old满了之后FGC发现也是回收不掉；这里基本可以确定是内存泄漏的问题了，下面我们有简单看了下机器的cpu、内存、磁盘状态 jstat命令： jstat(JVM statistics Monitoring)是用于监视虚拟机运行时状态信息的命令，它可以显示出虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。jstat -gc pid time ： -gc 监控jvm的gc信息，pid 监控的jvm进程id，time每个多少毫秒刷新一次jstat -gccause pid time ： -gccause 监控gc信息并显示上次gc原因，pid 监控的jvm进程id，time每个多少毫秒刷新一次jstat -class pid time： -class 监控jvm的类加载信息，pid 监控的jvm进程id，time每个多少毫秒刷新一次 在这里先简单说一下，堆的GC： 在GC开始的时候，对象只会存在于Eden区和名为“From”的Survivor区，Survivor区“To”是空的。紧接着进行GC，Eden区中所有存活的对象都会被复制到“To”，而在“From”区中，仍存活的对象会根据他们的年龄值来决定去向。 年龄达到一定值(年龄阈值，可以通过-XX:MaxTenuringThreshold来设置)的对象会被移动到年老代中，没有达到阈值的对象会被复制到“To”区域。经过这次GC后，Eden区和From区已经被清空。这个时候，“From”和“To”会交换他们的角色，也就是新的“To”就是上次GC前的“From”，新的“From”就是上次GC前的“To”。不管怎样，都会保证名为To的Survivor区域是空的，minor GC会一直重复这样的过程。 第三步，观察机器状态，确认问题使用top -p pid获取进程的cpu和内存使用率；查看RES 和 %CPU %MEM三个指标： 在这里先简单说一下，top命令展示的内容： VIRT：virtual memory usage 虚拟内存1、进程“需要的”虚拟内存大小，包括进程使用的库、代码、数据等2、假如进程申请100m的内存，但实际只使用了10m，那么它会增长100m，而不是实际的使用量 RES：resident memory usage 常驻内存1、进程当前使用的内存大小，但不包括swap out2、包含其他进程的共享3、如果申请100m的内存，实际使用10m，它只增长10m，与VIRT相反4、关于库占用内存的情况，它只统计加载的库文件所占内存大小 SHR：shared memory 共享内存1、除了自身进程的共享内存，也包括其他进程的共享内存2、虽然进程只使用了几个共享库的函数，但它包含了整个共享库的大小3、计算某个进程所占的物理内存大小公式：RES – SHR4、swap out后，它将会降下来 DATA1、数据占用的内存。如果top没有显示，按f键可以显示出来。2、真正的该程序要求的数据空间，是真正在运行中要使用的。 ps : 如果程序占用实存比较多，说明程序申请内存多，实际使用的空间也多。如果程序占用虚存比较多，说明程序申请来很多空间，但是没有使用。 发现机器的自身状态不存在问题， so毋庸置疑，发现问题了，典型的内存泄漏。。 第四步，使用jmap获取jvm进程dump文件我们使用jmap -dump:format=b,file=dump_file_name pid 命令，将当前机器的jvm的状态dump下来或缺的一份dump文件，用做下面的分析 jmap命令： jmap(JVM Memory Map)命令用于生成heap dump文件，还可以查询finalize执行队列、Java堆和永久代的详细信息，如当前使用率、当前使用的是哪种收集器等。jmap -dump:format=b,file=dump_file_name pid ： file=指定输出数据文件名， pid jvm进程号 接下来，回滚灰度的机器，开始解决问题=.= 解决问题第一步，dump文件分析在这里，我们分析dump文件，使用的Jprofiler软件，就是下面这个东东： 具体的使用方法，在这就不再赘述了，下面将dump文件导入到Jprofiler中：选择Heap Walker 中的Current Object Set，这里面显示的是当前的类的占用资源，从占用空间从大到小排序；从上图中，没有观察出什么问题，我们点击Biggest Objects，查看哪个对象的占用的内存高：从上图中，我们发现org.janusgraph.graphdb.database.StandardJanusGraph这个对象居然占用了高达724M的内存！ 看来内存泄漏八九不离十就是这个对象的问题了！再点开看看 ，如下图，可以发现是一个openTransactions的类型为ConcurrentHashMap的数据结构： 第二步，源码查找定位代码这到底是什么对象呢，去项目中查找一下，打开idea-打开项目-双击shift键-打开全局类查找-输入StandardJanusGraph，如下图：发现是我们项目使用的图数据库janusgraph的一个类，找到对应的数据结构：类型定义： 1private Set&lt;StandardJanusGraphTx&gt; openTransactions; 初始化为一个ConcurrentHashMap： 123openTransactions = Collections.newSetFromMap(new ConcurrentHashMap&lt;StandardJanusGraphTx, Boolean&gt;(100, 0.75f, 1)); 观察上述代码，我们可以看到，里面的存储的StandardJanusGraphTx从字面意义上理解是janusgraph框架中的事务对象，下面往上追一下代码，看看什么时候会往这个Map中赋值：// 1. 找到执行openTransactions.add()的方法 1234567891011public StandardJanusGraphTx newTransaction(final TransactionConfiguration configuration) { if (!isOpen) ExceptionFactory.graphShutdown(); try { StandardJanusGraphTx tx = new StandardJanusGraphTx(this, configuration); tx.setBackendTransaction(openBackendTransaction(tx)); openTransactions.add(tx); // 注意！ 此处对上述的map对象进行了add return tx; } catch (BackendException e) { throw new JanusGraphException(&quot;Could not start new transaction&quot;, e); }} // 2. 上述发现，是一个newTransaction，创建事务的一个方法，为确保起见，再往上跟找到调用上述方法的类： 12345678910public JanusGraphTransaction start() { TransactionConfiguration immutable = new ImmutableTxCfg(isReadOnly, hasEnabledBatchLoading, assignIDsImmediately, preloadedData, forceIndexUsage, verifyExternalVertexExistence, verifyInternalVertexExistence, acquireLocks, verifyUniqueness, propertyPrefetching, singleThreaded, threadBound, getTimestampProvider(), userCommitTime, indexCacheWeight, getVertexCacheSize(), getDirtyVertexSize(), logIdentifier, restrictedPartitions, groupName, defaultSchemaMaker, customOptions); return graph.newTransaction(immutable); // 注意！此处调用了上述的newTransaction方法 } // 3. 接着找上层调用，发现了最上层的方法 123public JanusGraphTransaction newTransaction() { return buildTransaction().start(); // 此处调用了上述的start方法} 在我们对图数据库中图数据操作的过程中，采用的是手动创建事务的方式，在每次查询图数据库之前，我们都会调用类似于dataDao.begin()代码，其中就是调用的 public JanusGraphTransaction newTransaction()这个方法； 最后，我们简单的看下源码可以发现，从上述内存泄漏的map中去除数据的逻辑就是commit事务的接口，调用链如下：// 1. 从openTransactions这个map中删除事务对象的方法 123456789public void closeTransaction(StandardJanusGraphTx tx) { openTransactions.remove(tx); // 从map中删除StandardJanusGraphTx对象}private void releaseTransaction() { isOpen = false; graph.closeTransaction(this); // 调用上述closeTransaction方法 vertexCache.close();} // 2. 最上层找到了commit方法，提交事务后就会将对应的事务对象从map中移除 12345678910111213141516171819202122232425262728public synchronized void commit() { Preconditions.checkArgument(isOpen(), &quot;The transaction has already been closed&quot;); boolean success = false; if (null != config.getGroupName()) { MetricManager.INSTANCE.getCounter(config.getGroupName(), &quot;tx&quot;, &quot;commit&quot;).inc(); } try { if (hasModifications()) { graph.commit(addedRelations.getAll(), deletedRelations.values(), this); } else { txHandle.commit(); // 这个commit方法中释放事务也是调用releaseTransaction } success = true; } catch (Exception e) { try { txHandle.rollback(); } catch (BackendException e1) { throw new JanusGraphException(&quot;Could not rollback after a failed commit&quot;, e); } throw new JanusGraphException(&quot;Could not commit transaction due to exception during persistence&quot;, e); } finally { releaseTransaction(); // // 调用releaseTransaction if (null != config.getGroupName() &amp;&amp; !success) { MetricManager.INSTANCE.getCounter(config.getGroupName(), &quot;tx&quot;, &quot;commit.exceptions&quot;).inc(); } } } 终于，我们找到了内存泄漏的根源所在：项目代码中存在调用了事务begin但是没有commit的代码! 第三步，修复问题验证解决问题： 找到内存泄漏接口的代码，并发现了没有commit()的位置，try-catch-finally中添加上了commit()代码； 提交-部署-发布-灰度一台机器后观察内存泄漏的现象消失，GC回收正常； 内存泄漏问题解决，项目如期上线~ 最后对于内存泄漏导致的内存溢出问题，排查步骤大致如上述，总的步骤：找出问题（GC、CPU、磁盘、内存、网络），定位问题（使用第三方工具分析dump文件等），解决问题； 另外，内存泄漏的分析方法有好多种，但是大致原理和流程都是相似的； 原创不易，如果大家有所收获，希望大家可以点赞评论支持一下~ 也欢迎大家关注我的CSDN支持一下作者，作者定期分享工作中的所见所得~","link":"/article/%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E9%97%AE%E9%A2%98%EF%BC%9F.html"},{"title":"操作系统-正在筹备中...","text":"文章正在筹备中，敬请期待！","link":"/article/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-%E6%AD%A3%E5%9C%A8%E7%AD%B9%E5%A4%87%E4%B8%AD.html"},{"title":"算法-正在筹备中...","text":"文章正在筹备中，敬请期待！","link":"/article/%E7%AE%97%E6%B3%95-%E6%AD%A3%E5%9C%A8%E7%AD%B9%E5%A4%87%E4%B8%AD.html"},{"title":"编程之美，从线程池状态管理来看二进制操作之美","text":"二进制操作在框架设计中被频繁使用，使用二进制在不同场景有提升计算速度、较少内存占用等多种优点； 下面，我们依据线程池的状态管理，来看下怎么通过操作二进制对状态进行管理，过程中会发现编程之美~ 线程池状态首先，为了文章的完整性，我们还是先了解一下线程池的状态，总结如下如： 线程池状态分为5种：RUNNING、SHUTDOWN、STOP、TIDYING、TERMINATED 状态代表的含义 RUNNING：（运行）接收新task，并且处理正在排队的task，不中断正在执行的任务 SHUTDOWN：（关闭）不接受新的task，只处理正在排队的task，不中断正在执行的任务 STOP：（停止）不接受新的task，也不处理正在排队的task，并且中断正在执行的任务 TIDYING：（整理）所有的task都已经终止，上述提到的workCount当前活跃线程数为0，被中断的任务和正在排队的任务执行当前任务的terminated()钩子方法 TERMINATED：（已终止）标识上述的TIDYING的过程结束，标识当前线程池成功完全停止的状态 状态转换大致的流程就是： RUNNING –&gt; SHUTDOWN –&gt; STOP –&gt; TIDYING –&gt; TERMINATED 上述流程是一个单方向的顺序，也就是说不会出现类似于STOP –&gt; SHUTDOWN 这种情况； 另外，并不是每一个状态多必须经过的； 什么时候进行线程池的状态转换呢？ RUNNING -&gt; SHUTDOWN：调用终止线程的方法shutdown()后 RUNNING or SHUTDOWN -&gt; STOP：调用shutdownNow()方法后，不管当前在RUNNING状态还是SHUTDOWN状态，都是直接转为STOP状态 SHUTDOWN -&gt; TIDYING：SHUTDOWN状态下当等待队列 和 正在执行的任务 都为空时，状态转为TIDYING STOP -&gt; TIDYING：STOP状态下当正在执行的任务全部中断完毕后，状态转为TIDYING TIDYING -&gt; TERMINATED：TIDYING状态下当所有的terminated()钩子方法全部执行完毕后，状态转为TERMINATED，线程池关闭完毕！ 管理线程池状态线程池中管理线程池状态 和 线程池当前活跃线程数，是通过一个AtomicInteger变量来管理这两个状态的 什么？ 一个变量管理两个这么不相干的状态？ 对的; CTL变量何许人也让我们来看一下线程池针对这部分的实现： 12345678910111213141516private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// runState is stored in the high-order bitsprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;// Packing and unpacking ctlprivate static int runStateOf(int c) { return c &amp; ~CAPACITY; }private static int workerCountOf(int c) { return c &amp; CAPACITY; }private static int ctlOf(int rs, int wc) { return rs | wc; }private static boolean isRunning(int c) { return c &lt; SHUTDOWN;} 下面，我们来剖析一下上述的实现：线程池包含5种状态如下：具体线程的状态代表的含义和状态的转换，下面会有讲解： 1234567private static final int COUNT_BITS = Integer.SIZE - 3;private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; 我们知道在java中 int 类型占用4个字节32位存储， 上述的几种状态：底层存储二进制为： 1111 1111 1111 1111 1111 1111 1111 1111(-1)0000 0000 0000 0000 0000 0000 0000 0000(0)0000 0000 0000 0000 0000 0000 0000 0001(1)0000 0000 0000 0000 0000 0000 0000 0010(2)0000 0000 0000 0000 0000 0000 0000 0011(3) 左移&lt;&lt;COUNT_BITS位， COUNT_BITS = Integer.SIZE - 3 也就是 COUNT_BITS = 29，改句子说明用32位的前3位存储线程池的状态后29位存储线程池中当前线程的个数， &lt;&lt; COUNT_BITS后，变为下面的二进制： 1110 0000 0000 0000 0000 0000 0000 00000000 0000 0000 0000 0000 0000 0000 00000010 0000 0000 0000 0000 0000 0000 00000100 0000 0000 0000 0000 0000 0000 00000110 0000 0000 0000 0000 0000 0000 0000 我们可以看到，前三位存储的是 标识线程状态的二进制 对于初始化存储这些状态的变量 AtomicInteger ctl 1private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)) 初始化AtomicInteger变量ctl，其中ctlOf(RUNNING, 0) 代码为： 1private static int ctlOf(int rs, int wc) { return rs | wc; } 其中rs标识线程池当前状态，wc为work count标识当前工作线程的数量 上述传入的是ctlOf(RUNNING, 0) ，当前状态为RUNING也就是1110 0000 0000 0000 0000 0000 0000 0000 ，wc为0，也就是当前工作线程数为0，其二进制为0000 0000 0000 0000 0000 0000 0000 0000 ，做&quot;|&quot;或操作，即 1110 0000 0000 0000 0000 0000 0000 0000 | 0000 0000 0000 0000 0000 0000 0000 0000= 1110 0000 0000 0000 0000 0000 0000 0000 上述得到的结果1110 0000 0000 0000 0000 0000 0000 0000就标识，当前线程池状态为RUNNING，线程池活跃线程个数为0！ 如何管理？通过上述创建的ctl变量获取 线程池当前状态 和 线程中活跃线程个数 这两个状态： 获取线程池当前状态，我们可以想一下该如何获取呢？ 现在知道的是ctl的前3位是线程池的状态，那我们直接构造一个前三位为1，后29位为0的int即可，然后取余就可以了呗，下面看下源码的实现，就是如此：使用方法runStateOf 1private static int runStateOf(int c) { return c &amp; ~CAPACITY; } 其中CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1 转化为二进制为：0001 1111 1111 1111 1111 1111 1111 1111取反”“后，二进制为：1110 0000 0000 0000 0000 0000 0000 0000也就是将前3位全部变为1，后面全部变为0；接下来，传入的ctl变量和`CAPACITY`做“&amp;”操作，只会保留ctl变量的前3位变量，后29位变量全部为0； 例如：一个标识当前状态为STOP状态的线程池和当前活跃线程数为3的ctl变量为：0010 0000 0000 0000 0000 0000 0000 0011和上述得到的1110 0000 0000 0000 0000 0000 0000 0000做“&amp;”操作后得到：0010 0000 0000 0000 0000 0000 0000 0000 和上述分析的STOP的状态的二进制相同！ 即获得了当前线程的状态！ 获取线程池当前状态，也很简单，我们知道ctl变量的32的后29位存储的是当前活跃线程数，直接构造一个前三位为0，后29位为1的int即可，然后取余就可以获取到了使用方法workerCountOf 1private static int workerCountOf(int c) { return c &amp; CAPACITY; } 上述知道CAPACITY为：0001 1111 1111 1111 1111 1111 1111 1111 例如：一个标识当前状态为STOP状态的线程池和当前活跃线程数为3的ctl变量为：0010 0000 0000 0000 0000 0000 0000 0011 和 0001 1111 1111 1111 1111 1111 1111 1111 取与后:0000 0000 0000 0000 0000 0000 0000 0011标识当前线程池中活跃线程数量为3！ 一些方法1、计算ctl的值 方法： 1private static int ctlOf(int rs, int wc) { return rs | wc; } 其中，入参rs代表当前线程状态，wc代表当前活跃线程数，取“|”或即可上述代码不出现问题的前提是：rs只使用的前3位，wc只使用了后29位！ 2、判断当前线程池是否正在运行 方法： 1private static boolean isRunning(int c) { return c &lt;小于SHUTDOWN;}值即可！ 上述我们知道，5中状态只有RUNNING小于0，SHUTDOWN状态等于0，其他的都是大于0的，所以我们直接把给定的ctl值小于SHUTDOWN值即可！ 最后上述，我们介绍了 线程池的状态 管理部分，主要通过不同位置的二进制来进行标识不同的状态，工作学习还会发现更多巧妙美妙的设计，等待着作为程序员我们去发现；","link":"/article/%E7%BC%96%E7%A8%8B%E4%B9%8B%E7%BE%8E%EF%BC%8C%E4%BB%8E%E7%BA%BF%E7%A8%8B%E6%B1%A0%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E6%9D%A5%E7%9C%8B%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%93%8D%E4%BD%9C%E4%B9%8B%E7%BE%8E.html"},{"title":"问题解决-idea 本地仓库中包含jar包，但是pom依赖文件标红","text":"现象，maven的本地仓库中，明明存在这个包，但是在项目的pom文件中就是标红报错，reimport也不可以，这次编译一个框架的源码时遇到了这个问题，解决了一下，下面说下解决方法： 首先，确认本地仓库存在该标红的jar包，版本也是对的！ 主要原因是因为，maven的本地仓库的索引（index）没有被更新为最新的问题，项目导入依赖时通过索引就拿不到对应的jar包，就报错。 解决方案：idea编译器 file -&gt; settings -&gt; Build,Execution,Deployment -&gt; Build Tools -&gt; Maven -&gt; Repositories 选择本地仓库地址，点击右侧update 等待更新完毕 重启idea，解决~ 当然，idea还是很友好的，你把鼠标指到对应的标红出，alt+entre就会看到显示出下面的提示：选择第一个update maven indices 更新maven索引，点击，就会看到在更新索引了，同上面的效果一样~","link":"/article/%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3-idea_%E6%9C%AC%E5%9C%B0%E4%BB%93%E5%BA%93%E4%B8%AD%E5%8C%85%E5%90%ABjar%E5%8C%85%EF%BC%8C%E4%BD%86%E6%98%AFpom%E4%BE%9D%E8%B5%96%E6%96%87%E4%BB%B6%E6%A0%87%E7%BA%A2.html"},{"title":"高并发-正在筹备中...","text":"文章正在筹备中，敬请期待！","link":"/article/%E9%AB%98%E5%B9%B6%E5%8F%91-%E6%AD%A3%E5%9C%A8%E7%AD%B9%E5%A4%87%E4%B8%AD.html"},{"title":"图解图库JanusGraph系列-一文知晓“图数据“底层存储结构（JanusGraph data model）","text":"本文详细讲解了JanusGraph的存储结构！ 大家好，我是洋仔，JanusGraph图解系列文章，实时更新~ 图数据库文章总目录： 整理所有图相关文章，请移步(超链)：图数据库系列-文章总目录 地址：https://liyangyang.blog.csdn.net/article/details/111031257 **源码分析相关可查看github（求star~~）**： https://github.com/YYDreamer/janusgraph 下述流程高清大图地址：https://www.processon.com/view/link/5f471b2e7d9c086b9903b629 版本：JanusGraph-0.5.2 转载文章请保留以下声明： 作者：洋仔聊编程微信公众号：匠心Java原文地址：https://liyangyang.blog.csdn.net/ 一：存储模式1、图内容本文以下所有内容基于：JanusGraph基于属性图来进行构造图数据： 属性图： 属性图是由 顶点（Vertex），边（Edge），属性（Property）组成的有向图 Vertex可以包含Properties；Edge也可以包含Properties； 2、存储方法图存储的方式常用的有两种：邻接列表 和 邻接矩阵 JanusGraph采用邻接列表进行图数据的存储，如下图所示：（此处将图中节点抽象为 只有节点，没有属性） 在Janusgraph中一个顶点的邻接列表包含该节点对应的属性和关联的边，下述会详细说明 Janusgraph中邻接列表是如何实现的； 3、图切割方式图的切割方式分为两种：按节点切割(Vertex Cut)和按边切割(Edge Cut) Vertex Cut：根据点进行切割，每个边只存储一次，只要是节点对应的边便会多一份该节点的存储 Edge Cut：根据边进行切割，以节点为中心，边会存储两次，源节点的邻接列表存储一次，目标节点的邻接列表存储一次 在Janusgraph中既存在Edge Cut，也存在Vertex Cut的情况； 在默认的情况下使用边切割，而针对热点节点可以通过配置makeVertexLabel('product').partition()来将节点类型为product类型的节点进行Vertex Cut； 也就是说，在没有上述makeVertexLabel('product').partition()配置的话，JanusGraph所有的图数据都是以Edge Cut的方式来进行切割存储的； 具体可以查看文章：《JanusGraph-分区》中自定义分区部分中关于图切割部分的介绍； 我们例子来说明一下： 如下图： 张三用户节点通过手机号关联出来李四用户节点 张三 和 李四 代表Vertex；指向的name、age、gender代表张三的属性 edgeA 和edgeB 代表Edge；也可以包含边的属性，例如下图中边包含属性create_time 按边切割后： 节点 张三 name(property) age(property) gender(property) edgeA(edge) phone phone(property) edgeA(edge) edgeB(edge) 李四 name(property) age(property) gender(property) edgeB(edge) 上述可以看到，按照边切割后每一条边会存储两次！ 二：BigTable模型 在JanusGraph的存储中， JanusGraph将图形的邻接列表的表示存储在支持Bigtable数据模型的任何存储后端中 BigTable模型如下图： 在Bigtable数据模型中，每个表是行的集合，由一个key唯一标识。 每行由任意（可以很大数量但是必须有限数量）数量的cell组成；cell由column和value组成，column唯一标识某一个cell。 上述图中，有两部分需要排序的支持：sorted by key 和 sorted by column： sorted by key：标识存储后端存储的数据时按照key的大小进行排序存储的 sorted by column：这是JanusGraph对Bigtable数据模型有一个额外要求，存储edge(边)的单元格必须按column排序，并且列范围指定的单元格子集必须是有效可检索的； 这句话详细解答在下述文章中有体现 在Bigtable模型中的行称为“宽行”，因为它们支持大量cell，并且不必像关系数据库中那样预先定义这些cell的column。 在关系型数据库中我们必须先定义好表的schema，才可以存储数据，如果存储过程中想要改变表结构，则所有的数据都要对变化的列做出变化。但是Bigtable模型存储中就不必如此，每个行的column不同，我们可以随时仅对某一行进行变化，也不许预先定义行的schema，只需要定义图的schema即可。 此外，特定的Bigtable实现可以使行按其键的顺序排序。JanusGraph可以利用这样的键序来有效地划分图形，从而为非常大的图形提供更好的加载和遍历性能。 JanusGraph是如何基于BigTable数据模型针对于自身的图数据特性进行设计的呢？ 下面我们看下JanusGraph的逻辑存储结构 三：存储逻辑结构 JanusGraph基于使用BigTable模型的存储后端 实现了自己的存储的逻辑结构 ps：为了更好的理解，下面部分知识点会基于HBase存储后端进行进一步的解释！ 1、整体结构 在JanusGraph中，以节点为中心，按切边的方式存储数据的。比如在Hbase中节点的ID作为HBase的Rowkey，节点上的每一个属性和每一条边，作为该Rowkey行的一个个独立的Cell。即每一个属性、每一条边，都是一个个独立的KCV结构(Key-Column-Value) 上图中，我们可以发现图的存储整体分为三部分：vertex id、property、edge： vertex id： 对应节点的唯一id，如果底层存储使用的是Hbase则代表着当前行的Rowkey，唯一代表某一个节点 property： 代表节点的属性 edge： 代表节点的对应的边 排序方式分为三种：sorted by id、sorted by type、sorted by sort key： sorted by id： 依据vertex id在存储后端进行顺序存储 sorted by type：此处的个人理解为针对于property 和 edge的类型进行排序，保证同种类型的属性或者边连续存储在一块便于遍历查找； // TODO 深层次理解 sorted by sort key： sort key是边组成以的一部分，主要作用是，在同种类型的edge下，针对于sort key进行排序存储，提升针对于指定sort key的检索速度；下面edge结构部分有详细介绍 2、Vertex id 的结构此处的Vertex id唯一标识图中的某一个节点；节点vertex id的组成结构我们在源码类IDManager的一段注释中可以发现： 123/* --- JanusGraphElement id bit format --- * [ 0 | count | partition | ID padding (if any) ]*/ 这是在Janusgraph在生成所有的id时统一的格式包含vertex id\\edge id\\property id的时候，这个顺序也 就是标识我们再使用gremlin查询出节点时，节点上标识的vertex id； 这个id值的顺序不同于hbase真实存储Rowkey的顺序！！！！！！！ 在对vertex id进行序列化存储时，位置有所调整为：[ partition | 0 | count | ID padding (if any) ] 如下图： 从图中可以看出： Vertex ID共包含一个字节、8位、64个bit Vertex ID由partition id、count、ID padding三部分组成 最高位5个bit是partition id。partition是JanusGraph抽象出的一个概念。当Storage Backend是HBase时，JanusGraph会根据partition数量，自动计算并配置各个HBase Region的split key，从而将各个partition均匀映射到HBase的多个Region中。然后通过均匀分配partition id最终实现数据均匀打散到Storage Backend的多台机器中 中间的count部分是流水号，其中最高位比特固定为0；出去最高位默认的0，count的最大值为2的(64-5-1-3)=55次幂大小：3 6028 7970 1896 3968，总共可以生成30000兆个id，完全满足节点的生成 最后几个bit是ID padding, 表示Vertex的类型。具体的位数长度根据不同的Vertex类型而不同。最常用的普通Vertex，其值为’000’ 为什么在序列化存储vertex id时，需要调整顺序序列化作为RowKey存储到Hbase呢？ 我们通过下面的3个问题来回答： 为什么JausGraph分配的逻辑区间值，可以影响hbase物理存储呢？ 可以将分区相同的数据存放的更近呢？ 在上述描述中，hbase使用vertex id作为rowkey，hbase根据rowkey顺序排序存储； 每个hbase region存储是一段连续的Rowkey行； 在janusgraph的vertex id的设计中，可以发现将分区值放到了64位的前5位存储！ 在存储数据到hbase时，对rowkey进行排序，因为partition id在前5位，所以同一个分区的vertex id对应的rowkey值相差较小，所以会存储在一块； 如何快速的查询到不同类型的节点呢？ 换个说法如何快速的确定当前的行就是我们需要的节点类型的行呢？ 在JanusGraph的vertex id中包含的 ID padding就代表当前的节点类型（注意此处的类型！=lable）。000标识为普通节点，在id的组成部分中，我们经过前面的分析，最前面是partition id，只有把 ID padding放在最后几个字节便于查找了； 为什么查询出的节点显示的vertex id要把0|count放在最前面、partiton和id padding放在后面呢？ 这里我们猜测一下：count占用55位数据！ 试想如果把count不放在最前面，那么id的最小值比2的55次幂还大，显示不友好！ 如果把0|count放在最前面呢？就会有两个效果： 0在有符号表示中标识当前id始终为正整数！ count是趋势递增的，所以id值也是从小到大趋势递增的，所以节点id的最小值在2的8次幂周边大小； 比把count放在后面显示的id值友好多了~~~ vertex id是如何保证全局唯一性的呢？ 主要是基于数据库 + 号段模式进行分布式id的生成； 体现在图中就是partition id + count 来保证分布式全局唯一性； 针对不同的partition都有自己的0-2的55次幂的范围的id； 每次要生成vertex id时，首先获取一个partition，获取对应partition对应的一组还未使用的id，用来做count； janusgraph在底层存储中存储了对应的partition使用了多少id，从而保证了再生成新的分布式vertex id时，不会重复生成！ ps ： JanusGraph中分布式唯一vertex id、edge id、property id的生成分析，请看《图解JanusGraph系列-分布式唯一id的生成机制》 3、edge 和 property的结构在上述的JanusGraph的整体结构中，property和edge都是作为cell存储在底层存储中；其中cell又分为column和value两部分，下图展示了这两部分的逻辑结构： 下面我们详细分析一下 property 和 edge对应的逻辑结构； 3.1 edge的结构Edge的Column组成部分： label id：边类型代表的id，在创建图schema的时候janusgraph自动生成的label id，不同于边生成的唯一全局id direction：图的方向，out：0、in：1 sort key：可以指定边的属性为sort key，可多个；在同种类型的edge下，针对于sort key进行排序存储，提升针对于指定sort key的检索速度； 该key中使用的关系类型必须是属性非唯一键或非唯一单向边标签； 存储的为配置属性的value值，可多个（只存property value是因为，已经在schema的配置中保存有当前Sort key对应的属性key了，所以没有必要再存一份） adjacent vertex id：target节点的节点id，其实存储的是目标节点id和源节点id的差值，这也可以减少存储空间的使用 edge id：边的全局唯一id Edge的value组成部分： signature key：边的签名key 该key中使用的关系类型必须是属性非唯一键或非唯一单向边标签； 存储压缩后的配置属性的value值，可多个（只存property value是因为，已经在schema的配置中保存有当前signature key对应的属性key了，所以没有必要再存一份） 主要作用提升edge的属性的检索速度，将常用检索的属性设置为signature key，提升查找速度 other properties：边的其他属性 注意！ 不包含配置的sort key和signature key属性值，因为他们已经在对应的位置存储过了，不需要多次存储！ 此处的属性，要插入属性key label id和属性value来标识是什么属性，属性值是什么； 此处的property的序列化结构不同于下述所说的vertex节点的property结构，edge中other properties这部分存储的属性只包含：proeprty key label id + property value；不包含property全局唯一id！ 详细解释及思考： 在进行详细分析前，请大家思考几个问题，如下: 基于上述的edge逻辑结构，JanusGraph是如何构造邻接列表的 或者 是如何获取源节点的邻接节点的？ 上述的Edge逻辑结构中的，每部分的排列的顺序的含义是什么？ 1、基于上述的edge逻辑结构，JanusGraph是如何构造邻接列表的 或者 是如何获取源节点的邻接节点的？ 从上述的整体结构部分中，我们可以知道，vertexId行后跟着当前的节点关联的所有的edge； 而在上述的edge的逻辑结构中，有一个adjacent vertex id字段，通过这个字段就可以获取到target节点的vertex id，就相当于指向了target节点，整理一下： 如上图，通过上述的条件，就可以构造一个VertexA指向VertexB 和 VertexC的邻接链表； 其实，JanusGraph可以理解为构造的是双向邻接列表， 依据上图，我们知道vertexA 和 vertexB 和 vertexC存在边关系； 当我们构造vertexB的邻接列表时，会包含指向vertexA的节点，只是说在edge对应的逻辑结构中边的方向不同而已： 总结：JanusGraph通过vertex id行中包含所有关联的edge，edge逻辑结构中包含指向target节点的数据来组成双向邻接列表的结构； 2、上述的Edge逻辑结构中的，每部分的排列的顺序的含义是什么？ 首先，在查询的时候为了提升查询速度，我们首先要过滤的是什么，针对于edge毋庸置疑是边的类型和边的方向； 所以，为了我们可以更快的拿到类型和方向，所以在edge的存储结构中，我们发现作者将类型和方向存放在了column中，并且是column的最前面部分；这样我们可以直接通过判断column的第一部分字节就可以对边类型和方向进行过滤！ ps：虽然我们在写Gremlin语句的时候，可能是语句写的是先过滤边的属性或者其他，但是JanusGraph会针对我们的gremlin语句进行优化为先过滤边类型和方向 接下来，我们可能对边的属性进行过滤，我们怎样提升经常要过滤的属性的查询速度呢？ 我们将经常用于范围查询的属性配置为sort key，然后就可以在过滤完边类型和方向后快速的进行属性的范围过滤（此处快速的指过滤配置为sort key的属性）； 3.2 property的结构property的存储结构十分的简单，只包含key id、property id和value三部分： key id：属性label对应的id，有创建schema时JanusGraph创建； 不同于属性的唯一id property id：属性的唯一id，唯一代表某一个属性 value：属性值 注意：属性的类型包含SINGLE、LIST和SET三种Cardinality；当属性被设置为LIST类型时，因为LIST允许当前的节点存在多个相同的属性kv对，仅通过key id也就是属性的label id是无法将相同的属性label区分出来的 所以在这种情况下，JanusGraph的property的存储结构有所变化， property id也将会被存储在column中，如下图： 四：index存储结构1、Composite Index-vertex index结构图一（唯一索引Composite Index结构图）： 图二（非唯一索引Composite Index结构图）： Rowkey由index label id 和properties value两大部分组成： index label id：标识当前索引类型 properties value：索引中包含属性的所有属性值，可多个； 存在压缩存储，如果超过16000个字节，则使用GZIP对property value进行压缩存储！ Column由第一个字节0 和 vertex id组成： 第一个字节0：无论是唯一索引，还是非唯一索引此部分都会存在；如图一 vertex id：非唯一索引才会在column中存在，用于分别多个相同索引值对应的不同节点；如图二 value由vertex id组成： vertex id：针对于rowkey + column查询到的value是vertex id，然后通过vertex id查询对应的节点 2、Composite Index-edge index结构图一（唯一索引Composite Index结构图）： 图二（非唯一索引Composite Index结构图）： Rowkey同Vertex index部分 Column由第一个字节0 和 edge id组成： 第一个字节0：无论是唯一索引，还是非唯一索引此部分都会存在；如图一 edge id：非唯一索引才会在column中存在，用于分别多个相同索引值对应的不同节点；如图二 value由以下4部分组成： edge id：边id out vertex id：边对应的出边id type id：edge 的label type id in vertex id：边对应的入边id 2、Mixed Index结构这里以ES作为第三方索引库为例，这里只介绍普通的范围查找的mixed index的构造： ES的概念为：index 包含多个 type；每个type包含多个document id，每个document id包含多个field name 和对应的field value； 在Jausgraph中 index：包含两种，janusgraph_edge 和 janusgraph_vertex两种 type：可自定义 document id：edge id或者 vertex id field name：索引对应属性的label string field value：属性对应的property value 基于倒排索引的查询顺序为，给定过一个property label 和 property value查询对应的Vertex id 或者 edge id，则查询满足要求的field name 和 field value，就可以获取到对应的document id即Vertex id 或者 edge id； 五：序列化数据案例以序列化实例来看下上述所说的整体结构 测试节点数据： 123456789{ &quot;label&quot;:&quot;user&quot;, &quot;propertyMap&quot;:{ &quot;create_time&quot;:&quot;2016-12-09 02:29:26&quot;, &quot;user_name&quot;:&quot;张三&quot;, &quot;user_id&quot;:&quot;test110&quot; }, &quot;vertexId&quot;:4152} 测试边数据： 123456789{ &quot;edgeId&quot;:17514510, &quot;label&quot;:&quot;user_login_phone_number&quot;, &quot;propertyMap&quot;:{ &quot;productid&quot;:&quot;2&quot; }, &quot;sourceId&quot;:4152, &quot;targetId&quot;:40964120} 跟踪Janusgraph源码，获取其序列化信息，后端存储使用Hbase： 节点序列化后数据（不包含索引）： 边序列化后数据（不包含索引）： 节点的vertex id序列化后的数据为56 0 0 0 0 0 0 -128；一个节点对应的属性和边的Rowkey相同，依据qualifier也就是column来进行区分； 在边的序列化结果中，包含两部分：一部分是节点4152的kcv，一个是节点40964120的kcv；这地方也可以说明JanusGraph是采用的双向邻接链表进行图存储的 五：Schema的使用从上述来看，我们可以知道，JanusGraph图的schema该怎样定义主要是由edge labels 、property keys 和vertex labels 组成（Each JanusGraph graph has a schema comprised of the edge labels, property keys, and vertex labels used therein） JanusGraph的schema可以显式或隐式创建，推荐用户采用显式定义的方式。JanusGraph的schema是可以在使用过程中修改的，而且不会导致服务宕机，也不会拖慢查询速度。 比如一个简单的显示定义的销售图的scheme： 1234567891011121314151617181920212223242526272829303132333435&lt;propertyKey value=&quot;salesman_id&quot; explain=&quot;销售人员id&quot; index=&quot;&quot; type=&quot;java.lang.String&quot; /&gt;&lt;propertyKey value=&quot;real_name&quot; explain=&quot;姓名&quot; index=&quot;&quot; type=&quot;java.lang.String&quot; /&gt;&lt;propertyKey value=&quot;role&quot; explain=&quot;角色&quot; type=&quot;&quot; /&gt;&lt;propertyKey value=&quot;city_code&quot; explain=&quot;所处城市代码&quot; index=&quot;&quot; type=&quot;&quot; /&gt;&lt;propertyKey value=&quot;create_time&quot; explain=&quot;创建时间&quot; index=&quot;&quot; type=&quot;&quot; /&gt;&lt;edgeLabel value=&quot;saleman_service_for&quot; explain=&quot;销售引导&quot;&gt; &lt;propertys&gt; &lt;property value=&quot;create_time&quot;/&gt; &lt;/propertys&gt;&lt;/edgeLabel&gt;&lt;edgeLabel value=&quot;own_salaman_Idcard&quot; explain=&quot;销售身份&quot;&gt; &lt;propertys&gt; &lt;property value=&quot;create_time&quot;/&gt; &lt;/propertys&gt;&lt;/edgeLabel&gt;&lt;index elementType=&quot;vertex&quot; indexType=&quot;compositeIndex&quot; name=&quot;salesman_id_I&quot; &gt; &lt;propertyKeys&gt; &lt;propertyKey value=&quot;salesman_id&quot; /&gt; &lt;/propertyKeys&gt;&lt;/index&gt;&lt;vertexLabel value=&quot;salesman&quot; explain=&quot;销售&quot; &gt; &lt;propertys&gt; &lt;property value=&quot;salesman_id&quot; /&gt; &lt;property value=&quot;real_name&quot; /&gt; &lt;property value=&quot;role&quot; /&gt; &lt;property value=&quot;city_code&quot; /&gt; &lt;/propertys&gt; &lt;edges&gt; &lt;edge value=&quot;saleman_service_for&quot; direction=&quot;out&quot; /&gt; &lt;edge value=&quot;own_salaman_Idcard&quot; direction=&quot;out&quot; /&gt; &lt;/edges&gt;&lt;/vertexLabel&gt; 当然，我们也可以添加一些其他的可以组成schema的元素，上述三个是必须的，另外的比如索引（index）等，主要的结构还是： JanusGraph Schema |-----------Vertex Lables |-----------Property Keys |-----------Edge Labels 和关系型数据库不同，图数据的schema是定义一张图，而非定义一个vertex的。在Mysql中，我们通常将建立一张表定义为创建一个schema，而在JanusGraph中，一个Graph用于一个schema。 六：源码分析源码分析已经push到github：https://github.com/YYDreamer/janusgraph 七：总结 JanusGraph采用Edge cut的方式进行图切割，并且按照双向邻接列表的形式进行图存储 JanusGraph每个节点都是对应的kcv结构； vertex id唯一标识节点；对应的行cell存储节点属性和对应的边 节点id的分布式唯一性采用数据库+号段模式进行生成；","link":"/article/JanusGraph_-_data_model.html"},{"title":"图解JanusGraph系列-JanusGraph指标监控报警（Monitoring JanusGraph）","text":"本文详细介绍JanusGraph的监控机制；大家好，我是洋仔，JanusGraph图解系列文章，实时更新~ 图数据库文章总目录： 整理所有图相关文章，请移步(超链)：图数据库系列-文章总目录 地址：https://liyangyang.blog.csdn.net/article/details/111031257 源码分析相关可查看github（码文不易，求个star~）： https://github.com/YYDreamer/janusgraph 转载文章请保留以下声明： 作者：洋仔聊编程、微信公众号：匠心Java、原文地址：https://liyangyang.blog.csdn.net/ 正文JanusGraph框架提供了一些可监控的指标，用于我们在使用janus图数据库时可以对一些指标进行监控，下面我们看下如何配置使用Janusgraph监控！ 本文主要讲解了3部分： 监控的指标类型和配置 监控指标数据展示存储的位置（Reporter） 实战应用案例，并对打印出的指标进行了分析 最后给出一个监控设计的架构图 一：监控的底层实现JanusGraph通过支持Metrics来实现指标数据收集，什么是Metrics? Metrics是框架Dropwizard提供的一个lib包，主要用于项目指标的收集作用，JanusGraph就是基于Metrics这个组件开发的指标收集模块； Dropwizard是一个Java框架，用于开发对操作友好的高性能RESTful Web服务，将来自Java生态系统的稳定，成熟的库汇集到一个简单的程序包中，使我们可以专注于完成工作。 Dropwizard对复杂的配置，应用程序指标，日志记录，操作工具等提供了开箱即用的支持； 二：JanusGraph中的指标JanusGraph可以收集以下指标： begin，commit和 roll back的事务数 每种存储后端操作类型的 请求次数 和 失败次数 每种存储后端操作类型的响应时间分布 2.1 配置指标收集要启用指标标准收集，需要在JanusGraph的属性文件中设置以下内容： 12# Required to enable Metrics in JanusGraphmetrics.enabled = true 此设置使JanusGraph在运行时使用计时器、计数器、直方图等Metrics类记录测量结果。 自定义默认指标名称默认情况下，JanusGraph为所有度量标准名称添加“ org.janusgraph”前缀。可以通过metrics.prefix配置属性设置此前缀。例如，将默认的“ org.janusgraph”前缀缩短为“ janusgraph”： 12# Optionalmetrics.prefix = janusgraph 特定事务指标名称每个JanusGraph事务都可以选择指定其自己的指标名称前缀，从而覆盖默认的指标名称前缀和 metrics.prefix配置属性。例如，可以将前缀更改为打开JanusGraph事务的前端应用程序的名称。 请注意，Metrics在内存中维护度量标准名称及其相关对象的ConcurrentHashMap，因此，保持不同度量标准前缀的数量较小可能是个好主意。 下面使用案例： 123JanusGraph graph = ...; // 获取图实例连接TransactionBuilder tbuilder = graph.buildTransaction(); // 开启一个事务构建器JanusGraphTransaction tx = tbuilder.groupName(&quot;foobar&quot;).start(); // 开启一个事务，并开启指标收集，Metrics前缀为foobar 下面为groupName的方法定义： 12345678 * Sets the name prefix used for Metrics recorded by this transaction. If * metrics is enabled via {@link GraphDatabaseConfiguration#BASIC_METRICS}, * this string will be prepended to all JanusGraph metric names. * * @param name Metric name prefix for this transaction * @return Object containing transaction prefix name property */TransactionBuilder groupName(String name); 分开指标统计JanusGraph在默认情况下组合了其各种内部存储后端句柄的指标，也就是说：会将所有的操作统一收集为一种类型指标； 存储后端交互的所有指标标准都遵循“ &lt;prefix&gt; .stores.&lt;opname&gt;”模式，无论它们是否是idStore，edgeStore的操作等； 如果想要分开每种操作类型收集的指标，配置以下参数： 1metrics.merge-basic-metrics = false metrics.merge-basic-metrics = false在JanusGraph的属性文件中进行设置时，指标标准名称“stores”将被替换为对应“ idStore”，“ edgeStore”，“ vertexIndexStore”或“ edgeIndexStore”，如下述： 1234&lt;prefix&gt;.idStore.&lt;opname&gt;&lt;prefix&gt;.edgeStore.&lt;opname&gt;&lt;prefix&gt;.vertexIndexStore.&lt;opname&gt;&lt;prefix&gt;.edgeIndexStore.&lt;opname&gt; 2.2 配置指标报告要访问这些收集好的指标值，必须配置一个或多个Metrics的Reporting； 也就是说配置一个或多个指标的输出存储的位置； JanusGraph 支持下述的这7种 Metrics reporters: Console CSV Ganglia Graphite JMX Slf4j User-provided/Custom 每种reporter类型独立于其他reporter，并且可以共存。 例如，可以将Ganglia、JMX和Slf4j Metrics报告器配置为同时运行，只需在janusgraph.properties中设置它们各自的配置键即可（并启用metrics.enabled = true） Console Reporter 配置键 是否必须？ 值 默认 metrics.console.interval 是 将指标转储到控制台之间需要等待的毫秒数 空值 示例janusgraph.properties片段，每分钟将指标输出到控制台一次： 123metrics.enabled = true# Required; specify logging interval in millisecondsmetrics.console.interval = 60000 CSV文件 Reporter 配置键 是否必须？ 值 默认 metrics.csv.interval 是 写入CSV行之间需要等待的毫秒数 空值 metrics.csv.directory 是 写入CSV文件的目录（如果不存在则将创建） 空值 示例janusgraph.properties片段，每分钟将CSV文件写入一次到目录./foo/bar/（相对于进程的工作目录）： 1234metrics.enabled = true# Required; specify logging interval in millisecondsmetrics.csv.interval = 60000metrics.csv.directory = foo/bar Ganglia Reporter注意 由于Ganglia的LGPL许可与JanusGraph的Apache 2.0许可冲突，因此配置Ganglia需要一个附加的库，该库未与JanusGraph一起打包。要使用Ganglia监视运行，请org.acplt:oncrpc从此处下载 jar 并将其复制到JanusGraph/lib目录，然后再启动服务器。 配置键 是否必须？ 值 默认 metrics.ganglia.hostname 是 将指标发送到的单播主机或多播组 空值 metrics.ganglia.interval 是 发送数据报之间等待的毫秒数 空值 metrics.ganglia.port 否 我们向其发送指标数据报的UDP端口 8649 metrics.ganglia.addressing-mode 否 必须为“unicast”或“multicast” unicast metrics.ganglia.ttl 否 组播数据报TTL; 忽略单播 1个 metrics.ganglia.protocol-31 否 布尔值 使用Ganglia协议3.1为true，使用3.0为false true metrics.ganglia.uuid 否 要报告而不是IP：主机名的主机UUID 空值 metrics.ganglia.spoof 否 覆盖IP：向Ganglia报告的主机名 空值 示例janusgraph.properties片段，每30秒发送一次单播UDP数据报到默认端口上的localhost： 12345metrics.enabled = true# Required; IP or hostname stringmetrics.ganglia.hostname = 127.0.0.1# Required; specify logging interval in millisecondsmetrics.ganglia.interval = 30000 示例janusgraph.properties片段，将单播UDP数据报发送到非默认目标端口，并且还配置报告给Ganglia的IP和主机名： 12345678metrics.enabled = true# Required; IP or hostname stringmetrics.ganglia.hostname = 1.2.3.4# Required; specify logging interval in millisecondsmetrics.ganglia.interval = 60000# Optionalmetrics.ganglia.port = 6789metrics.ganglia.spoof = 10.0.0.1:zombo.com Graphite Reporter 配置键 是否必须？ 值 默认 metrics.graphite.hostname 是 将Graphite纯文本协议数据发送到的IP地址或主机名 空值 metrics.graphite.interval 是 将数据推送到Graphite之间需要等待的毫秒数 空值 metrics.graphite.port 否 Graphite纯文本协议报告发送到的端口 2003 metrics.graphite.prefix 否 发送到Graphite的所有度量标准名称前都带有任意字符串 空值 每分钟将指标发送到192.168.0.1上的Graphite服务器的示例janusgraph.properties片段： 12345metrics.enabled = true# Required; IP or hostname stringmetrics.graphite.hostname = 192.168.0.1# Required; specify logging interval in millisecondsmetrics.graphite.interval = 60000 JMX Reporter 配置键 是否必须？ 值 默认 metrics.jmx.enabled 是 布尔型 false metrics.jmx.domain 否 指标将显示在此JMX域中 Metrics’s own default metrics.jmx.agentid 否 指标将使用此JMX代理ID报告 Metrics’s own default janusgraph.properties示例片段： 123456metrics.enabled = true# Requiredmetrics.jmx.enabled = true# Optional; if omitted, then Metrics uses its default valuesmetrics.jmx.domain = foometrics.jmx.agentid = baz Slf4j Reporter 配置键 是否必须？ 值 默认 metrics.slf4j.interval 是 将指标转储到记录器之间需要等待的毫秒数 空值 metrics.slf4j.logger 否 要使用的Slf4j记录器名称 “metrics” 示例janusgraph.properties片段每分钟将一次指标记录到名为的记录器中foo： 12345metrics.enabled = true# Required; specify logging interval in millisecondsmetrics.slf4j.interval = 60000# Optional; uses Metrics default when unsetmetrics.slf4j.logger = foo 用户自定义 Reporter如果上面列出的Metrics报告程序配置选项不足以支持我们当前的业务，JanusGraph提供了一个实用方法来访问单个MetricRegistry实例，该实例保存了它的所有度量； 使用方法如下： 12com.codahale.metrics.MetricRegistry janusgraphRegistry = org.janusgraph.util.stats.MetricManager.INSTANCE.getRegistry(); 以这种方式访问janusgraphRegistry的代码可以将非标准报告类型或具有外来配置的标准报告类型附加到janusgraphRegistry。 如果周围的应用程序已经有了度量报告器配置的框架，或者如果应用程序需要JanusGraph支持的报告器类型的多个不同配置的实例，这种方法也很有用。 例如，可以使用这种方法来设置多个Graphite Reporter，而JanusGraph的属性配置仅限于一个Graphite Reporter。 三：实际应用配置如下： 开启指标收集 使用Console Reporter，配置指标打印在Console中，时间间隔为1分钟 关闭指标收集merge操作 配置指标收集前缀由默认的org.janusgraph修改为myprefix 完整具体文件配置如下： 12345678910111213141516171819202122232425262728# 其他配置gremlin.graph=org.janusgraph.core.JanusGraphFactorystorage.backend=hbasestorage.hostname=127.0.0.1storage.port=2184storage.hbase.table=testGraphcache.db-cache=truecache.db-cache-clean-wait=20cache.db-cache-time=180000cache.db-cache-size=0.5index.search.backend=elasticsearchindex.search.hostname=127.0.0.1index.search.index-name=searchindex.search.port=9200index.search.elasticsearch.http.auth.type=basicindex.search.elasticsearch.http.auth.basic.username=testindex.search.elasticsearch.http.auth.basic.password=testquery.batch=truequery.batch-property-prefetch=true# janusgraph监控相关配置metrics.enabled = truemetrics.prefix = myprefixmetrics.console.interval = 60000metrics.merge-basic-metrics = false 我们在服务器使用gremlin.sh脚本启动gremlin console，并创建图实例： 123./gremlin.shgraph=JanusGraphFactory.open('/opt/soft/janusgraph-0.5.1-test/conf/janusgraph-hbase-es.properties') 我们就会发现，每隔一分钟就会在控制台打印出对应指标信息，因为指标信息很多，为了便于文章阅读，下述只展示出主要部分： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210312/23/20 10:37:28 AM ===========================================================-- Counters --------------------------------------------------------------------global.storeManager.openDatabase.calls count = 20global.storeManager.startTransaction.calls count = 461myprefix.caches.misses count = 5// 此处省略部分统计myprefix.tx.begin count = 12myprefix.tx.commit count = 3myprefix.tx.rollback count = 9org.janusgraph.caches.misses count = 3org.janusgraph.caches.retrievals count = 3// 此处省略部分统计org.janusgraph.sys.stores.getSlice.calls count = 286org.janusgraph.sys.stores.getSlice.entries-returned count = 231org.janusgraph.sys.stores.mutate.calls count = 7org.janusgraph.tx.begin count = 1-- Histograms ------------------------------------------------------------------myprefix.stores.getSlice.entries-histogram count = 7 min = 0 max = 8 mean = 1.57 stddev = 2.88 median = 1.00 75% &lt;= 1.00 95% &lt;= 8.00 98% &lt;= 8.00 99% &lt;= 8.00 99.9% &lt;= 8.00org.janusgraph.stores.getSlice.entries-histogram // 具体指标统计省略org.janusgraph.sys.schema.stores.getSlice.entries-histogram // 具体指标统计省略org.janusgraph.sys.stores.getSlice.entries-histogram // 具体指标统计省略-- Timers ----------------------------------------------------------------------myprefix.query.graph.execute.time count = 2 mean rate = 0.01 calls/second 1-minute rate = 0.01 calls/second 5-minute rate = 0.07 calls/second 15-minute rate = 0.14 calls/second min = 1.19 milliseconds max = 2.25 milliseconds mean = 1.72 milliseconds stddev = 0.75 milliseconds median = 1.72 milliseconds 75% &lt;= 2.25 milliseconds 95% &lt;= 2.25 milliseconds 98% &lt;= 2.25 milliseconds 99% &lt;= 2.25 milliseconds 99.9% &lt;= 2.25 millisecondsmyprefix.query.graph.getNew.time // 具体指标统计省略myprefix.query.graph.hasDeletions.time // 具体指标统计省略myprefix.query.vertex.execute.time // 具体指标统计省略myprefix.query.vertex.getNew.time // 具体指标统计省略myprefix.query.vertex.hasDeletions.time // 具体指标统计省略myprefix.storeManager.mutate.time // 具体指标统计省略myprefix.stores.getSlice.time // 具体指标统计省略myprefix.stores.mutate.time // 具体指标统计省略org.janusgraph.query.graph.execute.time count = 2 mean rate = 0.00 calls/second 1-minute rate = 0.00 calls/second 5-minute rate = 0.02 calls/second 15-minute rate = 0.09 calls/second min = 2.52 milliseconds max = 221.12 milliseconds mean = 111.82 milliseconds stddev = 154.57 milliseconds median = 111.82 milliseconds 75% &lt;= 221.12 milliseconds 95% &lt;= 221.12 milliseconds 98% &lt;= 221.12 milliseconds 99% &lt;= 221.12 milliseconds 99.9% &lt;= 221.12 millisecondsorg.janusgraph.query.graph.getNew.time // 具体指标统计省略org.janusgraph.query.graph.hasDeletions.time // 具体指标统计省略org.janusgraph.stores.getKeys.iterator.hasNext.time // 具体指标统计省略org.janusgraph.stores.getKeys.iterator.next.time // 具体指标统计省略org.janusgraph.stores.getKeys.time // 具体指标统计省略 // 具体指标统计省略org.janusgraph.stores.getSlice.time // 具体指标统计省略org.janusgraph.sys.schema.query.graph.execute.time // 具体指标统计省略org.janusgraph.sys.schema.query.graph.getNew.time // 具体指标统计省略org.janusgraph.sys.schema.query.graph.hasDeletions.time // 具体指标统计省略org.janusgraph.sys.schema.stores.getSlice.time // 具体指标统计省略org.janusgraph.sys.storeManager.mutate.time // 具体指标统计省略org.janusgraph.sys.stores.getSlice.time // 具体指标统计省略org.janusgraph.sys.stores.mutate.time // 具体指标统计省略 观察上述指标统计数据，我们可以发现，主要分为4大部分： 当前统计指标时间，年月日，时间精确到秒 统计所有不同操作的数量Counters 统计数据直方图的表示Histograms 统计所有操作的执行时间Timers 第一部分：当前统计指标时间，年月日，时间精确到秒 可以作为janusgraph指标监控的横向维度和时间维度 第二部分：统计所有不同操作的数量Counters 这一部分主要用于收集所有操作的总数量，包含事务相关统计、缓存命中相关统计、数据的CURD统计、后端存储不同操作调用统计等48个维度！ 具体可以自行尝试； 第三部分：统计数据直方图的表示Histograms 首先，什么是直方图？ 直方图，形状类似柱状图却有着与柱状图完全不同的含义。直方图牵涉统计学的概念，首先要对数据进行分组，然后统计每个分组内数据元的数量。 在平面直角坐标系中，横轴标出每个组的端点，纵轴表示频数，每个矩形的高代表对应的频数，称这样的统计图为频数分布直方图 此部分主要涉及对后端存储的统计； 第四部分：统计所有操作的执行时间Timers 这一部分主要收集统计数据CURD操作和数据插入等操作的时间统计包含23种维度！ 时间维度包含平均值、最大值、最小值、时间分布等14个时间维度； 四：监控架构图 整体分为： 收集指标 –&gt; 解析指标数据 –&gt; 格式化存储 –&gt; 监控组件|报警组件 有任何问题，欢迎交流沟通 参考：JanusGraph官网","link":"/article/%E5%9B%BE%E8%A7%A3JanusGraph%E7%B3%BB%E5%88%97-JanusGraph%E6%8C%87%E6%A0%87%E7%9B%91%E6%8E%A7%E6%8A%A5%E8%AD%A6%EF%BC%88Monitoring_JanusGraph%EF%BC%89.html"},{"title":"图解JanusGraph系列 - 关于JanusGraph图数据批量快速导入的方案和想法（bulk load data）","text":"JanusGraph的批量导入速度一直是用户使用的痛点， 下面会依托官网的介绍和个人理解，聊一下关于图数据批量快速导入的一些方案、方案使用场景和一些想法； 大家好，我是洋仔，JanusGraph图解系列文章，实时更新~ 图数据库文章总目录： 整理所有图相关文章，请移步(超链)：图数据库系列-文章总目录 地址：https://liyangyang.blog.csdn.net/article/details/111031257 源码分析相关可查看github（码文不易，求个star~）： https://github.com/YYDreamer/janusgraph 版本：JanusGraph-0.5.2 转载文章请保留以下声明： 作者：洋仔聊编程 微信公众号：匠心Java 原文地址：https://liyangyang.blog.csdn.net/ 前言JanusGraph的批量导入速度一直是用户使用的痛点， 下面会依托官网的介绍和个人理解，聊一下关于图数据批量快速导入的一些方案、方案使用场景和一些想法； 写这篇文章的目的主要是为了让大家了解一下janus的导入的一些常用方案，算是一个总结吧，如有疑问或者文章错误，欢迎留言联系我 首先，说一下JanusGraph的批量导入的可配置的优化配置选项 和 基于第三方存储和索引的优化配置选项： 批量导入的配置选项 第三方存储后端的优化选项（Hbase为例） 第三方索引后端的优化选项（ES为例） 之后分析一下数据导入的五个方案： 基于JanusGraph Api的批量导入 基于Gremlin Server的批量导入 使用JanusGraph-utils的批量导入 基于bulk loader 导入方式 基于抽取序列化逻辑生成Hfile离线批量导入 最后聊一下关于批量导入的一些想法； 一：批量导入的优化配置选项1、批量导入的配置选项JanusGraph中有许多配置选项和工具可以将大量的图数据更有效地导入。这种导入称为批量加载，与默认的事务性加载相反，默认的事务性加载单个事务只会添加少量数据。 下述介绍了配置选项和工具，这些工具和工具使JanusGraph中的批量加载更加高效。 在继续操作之前，请仔细遵守每个选项的限制和假设，以免丢失数据或损坏数据。 配置选项 janusgraph支持批量导入，可通过相关配置项设置 下面具体看一下对应配置项的详细作用： 批量加载1) 配置项：storage.batch-loading 启用该配置项，相当于打开了JanusGraph的批量导入开关； 影响： 启用批处理加载会在许多地方禁用JanusGraph内部一致性检查，重要的是会禁用lock锁定来保证分布式一致性；JanusGraph假定要加载到JanusGraph中的数据与图形一致，因此出于性能考虑禁用了自己的一致性检查。 换句话说，我们要在导入数据之前，保证要导入的数据和图中已有的数据不会产生冲突也就是保持一致性！ 为什么要禁用一致性检查来提升性能？ 在许多批量加载方案中，在加载数据之前确保去数据一致性，然后在将数据加载到数据库比在加载数据到图库时确保数据一致性，消耗的成本要便宜的多。 例如，将现有用户数据文件批量加载到JanusGraph中的用例：假设用户名属性键具有定义的唯一复合索引，即用户名在整个图中必须是唯一的。 那么按名称对数据文件进行排序并过滤出重复项或编写执行此类过滤的Hadoop作业消耗的时间成本，就会比开启一致性检查在导入图数据时janusgraph检查花费的成本要少的多。 基于上述，我们可以启用 storage.batch-loading配置，从而大大减少了批量加载时间，因为JanusGraph不必检查每个添加的用户该名称是否已存在于数据库中。 重要提示： 启用storage.batch-loading要求用户确保加载的数据在内部是一致的，并且与图中已存在的任何数据一致。 特别是，启用批处理加载时，并发类型创建会导致严重的数据完整性问题。因此，我们强烈建议通过schema.default = none在图形配置中进行设置来禁用自动类型创建。 优化ID分配1、ID块大小 1）配置项：ids.block-size 该配置项为配置在分布式id的生成过程中每次获取 id block的大小； 分布式id相关具体可看文章《图解Janusgraph系列-分布式id生成策略分析》 原理： 每个新添加的顶点或边都分配有唯一的ID。JanusGraph的ID池管理器以block的形式获取特定JanusGraph实例的ID。id块获取过程很昂贵，因为它需要保证块的全局唯一分配。 增加 ids.block-size会减少获取次数，但可能会使许多ID未被分配，从而造成浪费。对于事务性工作负载，默认块大小是合理的，但是在批量加载期间，顶点和边的添加要频繁得多，而且要快速连续。 因此，通常建议将块大小增加10倍或更多，具体取决于每台机器要添加的顶点数量。 经验法则： 设置ids.block-size为您希望每小时为每个JanusGraph实例添加的顶点数。 重要提示： 必须为所有JanusGraph实例配置相同的值，ids.block-size以确保正确的ID分配。因此，在更改此值之前，请务必关闭所有JanusGraph实例。 2、ID Acquisition流程 当许多JanusGraph实例频繁并行分配id块时，不可避免地会出现实例之间的分配冲突，从而减慢了分配过程。 此外，由于大容量加载而导致的增加的写负载可能会使该过程进一步减慢到JanusGraph认为失败并引发异常的程度。可以调整2个配置选项来避免这种情况； 1）配置项：ids.authority.wait-time 配置ID池管理器等待应用程序获取ID块被存储后端确认的时间（以毫秒为单位）。这段时间越短，应用程序在拥挤的存储群集上发生故障的可能性就越大。 经验法则： 将其设置为负载下存储后端集群上测量的第95百分位读写时间的总和。 重要说明： 所有JanusGraph实例的该值都应该相同。 2）配置项：ids.renew-timeout 配置JanusGraph的ID池管理器在尝试获取新的ID块总共等待的毫秒数。 经验法则： 将此值设置为尽可能大，不必为不可恢复的故障等待太久。增加它的唯一缺点是JanusGraph将在不可用的存储后端群集上尝试更长的时间 优化读写1、缓冲区大小 JanusGraph在数据导入时存在一个缓冲区，用来缓冲当前事务的部分请求，从而可以小批量的写入和执行，从而减少针对存储后端的请求数。在短时间内执行大量写操作时，存储后端可能会因为大量的写请求打入而变得超负荷； 配置项：storage.buffer-size 这些批次的大小由storage.buffer-size来控制。 增加storage.buffer-size可以通过增加缓冲区大小，来使得批次保存更多的请求，从而减少写请求的次数来避免上述失败。 注意： 增加缓冲区大小会增加写请求的等待时间及其失败的可能性。因此，不建议为事务性负载增加此设置，并且应该在批量加载期间仔细尝试此设置的一个合适的值。 2、读写健壮性 在批量加载期间，群集上的负载通常会增加，从而使读和写操作失败的可能性更大（尤其是如上所述，如果缓冲区大小增加了）。 1）配置项：storage.read-attempts 该配置项配置JanusGraph在放弃之前尝试对存储后端执行读取或写入操作的次数。 2）配置项：storage.attempt-wait 该配置项指定JanusGraph在重新尝试失败的后端操作之前将等待的毫秒数。较高的值可以确保重试操作不会进一步增加后端的负载。 注意： 如果在批量加载期间后端上可能会有很高的负载，通常建议增加这些配置选项。 其他1）配置项：storage.read-attempts 2、第三方存储后端的优化选项针对于第三方存储的优化分为两部分： 第三方存储集群自身的优化 JanusGraph结合第三方存储的优化选项 集群自身的优化集群自身的优化，本文主要介绍janusgraph相关优化这里就不多说这部分了，主要是提升hbase集群的读写能力； 这里主要还是关注的Hbase的写数据能力优化后的提升！这部分的优化至关重要！ 下面举几个例子： 1）配置项： hbase.client.write.buffer 设置buffer的容量 HBase Client会在数据累积到设置的阈值后才提交Region Server。这样做的好处在于可以减少RPC连接次数。 计算一下服务端因此而消耗的内存：hbase.client.write.buffer * hbase.regionserver.handler.count从在减少PRC次数和增加服务器端内存之间找到平衡点。 2）配置项： hbase.regionserver.handler.count 定义每个Region Server上的RPC Handler的数量 Region Server通过RPC Handler接收外部请求并加以处理。所以提升RPC Handler的数量可以一定程度上提高HBase接收请求的能力。 当然，handler数量也不是越大越好，这要取决于节点的硬件情况。 等等各种配置项 3）针对一些CF、RowKey设计之类的优化点，因为这些都是janus预设好的，所以在janusGraph中使用不到； JanusGraph针对优化针对于JanusGraph+第三方存储的优化，官网(配置项文档超链接) 给出了一些配置选项，可从其找出对应的配置项； 针对于hbase，我在配置项中找出了对应的一些可能有作用的配置如下： 1）配置项： storage.hbase.compression-algorithm hbase存储数据压缩算法的配置，我们在《图解图库JanusGraph系列-一文知晓“图数据“底层存储结构》文章中提到有好几个地方都是压缩存储的，此处就是配置的压缩算法； 类型: 枚举值，支持 lzo、gz、snappy、lz4、bzip2、zstd五种压缩算法 和 不压缩配置：none 默认值： gz压缩； 注意：此处配置的算法需要hbase也支持才可以！ 如果存储空间足够，可以考虑配置为不压缩，也会提升导入速率！ 2）配置项：storage.hbase.skip-schema-check 假设JanusGraph的HBase表和列族已经存在。 如果是这样，JanusGraph将不会检查其 table/ CF 的存在，也不会在任何情况下尝试创建它们。 类型： 布尔值 默认值： false，检查 注意： 可以在数据导入时，将该配置项设置为true，去除table/ CF的检查，这个其实作用不大；因为都是在初始化图实例的时候就去检查了。。 3、第三方索引后端的优化选项针对于第三方存索引的优化分为两部分： 第三方索引集群自身的优化 JanusGraph结合第三方索引的优化选项 集群自身的优化集群自身的优化，本文主要介绍janusgraph相关优化这里就不多说这部分了，主要是提升索引集群的读写能力； 这里主要还是关注的索引的写数据能力优化后的提升！这部分的优化至关重要！ 例如es的线程池参数优化等 JanusGraph针对优化针对于JanusGraph+第三方索引的优化，官网(配置项文档超链接) 给出了一些配置选项，可从其找出对应的配置项； 针对于es，我在配置项中找出了对应的一些可能有作用的配置如下： 1）配置项： index.[X].elasticsearch.retry_on_conflict 指定在发生冲突时应重试操作多少次。 类型: 整数 默认值： 0次 注意： 增大该值可以提升在批量导入中，发生冲突后解决冲突的几率 3、JVM的优化JanusGraph基于Java语言编写，则毋庸置疑会用到JVM 对JVM的调优也主要集中到垃圾收集器和堆内存的调优 堆大小调整： 我们在导入图数据时会产生大量的临时数据，这里需要我们调整一个合适的堆空间； 推荐至少为8G 垃圾收集器调优： 如果在使用CMS发现GC过于频繁的话，我们可以考虑将垃圾收集器设置为：G1 这个收集器适用于大堆空间的垃圾收集，有效的减少垃圾收集消耗的时间； 注意： 此处的JVM调优设计JanusGraph java api项目 和 gremlin server部分的JVM调优； 二：基于数据层面的优化2.1 拆分图 并发执行在某些情况下，图数据可以分解为多个断开连接的子图。这些子图可以跨多台机器独立地并行加载；不管是采用下述的那种方式加载都可以； 这里有一个前提： 底层第三方存储集群的处理能力没有达到最大； 如果底层存储集群当前的平均cpu已经是80 90%的了，就算拆分多个图也没用，底层存储的处理能力已经被限制住当前的速度了； 这个方式官网上提了一句，这个地方其实很难可以将图拆分为断开的子图，并且针对于拆分为多个子图来说，主要还是依托于底层存储集群的处理能力； 一般情况下，不用拆分图进行一个好的优化后，底层存储集群的处理能力都可以完全调用起来； 2.2 分步骤 并发执行如果无法分解图形，则分多个步骤加载通常是有益的，也就是将vertex 和 edge 分开导入； 这种方式，需要数据同学做好充分的数据探查，不然可能会产生数据不一致的情况！ 下面是步骤（其中最后两个步骤可以在多台计算机上并行执行）： 前提： 确保vertex和edge数据集 删除了重复数据 并且是一致的 环境配置： 设置batch-loading=true 并且优化上述介绍的其他选项 vertex全量导入： 将所有的vertex及节点对应的property添加到图中。维护一份从顶点ID（由加载的数据用户自定义）到JanusGraph的内部顶点分布式一致性ID（即vertex.getId()）的映射，该ID 为64位长 edge全量导入： 使用映射添加所有的边 来查找JanusGraph的顶点id 并使用该id检索顶点。 讲述过程： 假设存在3个用户，“-”号后为对应的自定义的顶点id值（注意，非导入图库中的顶点id，只是标识当前节点的业务id）： 123user1-1user2-2user3-3 上述第三步，我们将这些节点导入到图库中！ 产生一个业务id 与 图库中节点的分布式唯一id的对应关系如下： 我们在导入一个点后，janus会返回一个vertex实例对象，通过这个对象就可以拿到对应的图库vertexId 1234业务id-图库中节点id1-42612-42743-4351 注意：这一步骤，我们可以多线程并行导入而无需担心一致性问题，因为节点全部唯一 节点导入完成！ 假设存在对应的有3条边如下， 123edge1：user1 --&gt; user2 edge2：user1 --&gt; user3edge3：user2 --&gt; user3 我们通过user1对应业务id：1，而业务id：1对应节点id：4261，我们就可以转化为下述对应关系： 1234261 --&gt; 42744261 --&gt; 43514274 --&gt; 4351 在JanusGraph中通过节点id查询节点，是获取节点的最快方式！！ 我们就可以通过id获取图库中对应的vertex对象实例，然后使用addVertex将edge导入！ 注意：这一步骤，我们可以多线程并行导入而无需担心一致性问题，因为edge全部唯一 第三个步骤和第四个步骤也可以并行执行，我们在导入点的过程中，可以也同时将源节点和目标节点已经导入到图库中的edge同步入图； 三：批量导入方案下述介绍一下5种导入方案，其中包含3种批量导入方案； 3.1 方案一：基于JanusGraph Api的数据导入该方案可以整合上述第二部分二：基于数据层面的优化； 涉及方法： 12public JanusGraphVertex addVertex(Object... keyValues);public JanusGraphEdge addEdge(String label, Vertex vertex, Object... keyValues); 在janusGraph的业务项目中，可以开发一个数据导入模块，使用提供的类似于java api等，进行数据的导入； 流程： 这种是最简单的方案，具体的细节，这里就不给出了，节点导入大体流程为下述： 获取图实例 获取图实例事务对象 插入节点 提交事务 边导入大体流程如下： 获取图实例 获取图实例事务对象 查询源节点 + 目标节点（这个地方可能是性能瓶颈） 在两个节点中插入边 提交事务 主要作用： 此方案可以用于数据量较小的情况下使用，例如每天的增量导入等； 优化点： 1、批量事务提交 此处的事务提交，我们可以通过一个常用的优化手段： 处理多个vertex 或者 edge后再提交事务！ 可以减少janus与底层存储的交互，减少网络消耗和连接数，提升导入的性能！ 处理的个数多少主要还是和底层存储集群相关，几百还是几千这就需要自己调试获取当前环境下的最优配置了 注意： 如果开启了上述提到的storage.batch-loading，则需要你们现在的环境下注意一致性的问题； 例如图库中原本存在一个a节点，你又插入一个a节点，便会有一致性问题； 我们可以通过插入数据前，先通过唯一索引查询节点，节点存在则更新节点，不存在则插入节点； 3.2 方案二：基于Gremlin Server的批量导入该方案可以整合上述第二部分二：基于数据层面的优化； 这里需要我们搭建一个Gremlin server服务器，通过在服务器执行gremlin-server.sh即可，暴露出一个tcp接口； 则可以将对应的gremlin 语句提交到对应的gremlin服务器执行； 具体的流程和第一个方案一致 优化点： 同上一个方案优化点1； 3、gremlin server池参数调整 除了上述给定的一些配置的优化项，还有两个gremlin server的优化项需要调整 threadPoolWorke：最大2*core个数，用于处理非阻塞读写的Gremlin服务器可用的线程数； gremlinPool：用于在ScriptEngine中执行实际脚本的“Gremlin”线程的数量。此池表示Gremlin服务器中可用于处理阻塞操作的工作者； 和线程池调优一样，要找出最合适的一个值，太小不好，太大也不好； 注意： 该方案本质上和第一个方案类似，只不过是一个是通过给定的java api提交插入请求，一个直接通过gremlin语句提交插入请求到gremlin server； 3.3 方案三：IBM的janusgraph-utils这个方案没用过，简单看了一下，这个主要也是通过多线程对数据进行导入； 自己手动组装对应的schema文件，将schema导入到数据库； 然后将组装为特定格式的csv文件中的数据，导入到图库中； github地址： https://github.com/IBM/janusgraph-utils 优点： 1、使用难度不高，让我们不用再去手写多线程的导入了；减少工作量 2、直连hbase和es，相对于前两种减少了对应的gremlin server和janus server的网络交互 3、支持通过配置文件自动创建Janusgraph schema和index 4、可配置化的线程池大小和每次批量提交的数量 问题： 1、schema和csv文件也是要用户组装出对应格式 2、相对于前两种方式性能提升有限，主要是少了一层网络交互。多线程和批量提交，前两种都可以手动去实现；还需要引入一个新的组件 3、支持janus版本较低，可以手动升级，不难 4、相对于下面两种方案，性能还是较低 3.4 方案四：bulk loader官方提供的批量导入方式；需要hadoop集群和spark集群的支持； hadoop和spark集群配置，可以看官网：https://docs.janusgraph.org/advanced-topics/hadoop/ 该方案对导入的数据有着严格的要求，支持多种数据格式：json、csv、xml、kryo； 数据要求： 节点、节点对应的属性、节点对应的边需要在一行中（一个json中、一个xml项中） 数据案例： 下面给一下官网的案例，在data目录下： 12345678-- json格式{&quot;id&quot;:2,&quot;label&quot;:&quot;song&quot;,&quot;inE&quot;:{&quot;followedBy&quot;:[{&quot;id&quot;:0,&quot;outV&quot;:1,&quot;properties&quot;:{&quot;weight&quot;:1}},{&quot;id&quot;:323,&quot;outV&quot;:34,&quot;properties&quot;:{&quot;weight&quot;:1}}]},&quot;outE&quot;:{&quot;followedBy&quot;:[{&quot;id&quot;:6190,&quot;inV&quot;:123,&quot;properties&quot;:{&quot;weight&quot;:1}},{&quot;id&quot;:6191,&quot;inV&quot;:50,&quot;properties&quot;:{&quot;weight&quot;:1}}],&quot;sungBy&quot;:[{&quot;id&quot;:7666,&quot;inV&quot;:525}],&quot;writtenBy&quot;:[{&quot;id&quot;:7665,&quot;inV&quot;:525}]},&quot;properties&quot;:{&quot;name&quot;:[{&quot;id&quot;:3,&quot;value&quot;:&quot;IM A MAN&quot;}],&quot;songType&quot;:[{&quot;id&quot;:5,&quot;value&quot;:&quot;cover&quot;}],&quot;performances&quot;:[{&quot;id&quot;:4,&quot;value&quot;:1}]}}-- xml格式&lt;node id=&quot;4&quot;&gt;&lt;data key=&quot;labelV&quot;&gt;song&lt;/data&gt;&lt;data key=&quot;name&quot;&gt;BERTHA&lt;/data&gt;&lt;data key=&quot;songType&quot;&gt;original&lt;/data&gt;&lt;data key=&quot;performances&quot;&gt;394&lt;/data&gt;&lt;/node&gt;&lt;node id=&quot;5&quot;&gt;&lt;data key=&quot;labelV&quot;&gt;song&lt;/data&gt;&lt;data key=&quot;name&quot;&gt;GOING DOWN THE ROAD FEELING BAD&lt;/data&gt;&lt;data key=&quot;songType&quot;&gt;cover&lt;/data&gt;&lt;data key=&quot;performances&quot;&gt;293&lt;/data&gt;&lt;/node&gt;&lt;node id=&quot;6&quot;&gt;&lt;data key=&quot;labelV&quot;&gt;song&lt;/data&gt;&lt;data key=&quot;name&quot;&gt;MONA&lt;/data&gt;&lt;data key=&quot;songType&quot;&gt;cover&lt;/data&gt;&lt;data key=&quot;performances&quot;&gt;1&lt;/data&gt;&lt;/node&gt;&lt;node id=&quot;7&quot;&gt;&lt;data key=&quot;labelV&quot;&gt;song&lt;/data&gt;&lt;data key=&quot;name&quot;&gt;WHERE HAVE THE HEROES GONE&lt;/data&gt;&lt;data key=&quot;songType&quot;&gt;&lt;/data&gt;&lt;data key=&quot;performances&quot;&gt;0&lt;/data&gt;&lt;/node&gt;-- csv格式2,song,IM A MAN,cover,1 followedBy,50,1|followedBy,123,1|sungBy,525|writtenBy,525 followedBy,1,1|followedBy,34,1 我们可以观察到，这其实是不容易构造的，节点属性边全部需要整合到一块； 数据整理方案： spark的cogroup， cogroup的作用就是将多个 RDD将相同的key jion成一行，从而使用csv格式进行导入，操作实示例如下： 123456789val rdd1 = sc.parallelize(Array((&quot;aa&quot;,1),(&quot;bb&quot;,2),(&quot;cc&quot;,6)))val rdd2 = sc.parallelize(Array((&quot;aa&quot;,3),(&quot;dd&quot;,4),(&quot;aa&quot;,5)))rdd1.cogroup(rdd2).collect()output:(aa,(CompactBuffer(1),CompactBuffer(3, 5)))(dd,(CompactBuffer(),CompactBuffer(4)))(bb,(CompactBuffer(2),CompactBuffer()))(cc,(CompactBuffer(6),CompactBuffer())) 这里大家可以参考360对这方面的处理，转化代码github地址：https://github.com/360jinrong/janusgraph-data-importer 注意： 此处的原始数据的准备需要细致，一致性保证完全依赖于原始数据的一致性保证； 3.5 方案五：基于抽取序列化逻辑的生成Hbase File离线批量导入博主在图库初始化时采用了这种方式，前前后后花费了接近一个月的时间，经过细致的验证，现已应用到生产环境使用，下面介绍一下对应的注意点和主要流程： 方案： 依据源码抽取出对应的序列化逻辑，分布式生成Hfile，将Hfile导入到Hbase； 问题： 人力成本过高，需要看源码抽逻辑，并且需要一个细致的验证； 方案难点： JanusGraph对于Hbase的数据底层格式，可以看我写的博客： 《图解图库JanusGraph系列-一文知晓“图数据“底层存储结构（JanusGraph data model）》 《图解Janusgraph系列-图数据底层序列化源码分析（Data Serialize）》 这两篇博客，一个分析了底层存储的格式，一个进行了相应的源码分析； 流程+验证+建议： 请看我写的另外一个博客：《图解JanusGraph系列-生成Hbase file离线批量导入方案》 这种方式，其实消耗的人力会比较大；另外，对于抽取的逻辑是否开源，这个后续我们会考虑这个问题，开源后地址会同步更新到本文章； 四：几种场景4.1 图库中已经存在数据如果图库中已经存在数据，对于3.4 方案四：bulk loader 和 3.5 方案五：基于抽取序列化逻辑的生成Hbase File离线批量导入 这两种方案可能就无法使用了； 我们可以采取两种方式： 使用第一种方案和第二种方案进行导入（注意数据一致性） 整体迁移图库，将图库中现有数据和将要导入的数据整体迁移到另外一个新图库，就可以使用4、5方案进行导入 4.2 图数据初始化或者迁移数据量小，建议使用3.1 方案一：基于JanusGraph Api的数据导入 和 3.2 方案二：基于Gremlin Server的批量导入 和 3.3 方案三：IBM的janusgraph-utils； 数据量大，建议使用3.4 方案四：bulk loader 和 3.5 方案五：基于抽取序列化逻辑的生成Hbase File离线批量导入； 4.3 单纯只看业务数据量选择什么方式导入，单纯基于业务数据量给一些个人建议： 小数据量（亿级以下）： 直接janusgraph api 或者 gremlin server导入即可，几小时就ok了； 如果想要更快可以使用另外的方式，只是会增加人力成本； 中等数据量（十亿级以下）：数据充分探查，开启storage.batch-loading完全可以支持，使用api，2天左右可以完成全量的数据导入 大数据量（百亿级数据）：推荐采用bulk load方式，配置hadoop集群，使用spark cluster导入 另一个方案：如果上述还是无法满足你们的需求，可以采用依据源码抽取序列化逻辑生成Hfile，然后离线导入到Hbase的方案，不过这种是花费人力成本最大的一种方式，不过效果也几乎是最好的，尤其是数据量越大效果越明显 总结数据的批量导入一直是JanusGraph让人难受的地方，经过本文的介绍大家应该有一个大体的认识，针对于百亿级的数据导入，上述的几种方案是可以支持的； 其他：批量导入后，每天的增量采用消息中间件接入JanusGraph api导入即可； 数据导入过程中，针对于不同的底层存储、不同的版本还是会有一些问题，具体的导入的坑大家可以加我v，邀你加群 注意！！！以上仅作为参考，有任何问题可评论或加博主v讨论 参考：JanusGaph官网https://www.jianshu.com/p/f372f0ef6c42https://www.jianshu.com/p/4b59c00a15de/","link":"/article/%E5%9B%BE%E8%A7%A3JanusGraph%E7%B3%BB%E5%88%97-%E5%85%B3%E4%BA%8EJanusGraph%E5%9B%BE%E6%95%B0%E6%8D%AE%E6%89%B9%E9%87%8F%E5%BF%AB%E9%80%9F%E5%AF%BC%E5%85%A5%E7%9A%84%E6%96%B9%E6%A1%88%E5%92%8C%E6%83%B3%E6%B3%95%EF%BC%88bulk_load_data%EF%BC%89.html"},{"title":"图解图库JanusGraph系列-一文搞定janusgraph图数据库的本地源码编译（janusgraph source code compile）","text":"源码分析 的第一步就是要先编译好源代码，才能进行debug跟踪流程查看，本文总结了janusgraph源码编译的全流程！ 大家好，我是洋仔，JanusGraph图解系列文章，实时更新~ 图数据库文章总目录： 整理所有图相关文章，请移步(超链)：图数据库系列-文章总目录 地址：https://liyangyang.blog.csdn.net/article/details/111031257 **源码分析相关可查看github（求star~~）**： https://github.com/YYDreamer/janusgraph 下述流程高清大图地址：https://www.processon.com/view/link/5f471b2e7d9c086b9903b629 版本：JanusGraph-0.5.2 转载文章请保留以下声明： 作者：洋仔聊编程微信公众号：匠心Java原文地址：https://liyangyang.blog.csdn.net/ 前言源码分析 的第一步就是要先编译好源代码，才能进行debug跟踪流程查看，本文总结了janusgraph源码编译的全流程！ 注意： 安装好环境之后, 推荐下载github：https://github.com/YYDreamer/janusgraph/tree/master这个地址的janusgraph代码！ 里面包含源码分析+编译错误的解决完成 主要介绍了janusgraph+hbase+es的本地源码编译过程，最后介绍了janusgraph+berkeleyje+es的编译过程。 源码已经上传个人github：https://github.com/YYDreamer/janusgraph 欢迎star和fork~ 本文所有的 janusgraph源码项目文件 + hadoop安装包 + hadoop在windows的辅助安装包 + hbase安装包 + es安装包全部整合放到了公众号“匠心Java”中，微信搜索“匠心Java”，回复“图库资源”四个字即可 一 本地安装依赖环境本机安装hbase环境，这里我后台存储使用的是 hbase，这里为了更好的符合正常的使用情况，没有用janusgraph自带的inmemory存储形式 ps：如果你在其他服务器有hbase和es环境，就不用自己本地搭建了，如果没有的话，我们在本地搭建一个自己的hbase和es，这种网上特别多，下面也给出了我自己搭建看的博客连接 1. 安装hadoop 并 启动hbase依赖于hadoop环境，所以我们在安装hbase前，先安装hadoop环境 安装：参考博客：hadoop安装教程 启动： 管理员方式打开cmd 转到对应的hadoop的sbin目录下，例如我的：D:\\app\\app_develop\\hadoop-2.7.7\\sbin 使用命令start-all* 出现四个黑框框：hadoop namenode、hadoop datanode、yarn resourcemanager、yarn nodemanager即可 访问http://localhost:50070，可以访问即为成功！ 2. 安装hbase 并 启动注意：在选择安装hbase的版本时，需要查看自己下载的janusgraph源码的版本支持hbase的什么版本 这里我们选择的是janusgraph-0.5.2版本，支持hbase-2.1.x版本，所以这里我下载的hbase-2.1.5版本 安装：参考博客：hbase安装教程 启动： 管理员方式打开cmd 转到对应的hadoop目录下，例如我的：D:\\app\\app_develop\\hbase-2.1.5\\bin 输入命令，回车start-hbase.cmd 出现一个黑框框，如下 在hbase的bin目录下，输入命令hbase shell，能成功连接hbase shell，即为成功！ 如果启动报错：java.lang.ClassNotFoundException: org.apache.htrace.core.HTraceConfiguration 解决方法： 将本地hbase-2.1.5目录下的lib\\client-facing-thirdparty目录下的htrace-core-3.1.0-incubating.jar和htrace-core4-4.2.0-incubating.jar赋值到路径lib下，重新start-hbase.cmd命令，启动成功！ 3. 安装es 并 启动安装：参考博客：es安装教程 启动： 管理员方式打开cmd 转到对应的hadoop目录下，例如我的：D:\\app\\app_develop\\elasticsearch-6.8.3\\bin 输入命令，回车elasticsearch.bat 输入完命令后，当前的黑框中会启动es，最终出现下述： 访问http://localhost:9200/，出现下述json即为成功！1234567891011121314151617{ &quot;name&quot; : &quot;99Xn4Vd&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;YpMeVAlzQRuzuVis1JzHpA&quot;, &quot;version&quot; : { &quot;number&quot; : &quot;6.8.3&quot;, &quot;build_flavor&quot; : &quot;default&quot;, &quot;build_type&quot; : &quot;zip&quot;, &quot;build_hash&quot; : &quot;0c48c0e&quot;, &quot;build_date&quot; : &quot;2019-08-29T19:05:24.312154Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;7.7.0&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot; }, &quot;tagline&quot; : &quot;You Know, for Search&quot;} 最终，我们使用jps 可以看到我们刚才启动的进程，如下图： 经过上述步骤，我们 在本地安装了hbase、es的环境并启动 二. 配置文件修改和添加对应的依赖1. 修改对应的配置文件上述代码使用的是hbase 和 es作为底层存储和索引后端的， 这里我们找到对应的源码中janusgraph-dist模块下的\\src\\assembly\\cfilter\\conf\\janusgraph-hbase-es.properties下的这个文件： ps： 这里我用的是自己的绝对路径，其实这个文件是在janusgraph-dist模块下的一个文件，如下图： 将下面的代码注释删除掉，并删除JANUSGRAPHCFG字段和对应的大括号，下面默认使用的是本地的hbase和es 如果你需要配置远程的hbase和es注意修改对应的ip，修改后如下： 123456789gremlin.graph=org.janusgraph.core.JanusGraphFactorystorage.backend=hbasestorage.hostname=127.0.0.1cache.db-cache = truecache.db-cache-clean-wait = 20cache.db-cache-time = 180000cache.db-cache-size = 0.5index.search.backend=elasticsearchindex.search.hostname=127.0.0.1 2. 添加相应依赖我们在janusgrap-test模块编写自己的单测并运行，因为在janusgraph-test模块中使用的是inmory也就是内存作为存储后端的，没有对应的hbase和es依赖 而我们上述使用的是es 和 hbase所以添加了对应的依赖，根据你使用的在janusgraph-test模块的pom文件下添加对应依赖即可 123456789101112131415161718192021222324252627282930&lt;dependency&gt; &lt;groupId&gt;org.janusgraph&lt;/groupId&gt; &lt;artifactId&gt;janusgraph-hbase&lt;/artifactId&gt; &lt;version&gt;${project.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.janusgraph&lt;/groupId&gt; &lt;artifactId&gt;janusgraph-hbase-core&lt;/artifactId&gt; &lt;version&gt;${project.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.janusgraph&lt;/groupId&gt; &lt;artifactId&gt;janusgraph-hbase-10&lt;/artifactId&gt; &lt;version&gt;${project.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.janusgraph&lt;/groupId&gt; &lt;artifactId&gt;janusgraph-hbase-server&lt;/artifactId&gt; &lt;version&gt;${project.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;2.1.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.janusgraph&lt;/groupId&gt; &lt;artifactId&gt;janusgraph-es&lt;/artifactId&gt; &lt;version&gt;${project.version}&lt;/version&gt;&lt;/dependency&gt; 三：下载janusgraph源码并编译1. 下载janusgraph的源码到本地编译器方式一： 使用git的话，可以直接使用git clone从janusgraph的github仓库来clone一份远程代码到本地 janusgraph 的 github地址： https://github.com/JanusGraph/janusgraph 1git clone https://github.com/JanusGraph/janusgraph 方式二： 也可以直接下载源码的zip包，本地解压之后，用idea打开也可以 然后 git init 初始化为git项目做版本控制，关联到clone到自己的远程仓库即可 注意修改为自己的maven仓库 我的github中janusgraph对应的地址：https://github.com/YYDreamer/janusgraph，会将源码分析过程中相关修改提交到该仓库！ 1.经过上述的步骤，我们在idea中便有了一个一个janusgraph源码的项目，我们首先去除对应的maven插件 将项目最外层的pom.xml中的插件maven-enforcer-plugin部分注释掉，不然会出现Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.0.0-M2:enforce enforce-dependency-convergence) on project janusgraph-test: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. 错误！ 将janusgraph-dist模块中的pom.xml文件下的download-maven-plugin插件注释掉，不然会出现org.apache.http.conn.HttpHostConnectException: Connect to artifacts.elastic.co:443 [artifacts.elastic.co/151.101.110.222] failed: Connectiontimed out: connect错误！ 2.接下来，进行编译，点击idea下面的Terminal，输入命令：编译项目全部模块： 1mvn clean install -DskipTests -Drat.skip=true 或者单独编译一个模块，下面以janusgraph-test模块示例： 1mvn -pl janusgraph-test -am clean install -Dlicense.skip=true -DskipTests -P prod 这里，我们使用全部编译的语句，将项目全部编译！ 3.最后全部success即可！ 过程可能有点慢，如果看到编译过程在下载es等组件的话，可以先将下面讲的环境搭建起来，再用上述语句进行编译，成功图如下： 错误解决： 打开项目后，可能会发现有的pom文件报错，一是看看你的maven有没有正确配置，二是可能是maven的本地仓库索引没有更新，可以参考博客：pom问题解决，超链接直接点击即可 四. 编写单测 并 运行1. 编写单测在janusgraph-test模块下的test目录下，新建一个’mytest’文件夹，编写一个单元测试JanusGraphFirstTest，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class JanusGraphFirstTest { @Test public void firstTest() { // 创建图实例，存储使用hbase，索引使用es // TODO 注意！文件路径修改为自己当前电脑下的绝对路径 JanusGraph graph = JanusGraphFactory .open(&quot;D:\\\\code\\\\janusgraph-0.5.2\\\\janusgraph-dist\\\\src\\\\assembly\\\\cfilter\\\\conf\\\\janusgraph-hbase-es.properties&quot;); // 使用GraphOfTheGodsFactory加载“The Graph of the Gods”图，这是JanusGraph用于测试自定义的一个图 GraphOfTheGodsFactory.load(graph); // 获取图遍历对象实例 GraphTraversalSource g = graph.traversal(); // 获取属性&quot;name&quot;为&quot;saturn&quot;的节点 Vertex saturn = g.V().has(&quot;name&quot;, &quot;saturn&quot;).next(); // 获取上述节点对应的所有属性的kv GraphTraversal&lt;Vertex, Map&lt;Object, Object&gt;&gt; vertexMapGraphTraversal = g.V(saturn).valueMap(); // 输出 List&lt;Map&lt;Object, Object&gt;&gt; saturnProMaps = vertexMapGraphTraversal.toList(); for (Map&lt;Object, Object&gt; proMap : saturnProMaps) { proMap.forEach((key,value) -&gt; System.out.println(key + &quot;:&quot; + value)); } // 获取上述节点的father的father的姓名，也就是grandfather的姓名 GraphTraversal&lt;Vertex, Object&gt; values = g.V(saturn).in(&quot;father&quot;).in(&quot;father&quot;).values(&quot;name&quot;); String name = String.valueOf(values.next()); System.out.println(&quot;grandfather name:&quot; + name); // 获取在(latitude:37.97 and long:23.72)50km内的所有节点 GraphTraversal&lt;Edge, Edge&gt; place = g.E().has(&quot;place&quot;, geoWithin(Geoshape.circle(37.97, 23.72, 50))); // 获取边对应的节点 GraphTraversal&lt;Edge, Map&lt;String, Object&gt;&gt; node = place.as(&quot;source&quot;) .inV().as(&quot;god2&quot;) .select(&quot;source&quot;) .outV().as(&quot;god1&quot;) .select(&quot;god1&quot;, &quot;god2&quot;).by(&quot;name&quot;); // 输出 List&lt;Map&lt;String, Object&gt;&gt; maps = node.toList(); for (Map&lt;String, Object&gt; map : maps) { map.forEach((key,value) -&gt; System.out.println(key + &quot;:&quot; + value)); } }} 注意！！！！！！ 在上述新添加的JanusGraphFirstTest文件中最上面添加下面的注释，不然会因为不符合开源规则编译会报错！ 并且！ 我们在源码分析过程中，添加了任何的文件或者修改了原有的文件，下面的注释都是必须要加在文件最上面并且不可以删除！！ 12345678910111213// Copyright 2020 JanusGraph Authors//// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);// you may not use this file except in compliance with the License.// You may obtain a copy of the License at//// http://www.apache.org/licenses/LICENSE-2.0//// Unless required by applicable law or agreed to in writing, software// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.// See the License for the specific language governing permissions and// limitations under the License. 运行上述单测，前提保证hadoop、hbase、es都在启动状态！ 2. 运行成功后首先，cmd进入hbase的bin目录下，运行hbase shell命令，进入到hbase 的 shell界面： 2.1 输入list命令，如下，可以发现存在一个叫做janusgraph的表，这就是我们刚才运行的单测生成的一个默认table，用来存储对应的图数据 1234567hbase(main):005:0&gt; listTABLEjanusgraph1 row(s)Took 0.0070 seconds=&gt; [&quot;janusgraph&quot;]hbase(main):006:0&gt; 2.2 输入desc 'janusgraph' 命令，如下，查看对应的table schema，我们可以看到表状态为ENABLE可用状态，并且下面跟着好大一坨列簇和列的信息 其中的列簇e\\f\\g\\h\\i\\l\\m\\s\\t都存了对应的图信息，具体存了图的什么信息，我们下面的博文会分享，这里不做描述了 1234567891011hbase(main):012:0* desc 'janusgraph'Table janusgraph is ENABLEDjanusgraphCOLUMN FAMILIES DESCRIPTION{NAME =&gt; 'e', VERSIONS =&gt; '1', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_BEHAVIOR =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', CACHE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', REPLICATION_SCOPE =&gt; '0', BLOOMFILTER =&gt; 'ROW', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOMS_ON_WRITE =&gt; 'false', PREFETCH_BLOCKS_ON_OPEN =&gt; 'false', COMPRESSION =&gt; 'GZ', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536'}# 省略了一部分... 注意： 单测中包含GraphOfTheGodsFactory.load(graph)代码，是将janusgraph自带的一个测试图导入到hbase中，供我们测试使用，首次运行单测成功后，我们要把这个语句GraphOfTheGodsFactory.load(graph)注释掉，不需要重新load这个测试图！ 五. 底层存储使用berkeleyje数据库和es索引后端我们只需要将上述单测代码中的配置文件路径配置文件修改为相同位置下的janusgraph-berkeleyje-es.properties即可，将对应配置的注释去掉！ 然后，将对应的依赖放到janusgraph-test对应的pom文件中： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.janusgraph&lt;/groupId&gt; &lt;artifactId&gt;janusgraph-es&lt;/artifactId&gt; &lt;version&gt;${project.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.janusgraph&lt;/groupId&gt; &lt;artifactId&gt;janusgraph-berkeleyje&lt;/artifactId&gt; &lt;version&gt;${project.version}&lt;/version&gt; &lt;/dependency&gt; 运行即可！ 本文所有的 janusgraph源码项目文件 + hadoop安装包 + hadoop在windows的辅助安装包 + hbase安装包 + es安装包全部整合放到了公众号“匠心Java”中，微信搜索“匠心Java”，回复“图库资源”四个字即可，也可通过微信公众号联系博主，一同探讨!","link":"/article/%E5%9B%BE%E8%A7%A3%E5%9B%BE%E5%BA%93JanusGraph%E7%B3%BB%E5%88%97-%E4%B8%80%E6%96%87%E6%90%9E%E5%AE%9Ajanusgraph%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%9C%AC%E5%9C%B0%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%EF%BC%88janusgraph_source_code_compile%EF%BC%89.html"},{"title":"图解Janusgraph系列-并发安全：Lock锁机制(本地锁+分布式锁)分析","text":"在分布式系统中，难免涉及到对同一数据的并发操作，如何保证分布式系统中数据的并发安全呢？分布式锁！ 大家好，我是洋仔，JanusGraph图解系列文章，实时更新~ 图数据库文章总目录： 整理所有图相关文章，请移步(超链)：图数据库系列-文章总目录 地址：https://liyangyang.blog.csdn.net/article/details/111031257 **源码分析相关可查看github（码文不易，求个star~）**： https://github.com/YYDreamer/janusgraph 下述流程高清大图地址：https://www.processon.com/view/link/5f471b2e7d9c086b9903b629 版本：JanusGraph-0.5.2 转载文章请保留以下声明： 作者：洋仔聊编程微信公众号：匠心Java原文地址：https://liyangyang.blog.csdn.net/ 在分布式系统中，难免涉及到对同一数据的并发操作，如何保证分布式系统中数据的并发安全呢？分布式锁！ 一：分布式锁常用的分布式锁实现方式有三种： 1、基于数据库实现分布式锁 ​ 针对于数据库实现的分布式锁，如mysql使用使用for update共同竞争一个行锁来实现； 在JanusGraph中，也是基于数据库实现的分布式锁，这里的数据库指的是我们当前使用的第三方backend storage，具体的实现方式也和mysql有所不同，具体我们会在下文分析 2、基于Redis实现的分布式锁 ​ 基于lua脚本+setNx实现 3、基于zk实现的分布式锁 ​ 基于znode的有序性和临时节点+zk的watcher机制实现 4、MVCC多版本并发控制乐观锁实现 本文主要介绍Janusgraph的锁机制，其他的实现机制就不在此做详解了 下面我们来分析一下JanusGraph的锁机制实现~ 二：JanusGraph锁机制在JanusGraph中使用的锁机制是：本地锁 + 分布式锁来实现的； 2.1 一致性行为在JanusGraph中主要有三种一致性修饰词(Consistency Modifier)来表示3种不同的一致性行为，来控制图库使用过程中的并发问题的控制程度； 12345public enum ConsistencyModifier { DEFAULT, LOCK, FORK} 源码中ConsistencyModifier枚举类主要作用：用于控制JanusGraph在最终一致或其他非事务性后端系统上的一致性行为！其作用分别为： DEFAULT：默认的一致性行为，不使用分布式锁进行控制，对配置的存储后端使用由封闭事务保证的默认一致性模型，一致性行为主要取决于存储后端的配置以及封闭事务的（可选）配置；无需显示配置即可使用 LOCK：在存储后端支持锁的前提下，显示的获取分布式锁以保证一致性！确切的一致性保证取决于所配置的锁实现；需management.setConsistency(element, ConsistencyModifier.LOCK);语句进行配置 FORK：只适用于multi-edges和list-properties两种情况下使用；使JanusGraph修改数据时，采用先删除后添加新的边/属性的方式，而不是覆盖现有的边/属性，从而避免潜在的并发写入冲突；需management.setConsistency(element, ConsistencyModifier.FORK);进行配置 LOCK 在查询或者插入数据时，是否使用分布式锁进行并发控制，在图shcema的创建过程中，如上述可以通过配置schema元素为ConsistencyModifier.LOCK方式控制并发，则在使用过程中就会用分布式锁进行并发控制； 为了提高效率，JanusGraph默认不使用锁定。 因此，用户必须为定义一致性约束的每个架构元素决定是否使用锁定。 使用JanusGraphManagement.setConsistency（element，ConsistencyModifier.LOCK）显式启用对架构元素的锁定 代码如下所示： 123456mgmt = graph.openManagement() name = mgmt.makePropertyKey('consistentName').dataType(String.class).make() index = mgmt.buildIndex('byConsistentName', Vertex.class).addKey(name).unique().buildCompositeIndex() mgmt.setConsistency(name, ConsistencyModifier.LOCK) // Ensures only one name per vertex mgmt.setConsistency(index, ConsistencyModifier.LOCK) // Ensures name uniqueness in the graph mgmt.commit() FORK由于边缘作为单个记录存储在基础存储后端中，因此同时修改单个边缘将导致冲突。 FORK就是为了代替LOCK，可以将边缘标签配置为使用ConsistencyModifier.FORK。 下面的示例创建一个新的edge label，并将其设置为ConsistencyModifier.FORK 1234mgmt = graph.openManagement() related = mgmt.makeEdgeLabel('related').make() mgmt.setConsistency(related, ConsistencyModifier.FORK) mgmt.commit() 经过上述配置后，修改标签配置为FORK的edge时，操作步骤为： 首先，删除该边 将修改后的边作为新边添加 因此，如果两个并发事务修改了同一边缘，则提交时将存在边缘的两个修改后的副本，可以在查询遍历期间根据需要解决这些副本。 注意edge fork仅适用于MULTI edge。 具有多重性约束的边缘标签不能使用此策略，因为非MULTI的边缘标签定义中内置了一个唯一性约束，该约束需要显式锁定或使用基础存储后端的冲突解决机制 下面我们具体来看一下janusgrph的锁机制的实现： 2.2 LoackID在介绍锁机制之前，先看一下锁应该锁什么东西呢？ 我们都知道在janusgraph的底层存储中，vertexId作为Rowkey，属性和边存储在cell中，由column+value组成 当我们修改节点的属性和边+边的属性时，很明显只要锁住对应的Rowkey + Column即可； 在Janusgraph中，这个锁的标识的基础部分就是LockID： LockID = RowKey + Column 源码如下： 1KeyColumn lockID = new KeyColumn(key, column); 2.3 本地锁本地锁是在任何情况下都需要获取的一个锁，只有获取成功后，才会进行下述分布式锁的获取！ 本地锁是基于图实例维度存在的；主要作用是保证当前图实例下的操作中无冲突！ 本地锁的实现是通过ConcurrentHashMap数据结构来实现的，在图实例维度下唯一； 基于当前事务+lockId来作为锁标识； 获取的主要流程： 结合源码如下： 上述图建议依照源码一块分析，源码在LocalLockMediator类中的下述方法，下面源码分析模块会详细分析 12public boolean lock(KeyColumn kc, T requester, Instant expires) {} 引入本地锁机制，主要目的： 在图实例维度来做一层锁判断，减少分布式锁的并发冲突，减少分布式锁带来的性能消耗 2.4 分布式锁在本地锁获取成功之后才会去尝试获取分布式锁； 分布式锁的获取整体分为两部分流程： 分布式锁信息插入 分布式锁信息状态判断 分布式锁信息插入该部分主要是通过lockID来构造要插入的Rowkey和column并将数据插入到hbase中；插入成功即表示这部分处理成功！ 具体流程如下： 分布式锁信息状态判断该部分在上一部分完成之后才会进行，主要是判断分布式锁是否获取成功！ 查询出当前hbase中对应Rowkey的所有column，过滤未过期的column集合，比对集合的第一个column是否等于当前事务插入的column； 等于则获取成功！不等于则获取失败！ 具体流程如下： 三：源码分析 与 整体流程源码分析已经push到github：https://github.com/YYDreamer/janusgraph 1、获取锁的入口 12345678910111213141516171819202122232425public void acquireLock(StaticBuffer key, StaticBuffer column, StaticBuffer expectedValue, StoreTransaction txh) throws BackendException { // locker是一个一致性key锁对象 if (locker != null) { // 获取当前事务对象 ExpectedValueCheckingTransaction tx = (ExpectedValueCheckingTransaction) txh; // 判断：当前的获取锁操作是否当前事务的操作中存在增删改的操作 if (tx.isMutationStarted()) throw new PermanentLockingException(&quot;Attempted to obtain a lock after mutations had been persisted&quot;); // 使用key+column组装为lockID，供下述加锁使用！！！！！ KeyColumn lockID = new KeyColumn(key, column); log.debug(&quot;Attempting to acquireLock on {} ev={}&quot;, lockID, expectedValue); // 获取本地当前jvm进程中的写锁（看下述的 1：写锁获取分析） // （此处的获取锁只是将对应的KLV存储到Hbase中！存储成功并不代表获取锁成功） // 1. 获取成功（等同于存储成功）则继续执行 // 2. 获取失败（等同于存储失败），会抛出异常，抛出到最上层，打印错误日志“Could not commit transaction [&quot;+transactionId+&quot;] due to exception” 并抛出对应的异常，本次插入数据结束 locker.writeLock(lockID, tx.getConsistentTx()); // 执行前提：上述获取锁成功！ // 存储期望值，此处为了实现当相同的key + value + tx多个加锁时，只处理第一个 // 存储在事务对象中，标识在commit判断锁是否获取成功时，当前事务插入的是哪个锁信息 tx.storeExpectedValue(this, lockID, expectedValue); } else { // locker为空情况下，直接抛出一个运行时异常，终止程序 store.acquireLock(key, column, expectedValue, unwrapTx(txh)); }} 2、执行 locker.writeLock(lockID, tx.getConsistentTx()) 触发锁获取 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public void writeLock(KeyColumn lockID, StoreTransaction tx) throws TemporaryLockingException, PermanentLockingException { if (null != tx.getConfiguration().getGroupName()) { MetricManager.INSTANCE.getCounter(tx.getConfiguration().getGroupName(), M_LOCKS, M_WRITE, M_CALLS).inc(); } // 判断当前事务是否在图实例的维度 已经占据了lockID的锁 // 此处的lockState在一个事务成功获取本地锁+分布式锁后，以事务为key、value为map，其中key为lockID，value为加锁状态（开始时间、过期时间等） if (lockState.has(tx, lockID)) { log.debug(&quot;Transaction {} already wrote lock on {}&quot;, tx, lockID); return; } // 当前事务没有占据lockID对应的锁 // 进行(lockLocally(lockID, tx） 本地加锁锁定操作， if (lockLocally(lockID, tx)) { boolean ok = false; try { // 在本地锁获取成功的前提下： // 尝试获取基于Hbase实现的分布式锁； // 注意！！！（此处的获取锁只是将对应的KLV存储到Hbase中！存储成功并不代表获取锁成功） S stat = writeSingleLock(lockID, tx); // 获取锁分布式锁成功后（即写入成功后），更新本地锁的过期时间为分布式锁的过期时间 lockLocally(lockID, stat.getExpirationTimestamp(), tx); // update local lock expiration time // 将上述获取的锁，存储在标识当前存在锁的集合中Map&lt;tx,Map&lt;lockID,S&gt;&gt;， key为事务、value中的map为当前事务获取的锁，key为lockID，value为当前获取分布式锁的ConsistentKeyStatus（一致性密匙状态）对象 lockState.take(tx, lockID, stat); ok = true; } catch (TemporaryBackendException tse) { // 在获取分布式锁失败后，捕获该异常，并抛出该异常 throw new TemporaryLockingException(tse); } catch (AssertionError ae) { // Concession to ease testing with mocks &amp; behavior verification ok = true; throw ae; } catch (Throwable t) { // 出现底层存储错误！ 则直接加锁失败！ throw new PermanentLockingException(t); } finally { // 判断是否成功获取锁，没有获分布式锁的，则释放本地锁 if (!ok) { // 没有成功获取锁，则释放本地锁 // lockState.release(tx, lockID); // has no effect unlockLocally(lockID, tx); if (null != tx.getConfiguration().getGroupName()) { MetricManager.INSTANCE.getCounter(tx.getConfiguration().getGroupName(), M_LOCKS, M_WRITE, M_EXCEPTIONS).inc(); } } } } else { // 如果获取本地锁失败，则直接抛出异常，不进行重新本地争用 // Fail immediately with no retries on local contention throw new PermanentLockingException(&quot;Local lock contention&quot;); }} 包含两个部分： 本地锁的获取lockLocally(lockID, tx) 分布式锁的获取writeSingleLock(lockID, tx) 注意此处只是将锁信息写入到Hbase中，并不代表获取分布式锁成功，只是做了上述介绍的第一个阶段分布式锁信息插入 3、本地锁获取 lockLocally(lockID, tx) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public boolean lock(KeyColumn kc, T requester, Instant expires) { assert null != kc; assert null != requester; final StackTraceElement[] acquiredAt = log.isTraceEnabled() ? new Throwable(&quot;Lock acquisition by &quot; + requester).getStackTrace() : null; // map的value，以事务为核心 final AuditRecord&lt;T&gt; audit = new AuditRecord&lt;&gt;(requester, expires, acquiredAt); // ConcurrentHashMap实现locks, 以lockID为key，事务为核心value final AuditRecord&lt;T&gt; inMap = locks.putIfAbsent(kc, audit); boolean success = false; // 代表当前map中不存在lockID，标识着锁没有被占用，成功获取锁 if (null == inMap) { // Uncontended lock succeeded if (log.isTraceEnabled()) { log.trace(&quot;New local lock created: {} namespace={} txn={}&quot;, kc, name, requester); } success = true; } else if (inMap.equals(audit)) { // 代表当前存在lockID，比对旧value和新value中的事务对象是否是同一个 // requester has already locked kc; update expiresAt // 上述判断后，事务对象为同一个，标识当前事务已经获取这个lockID的锁； // 1. 这一步进行cas替换，作用是为了刷新过期时间 // 2. 并发处理，如果因为锁过期被其他事务占据，则占用锁失败 success = locks.replace(kc, inMap, audit); if (log.isTraceEnabled()) { if (success) { log.trace(&quot;Updated local lock expiration: {} namespace={} txn={} oldexp={} newexp={}&quot;, kc, name, requester, inMap.expires, audit.expires); } else { log.trace(&quot;Failed to update local lock expiration: {} namespace={} txn={} oldexp={} newexp={}&quot;, kc, name, requester, inMap.expires, audit.expires); } } } else if (0 &gt; inMap.expires.compareTo(times.getTime())) { // 比较过期时间，如果锁已经过期，则当前事务可以占用该锁 // the recorded lock has expired; replace it // 1. 当前事务占用锁 // 2. 并发处理，如果因为锁过期被其他事务占据，则占用锁失败 success = locks.replace(kc, inMap, audit); if (log.isTraceEnabled()) { log.trace(&quot;Discarding expired lock: {} namespace={} txn={} expired={}&quot;, kc, name, inMap.holder, inMap.expires); } } else { // 标识：锁被其他事务占用，并且未过期，则占用锁失败 // we lost to a valid lock if (log.isTraceEnabled()) { log.trace(&quot;Local lock failed: {} namespace={} txn={} (already owned by {})&quot;, kc, name, requester, inMap); log.trace(&quot;Owner stacktrace:\\n {}&quot;, Joiner.on(&quot;\\n &quot;).join(inMap.acquiredAt)); } } return success; } 如上述介绍，本地锁的实现是通过ConcurrentHashMap数据结构来实现的，在图实例维度下唯一！ 4、分布式锁获取第一个阶段：分布式锁信息插入 123456789101112131415161718192021222324252627protected ConsistentKeyLockStatus writeSingleLock(KeyColumn lockID, StoreTransaction txh) throws Throwable { // 组装插入hbase数据的Rowkey final StaticBuffer lockKey = serializer.toLockKey(lockID.getKey(), lockID.getColumn()); StaticBuffer oldLockCol = null; // 进行尝试插入 ，默认尝试次数3次 for (int i = 0; i &lt; lockRetryCount; i++) { // 尝试将数据插入到hbase中；oldLockCol表示要删除的column代表上一次尝试插入的数据 WriteResult wr = tryWriteLockOnce(lockKey, oldLockCol, txh); // 如果插入成功 if (wr.isSuccessful() &amp;&amp; wr.getDuration().compareTo(lockWait) &lt;= 0) { final Instant writeInstant = wr.getWriteTimestamp(); // 写入时间 final Instant expireInstant = writeInstant.plus(lockExpire);// 过期时间 return new ConsistentKeyLockStatus(writeInstant, expireInstant); // 返回插入对象 } // 赋值当前的尝试插入的数据，要在下一次尝试时删除 oldLockCol = wr.getLockCol(); // 判断插入失败原因，临时异常进行尝试，非临时异常停止尝试！ handleMutationFailure(lockID, lockKey, wr, txh); } // 处理在尝试了3次之后还是没插入成功的情况，删除最后一次尝试插入的数据 tryDeleteLockOnce(lockKey, oldLockCol, txh); // TODO log exception or successful too-slow write here // 抛出异常，标识导入数据失败 throw new TemporaryBackendException(&quot;Lock write retry count exceeded&quot;);} 上述只是将锁信息插入，插入成功标识该流程结束 5、分布式锁获取第一个阶段：分布式锁锁定是否成功判定 这一步，是在commit阶段进行的验证 12345public void commit() throws BackendException { // 此方法内调用checkSingleLock 检查分布式锁的获取结果 flushInternal(); tx.commit();} 最终会调用checkSingleLock方法，判断获取锁的状态！ 12345678910111213141516171819202122232425262728293031protected void checkSingleLock(final KeyColumn kc, final ConsistentKeyLockStatus ls, final StoreTransaction tx) throws BackendException, InterruptedException { // 检查是否被检查过 if (ls.isChecked()) return; // Slice the store KeySliceQuery ksq = new KeySliceQuery(serializer.toLockKey(kc.getKey(), kc.getColumn()), LOCK_COL_START, LOCK_COL_END); // 此处从hbase中查询出锁定的行的所有列！ 默认查询重试次数3 List&lt;Entry&gt; claimEntries = getSliceWithRetries(ksq, tx); // 从每个返回条目的列中提取timestamp和rid，然后过滤出带有过期时间戳的timestamp对象 final Iterable&lt;TimestampRid&gt; iterable = Iterables.transform(claimEntries, e -&gt; serializer.fromLockColumn(e.getColumnAs(StaticBuffer.STATIC_FACTORY), times)); final List&lt;TimestampRid&gt; unexpiredTRs = new ArrayList&lt;&gt;(Iterables.size(iterable)); for (TimestampRid tr : iterable) { // 过滤获取未过期的锁！ final Instant cutoffTime = now.minus(lockExpire); if (tr.getTimestamp().isBefore(cutoffTime)) { ... } // 将还未过期的锁记录存储到一个集合中 unexpiredTRs.add(tr); } // 判断当前tx是否成功持有锁！ 如果我们插入的列是读取的第一个列，或者前面的列只包含我们自己的rid（因为我们是在第一部分的前提下获取的锁，第一部分我们成功获取了基于当前进程的锁，所以如果rid相同，代表着我们也成功获取到了当前的分布式锁），那么我们持有锁。否则，另一个进程持有该锁，我们无法获得锁 // 如果，获取锁失败，抛出TemporaryLockingException异常！！！！ 抛出到顶层的mutator.commitStorage()处，最终导入失败进行事务回滚等操作 checkSeniority(kc, ls, unexpiredTRs); // 如果上述步骤未抛出异常，则标识当前的tx已经成功获取锁！ ls.setChecked();} 四：整体流程总流程如下图： 整体流程为： 获取本地锁 获取分布式锁 插入分布式锁信息 commit阶段判断分布式锁获取是否成功 获取失败，则重试 五：总结JanusGraph的锁机制主要是通过本地锁+分布式锁来实现分布式系统下的数据一致性； 分布式锁的控制维度为：property、vertex、edge、index都可以； JanusGraph支持在数据导入时通过前面一致性行为部分所说的LOCK来开关分布式锁： LOCK：数据导入时开启分布式锁保证分布式一致性 DEFAULT、FORK：数据导入时关闭分布式锁 是否开启分布式锁思考： 在开启分布式锁的情况下，数据导入开销非常大；如果是数据不是要求很高的一致性，并且数据量比较大，我们可以选择关闭分布式锁相关，来提高导入速度； 然后，针对于小数据量的要求高一致性的数据，单独开启分布式锁来保证数据安全； 另外，我们在不开启分布式锁定的情况下，可以通过针对于导入的数据的充分探查来减少冲突！ 针对于图schema的元素开启还是关闭分布式锁，还是根据实际业务情况来决定。 本文有任何问题，可加博主微信或评论指出，感谢！ 码文不易，给个赞和star吧~","link":"/article/%E5%9B%BE%E8%A7%A3Janusgraph%E7%B3%BB%E5%88%97-%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%EF%BC%9ALock%E9%94%81%E6%9C%BA%E5%88%B6(%E6%9C%AC%E5%9C%B0%E9%94%81+%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81)%E5%88%86%E6%9E%90.html"},{"title":"什么是静态代理和动态代理？","text":"学一个技术，要知道技术因何而产生，才能有学下去的目标和动力，才能更好的理解 首先，要明确为什么要存在代理呢？ 存在一个常见的需求：怎样在不修改类A代码的情况下，在调用类A的方法时进行一些功能的附加与增强呢？ 先不考虑什么代理不代理的，我们设计一个简单的实现方案： 新创建一个类B，类B组合类A，在类B中创建一个方法b，方法b中调用类A中的方法a，在调用前和调用后都可以添加一些自定义的附加与增强代码。 当有需求需要调用类A的方法a并且想要添加一个附加功能时，就去调用类B的方法b即可实现上述需求； 下面为了便于理解，附上伪代码： 12345678910111213141516171819202122// 定义类Apublic class ClassA{ public void methoda(){ System.out.println(&quot;我是方法a!&quot;); }}// 定义类Bpublic class ClassB{ // 组合ClassA ClassA A; public ClassB(ClassA A){ this.A = A; } @Override public void methodb(){ System.out.println(&quot;我是方法a的附加功能代码，我执行啦~!&quot;); A.methoda(); System.out.println(&quot;我是方法a的附加功能代码，我完成啦~!&quot;); }} 下面，让我们来调用一下ClassB的methodb方法，则会产生以下输出： 123我是方法a的附加功能代码，我执行啦~!我是方法a!我是方法a的附加功能代码，我完成啦~! 可以发现，方法a执行了，并且在没有修改类A代码的前提下，为方法a附加了其他的功能；不难吧，其实上述的代码就是一个最简单的代理模式了 代理存在的意义：使用代理模式可以在不修改别代理对象代码的基础上，通过扩展代理类，进行一些功能的附加与增强 代理种类代理分为静态代理和动态代理，其涉及的设计模式就是代理模式本尊了，代理模式一般包含几种元素，如下图： 主题接口(subject)：定义代理类和真实主题的公共对外方法，也是代理类代理真实主题的方法； 真实主题(RealSubject)：真正实现业务逻辑的类； 代理类(Proxy)：用来代理和封装真实主题； 客户端(Client)：使用代理类和主题接口完成一些工作。 为了更好的理解，我们将上述实现的最简易版的代理完善一下，添加接口，代理类也实现相应的被代理类的接口，实现同一个方法，伪代码如下： 123456789101112131415161718192021222324252627282930313233343536// 被代理类的接口（上图中subject）public interface ImpA{ void methoda();}// 定义类A（上图中RealSubject）public class ClassA implements ImpA{ public void methoda(){ System.out.println(&quot;我是方法a!&quot;); }}// 定义类B（上图中Proxy）public class ClassB implements ImpA { // 组合ClassA ImpA A; public ClassB(ClassA A){ this.A = A; } // 重写被代理类的方法 @Override public void methoda(){ System.out.println(&quot;我是方法a的附加功能代码，我执行啦~!&quot;); A.methoda(); System.out.println(&quot;我是方法a的附加功能代码，我完成啦~!&quot;); }}// 客户端类（上图中Client）public class Main{ // 创建被代理对象实例 ImpA A = new ClassA(); // 构造器注入被代理对象实例 ImpA B = new ClassB(A); // 调用代理方法 B.methoda();} 静态代理所谓静态代理也就是在程序运行前就已经存在代理类的字节码文件，代理类和委托类的关系在运行前就确定了。上面的代码就是实现了一个静态代理； 其实静态代理就已经能够满足上述需求了，为什么还需要动态代理呢？ 这里就涉及到静态代理的两个缺点了 代理对象的一个接口只服务于一种类型的对象，如果要代理的方法很多，势必要为每一种方法都进行代理，在程序规模稍大时静态代理代理类就会过多会造成代码混乱 如果接口增加一个方法，除了所有实现类需要实现这个方法外，所有代理类也需要实现此方法，增加了代码维护的复杂度。基于上述两个问题，动态代理诞生了~ 动态代理动态代理是在程序运行时，通过反射获取被代理类的字节码内容用来创建代理类 具体什么是动态代理呢？名词：动态，动态在程序中就是表达在程序运行时就根据配置自动的生成代理类并且代理类和被代理类是在运行时才确定相互之间的关系； 在JDK中包含两种动态代理的实现机制：JDK Proxy 和 CGLib； 下面我们以JDK Proxy为例，讲解一下动态代理和根据源码分析并简单说一下应用场景 JDK ProxyJDK Proxy动态代理，api在包java.lang.reflect下，大家可能发现了，为什么在反射的包下呢？这个问题我们下面的源码分析会解决； 其核心api包含两个重要的核心接口和类：一个是 InvocationHandler(Interface)、另一个则是 Proxy(Class)，简单说就这两个简单的很，这两个是我们实现动态代理所必需的用到的，下面简单介绍一下两个类：java.lang.reflect.Proxy（Class） ：Proxy是 Java 动态代理机制的主类，提供一组静态方法来为一组接口动态地生成代理类及其对象。包含以下四个静态方法： static InvocationHandler getInvocationHandler(Object proxy)该方法用于获取指定代理对象所关联的调用处理器 static Class getProxyClass(ClassLoader loader, Class[] interfaces)该方法用于获取关联于指定类装载器和一组接口的动态代理类的类对象 static boolean isProxyClass(Class cl)该方法用于判断指定类对象是否是一个动态代理类 static Object newProxyInstance(ClassLoader loader, Class[] interfaces,InvocationHandler h)该方法用于为指定类装载器、一组接口及调用处理器生成动态代理类实例，包含下面的参数： loader 指定代理类的ClassLoader加载器 interfaces 指定代理类要实现的所有接口 h: 表示的是当这个动态代理对象在调用方法的时候，会关联到哪一个InvocationHandler对象上 该方法用于为指定类装载器、一组接口及调用处理器生成动态代理类实例 java.lang.reflect.InvocationHandler（interface） ： InvocationHandler是上述newProxyInstance方法的InvocationHandler h参数传入，负责连接代理类和委托类的中间类必须实现的接口它自定义了一个 invoke 方法，用于集中处理在动态代理类对象上的方法调用，通常在该方法中实现对委托类的代理访问。 上述就是动态代理两个核心的方法，不太明白？先别急，我们先用上述实现一个动态代理，你先看一下 还是以上述的案例从静态代理来改造为动态代理，实现动态代理主要就两步，假设还是存在上述的ImplA、ClassA 1：创建一个处理器类实现InvocationHandler接口，重写invoke方法，伪代码如下： 1234567891011121314151617public class MyHandler implements InvocationHandler{ // 标识被代理类的实例对象 private Object delegate; // 构造器注入被代理对象 public MyHandler(Object delegate){ this.delegate = delegate; } // 重写invoke方法 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(&quot;被代理方法调用前的附加代码执行~ &quot;); // 真实的被代理方法调用 method.invoke(delegate, args); System.out.println(&quot;被代理方法调用后的附加代码执行~ &quot;); } } 好了，这样一个处理器就搞定了，当我们在调用被代理类的方法时，就是去执行上述重写的invoke方法，下面创建一个ClassA的代理类 2：创建代理类，并调用被代理方法 123456789101112public class MainTest{ public static void main(String[] args) { // 创建被代理对象 ImplA A = new ClassA(); // 创建处理器类实现 InvocationHandler myHandler = new MyHandler(A); // 重点！ 生成代理类， 其中proxyA就是A的代理类了 ImplA proxyA = (ImplA)Proxy.newProxyInstance(A.getClass().getClassLoader(), A.getClass().getInterfaces(), myHandler); // 调用代理类的代理的methoda方法， 在此处就会去调用上述myHandler的invoke方法区执行，至于为什么，先留着疑问，下面会说清楚~ proxyA.methoda(); }} 好了，至此一个动态代理就构建完成了，执行代码，会发现输出： 123被代理方法调用前的附加代码执行~我是方法a！被代理方法调用后的附加代码执行~ 太简单了有木有，这里总结一下动态代理的优缺点： 优点： 动态代理类不仅简化了编程工作，而且提高了软件系统的可扩展性，因为Java 反射机制可以生成任意类型的动态代理类。 动态代理类的字节码在程序运行时由Java反射机制动态生成，无需程序员手工编写它的源代码。 接口增加一个方法，除了所有实现类需要实现这个方法外，动态代理类会直接自动生成对应的代理方法。 缺点：JDK proxy只能对有实现接口的类才能代理，也就是说没有接口实现的类，jdk proxy是无法代理的，为什么呢？下面会解答. 有什么解决方案吗？ 当然有，还有一种动态代理的方案：CGLib，它是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法，并覆盖其中方法实现增强，但是因为采用的是继承，所以该类或方法最好不要声明成final，对于final类或方法，是无法继承的，和jdk proxy基本思想是相似的，毕竟都是动态代理的实现方案嘛，在这篇文章就不做详解了，博主会在其他的博文单独介绍这个nb的框架 上述带大家搞了一遍动态代理和静态代理的应用；在这过程中，你有没有想过，动态代理是怎么实现的呢？ 下面我们就从源码的角度分析一下，解决大家的疑问。 源码分析在开始分析的时候，我希望大家带着几个问题去阅读，可以帮助大家更好的理解： 问题1：代理类为什么可以在运行的时候自动生成呢？如何生成的呢？ 问题2：为什么调用代理类的相应的代理方法就可以调用到InvocationHandler实现类的invoke方法呢？ 问题3：为什么jdk proxy只支持代理有接口实现的类呢？ ps ：为了提升阅读体验，让大家有一个更清晰的认知，以下源码会将一些异常处理和日志打印代码删除，只保留主干代码，请知悉~ 我们就从两个核心：InvocationHandler和Proxy来进行分析整个脉络，他们都在java.lang.reflect包下 InvocationHandler源码1234public interface InvocationHandler { public Object invoke(Object proxy, Method method, Object[] args) throws Throwable;} 上述就是InvocationHandler的源码，没什么其他的就是一个接口，里面有一个待实现方法invoke，处理类实现此接口重写invoke方法 Proxy源码12345678910111213141516171819202122232425262728public class Proxy implements java.io.Serializable { // 处理类实例 变量 protected InvocationHandler h; // 用于存储 已经通过动态代理获取过的代理类缓存 private static final WeakCache&lt;ClassLoader, Class&lt;?&gt;[], Class&lt;?&gt;&gt; proxyClassCache = new WeakCache&lt;&gt;(new KeyFactory(),new ProxyClassFactory()); // 私有无参构造，使得只能通过传入InvocationHandler参数来创建该对象 private Proxy() {} // 保护 构造函数，入参InvocationHandler处理类 protected Proxy(InvocationHandler h) { Objects.requireNonNull(h); this.h = h; } public static Class&lt;?&gt; getProxyClass(ClassLoader loader,Class&lt;?&gt;... interfaces) throws IllegalArgumentException{ ... } public static boolean isProxyClass(Class&lt;?&gt; cl) { ... } public static InvocationHandler getInvocationHandler(Object proxy) throws IllegalArgumentException { ... } // 生成代理类的实现方法 public static Object newProxyInstance(ClassLoader loader,Class&lt;?&gt;[] interfaces,InvocationHandler h) throws IllegalArgumentException{ ... } // 各种私有方法 private ... ...} Proxy类的整体的架构就类似于上述，InvocationHandler h参数和两个构造函数、四个上述已经介绍过的共有方法，还有一系列的私有方法，getProxyClass、isProxyClass、getInvocationHandler功能就和上面介绍的一样，就不再详细介绍了 我们下面来主要看一下newProxyInstance方法newProxyInstance方法，我在方法内添加上了对应的注释： 1234567891011121314151617181920212223242526272829303132public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException{ // 1. 克隆对应的接口，用于代理类实现的接口数组 final Class&lt;?&gt;[] intfs = interfaces.clone(); ... /* * Look up or generate the designated proxy class. 源码中的介绍 * 2. 查找或者生成指定的代理类， 下面会详细介绍 */ Class&lt;?&gt; cl = getProxyClass0(loader, intfs); /* * Invoke its constructor with the designated invocation handler. * 3. 上面代码已经生成了代理类 cl，cl其中包含一个参数为传入的InvocationHandler h的构造函数， 获取该构造函数并通过该构造函数创建一个类的实例对象并返回 */ try { // 4. 通过《反射》获取参数为InvocationHandler的构造函数 final Constructor&lt;?&gt; cons = cl.getConstructor(constructorParams); final InvocationHandler ih = h; // 5. 判断构造函数是否为私有的，如果为私有的则需要设置私有可访问权限 if (!Modifier.isPublic(cl.getModifiers())) { AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() { public Void run() { cons.setAccessible(true); return null; } }); } // 6. 通过上述获取的构造函数创建对应的 实例对象，并返回！over~ return cons.newInstance(new Object[]{h}); } catch (IllegalAccessException|InstantiationException e) { // 各种异常处理 }} 在上面的代码中，我简单的标注了一下每行代码的作用，下面我们来详细分析一下； 代理类的字节码生成逻辑 我们知道，在加载jvm前，java文件都已经被编译成了class字节码文件， 然后jvm通过类加载器将字节码文件加载到jvm中； 我们的代理类也是这样，不同的是动态代理的类是在程序运行时产生的，我们要做的就是如何在程序运行的时候，通过被代理类的字节码生成代理类的字节码！ 我们接下来详细分析newProxyInstance方法： 在newProxyInstance中调用了Class&lt;?&gt; cl = getProxyClass0(loader, intfs);语句生成了代理类的字节码，此处调用了getProxyClass0方法，传入了指定的类加载器和对应要实现的接口 那么， 我们看看getProxyClass0方法的实现： 1234567private static Class&lt;?&gt; getProxyClass0(ClassLoader loader, Class&lt;?&gt;... interfaces) { if (interfaces.length &gt; 65535) { throw new IllegalArgumentException(&quot;interface limit exceeded&quot;); } // proxyClassCache是WeakCache弱引用缓存类，如果之前就生成过对应的代理类就从缓存中取，如果没生成过就重新生成 return proxyClassCache.get(loader, interfaces);} proxyClassCache是Proxy类中的静态变量，是WeakCache类，里面封装了两个类KeyFactory、ProxyClassFactory，都是BiFunction函数式接口(如果不清楚函数式接口，请自行google)； 将其拿过来看一下proxyClassCache = new WeakCache&lt;&gt;(new KeyFactory(),new ProxyClassFactory()); 其中调用了proxyClassCache.get(loader, interfaces)方法的实现 未避免代码过长，只粘贴了核心代码： 12345678910111213141516171819202122232425public V get(K key, P parameter) { ... // 这部分主要是获取对应的 函数式接口，如果不明白函数式接口，google一下吧~ Object subKey = Objects.requireNonNull(subKeyFactory.apply(key, parameter)); Supplier&lt;V&gt; supplier = valuesMap.get(subKey); Factory factory = null; while (true) { // 此处为什么是while循环呢， 主要是supplier不为空的话，则执行下面的语句赋值后，再循环执行下一次则supplier不为空 if (supplier != null) { // 如果存在对应的函数式接口， 调用函数式接口对应的代码 // 重点！！！调用函数式接口！！ V value = supplier.get(); if (value != null) { return value; } } if (factory == null) { // 创建一个 专门创建代理类字节码的工厂类，实现类是ProxyClassFactory factory = new Factory(key, parameter, subKey, valuesMap); } if (supplier == null) { supplier = valuesMap.putIfAbsent(subKey, factory); if (supplier == null) { // 将supplier赋值factory supplier = factory; }}}} } 总结一下上述方法的流程： 1234graph LRA[缓存中获取代理类字节码] --&gt; C{是否存在}C --&gt;|是| D[代理中获取并返回]C --&gt;|否| E[调用ProxyClassFactory的apply方法生成代理类字节码] 接着ProxyClassFactory.apply方法看一下： 12345678910111213141516171819202122232425262728293031323334353637383940414243public Class&lt;?&gt; apply(ClassLoader loader, Class&lt;?&gt;[] interfaces) { Map&lt;Class&lt;?&gt;, Boolean&gt; interfaceSet = new IdentityHashMap&lt;&gt;(interfaces.length); // 获取接口对应的接口class对象 for (Class&lt;?&gt; intf : interfaces) { Class&lt;?&gt; interfaceClass = null; try { interfaceClass = Class.forName(intf.getName(), false, loader); } if (interfaceSet.put(interfaceClass, Boolean.TRUE) != null) { } } String proxyPkg = null; int accessFlags = Modifier.PUBLIC | Modifier.FINAL; // 判断是否包含公有的接口对象，判断是否可以通过jdk proxy的方式进行生成代理类 for (Class&lt;?&gt; intf : interfaces) { int flags = intf.getModifiers(); if (!Modifier.isPublic(flags)) { accessFlags = Modifier.FINAL; String name = intf.getName(); int n = name.lastIndexOf('.'); String pkg = ((n == -1) ? &quot;&quot; : name.substring(0, n + 1)); if (proxyPkg == null) { proxyPkg = pkg; } else if (!pkg.equals(proxyPkg)) { } } } // 如果没有公有接口类，需要使用CGLib来实现。。。 if (proxyPkg == null) { proxyPkg = ReflectUtil.PROXY_PACKAGE + &quot;.&quot;; } // 组装代理类的类名称 long num = nextUniqueNumber.getAndIncrement(); String proxyName = proxyPkg + proxyClassNamePrefix + num; // 重点！！ 此处生成代理类的字节码数组 byte[] proxyClassFile = ProxyGenerator.generateProxyClassproxyName, interfaces, accessFlags); try { // 通过类加载器将字节码数组加载到JVm的方法区中生成Class对象！ return defineClass0(loader, proxyName, proxyClassFile, 0, proxyClassFile.length); } catch (ClassFormatError e) { } } 上述的 byte[] proxyClassFile = ProxyGenerator.generateProxyClass(proxyName, interfaces, accessFlags); 为生成代理类字节码数组的方法，调用的方法中调用了generateClassFile方法； 12345678910111213141516171819202122232425262728293031323334private byte[] generateClassFile() { // 首先，默认代理的三个方法：hashCode\\equals\\toString this.addProxyMethod(hashCodeMethod, Object.class); this.addProxyMethod(equalsMethod, Object.class); this.addProxyMethod(toStringMethod, Object.class); // 获取所有的要被代理类实现的接口 Class[] var1 = this.interfaces; int var2 = var1.length; int var3; Class var4; // 遍历上述获取的接口 for(var3 = 0; var3 &lt; var2; ++var3) { // 赋值： 将接口的Class对象赋值！ var4 = var1[var3]; // 通过“反射”获取所有方法 Method[] var5 = var4.getMethods(); int var6 = var5.length; for(int var7 = 0; var7 &lt; var6; ++var7) { Method var8 = var5[var7]; // 将方法添加到 要被代理的方法中 this.addProxyMethod(var8, var4); } } // 获取要代理方法后，开始组装字节码 var14.writeInt(-889275714); var14.writeShort(0); var14.writeShort(this.accessFlags); var14.writeShort(this.cp.getClass(dotToSlash(this.className))); // 注意！！！ .... 此处省略了绝大部分 字节码的组装过程，只给出了几行代码展示一下字节码的组装 // 最终返回组装好的字节码文件 return var13.toByteArray(); } } 在generateClassFile中，你会发现里面全部是重组字节码的代码， 主要是获取被代理类字节码和操作类InvocationHandler字节码组装出代理类的字节码，在重组的过程因为是在运行时进行了代理类的创建，无法像往常一样new一个被代理类的实例获取他的方法，让代理类进行调用。 获取字节码后，接下来就要将代理类的字节码加载进JVM中了，这里调用的是一个return defineClass0(loader, proxyName, proxyClassFile, 0, proxyClassFile.length) 其中的defineClass0是一个本地native 方法，传入了代理类名称、类加载器、代理类的字节码文件、文件长度参数，从而将字节码加载进JVM中！ 代码如下： 12private static native Class&lt;?&gt; defineClass0(ClassLoader loader, String name, byte[] b, int off, int len); 将代理类的字节码加载进JVM后，会在方法区内生成一个Class对象，标识这个代理类； 代理类的实例生成逻辑上面，我们知道了通过字节码技术生成了代理类字节码，并通过类加载器将字节码文件加载到了JVM的方法区中生成了一个Class对象，我们如何在运行时获取这个Class对象的实例呢？ 只有获取了对象实例才可以使用不是~还是回到newProxyInstance方法中，上面我们分析了Class&lt;?&gt; cl = getProxyClass0(loader, intfs)这部分逻辑，生成了Class对象cl，下面生辰该实例代码，过程很简单，相关逻辑我就直接在代码中注释了 12345678910111213141516// 定义构造函数的参数类型，下面的一个语句使用private static final Class&lt;?&gt;[] constructorParams = { InvocationHandler.class };// 通过反射获取上述获取的Class对象的带参构造函数，参数必须是上述定义的 InvocationHandler.class类型final Constructor&lt;?&gt; cons = cl.getConstructor(constructorParams);// 检查权限，如果是私有权限，设为可被访问if (!Modifier.isPublic(cl.getModifiers())) { AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() { public Void run() { cons.setAccessible(true); return null; } }); } // 通过构造函数传入对应 处理类h 参数，生成实例！return cons.newInstance(new Object[]{h}); 上述就是生成实例的代码，生成实例后newProxyInstance就返回该实例了，就可以使用了~ 反射：在运行时获取被代理类的字节码那如何才能在运行时获取到被代理类的构造函数、方法、属性等字节码呢？ 此时“反射！”登场了！我们通过反射可以在运行时获取到类的所有信息，所有哦。定义： JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意方法和属性；这种动态获取信息以及动态调用对象方法的功能称为java语言的反射机制。 比如，在上述所说的组装代理类字节码时，在获取被代理类的所有方法时，就调用了Method[] var5 = var4.getMethods(); 反射中的getMethods方法，通过反射获取到了被代理类的所有方法，这样我们就可以在运行时获取到任何类的所有的字节码信息了! 从而可以组装出我们想要的代理类字节码！ 所以说，反射也为动态代理的实现提供了理论支持！！因为只有在运行时能获取到对应类的信息，才可以通过信息创造出对应的我们所需要的代理类； 源码分析总结总而言之，动态代理的理论支持是可以通过反射机制在运行时获取到类的所有信息，如果运行时获取不到被代理类的信息，那还咋生成代理类。动态代理的大致流程： 12345678graph TDA[反射获取被代理字节码] --&gt; B[反射获取InvocationHandler实现类字节码]B --&gt;C[依据被代理类字节码和InvocationHandler实现类字节码 通过操作字节码组装 代理类 字节码]C --&gt;D[通过给定的classLoad将 代理类字节码 加载到JVM中]D --&gt;E[调用native方法 加载到JVM的方法区 生成Class对象]E --&gt;F[反射获取代理类的Class对象的构造函数]F --&gt;G[通过反射获取的构造函数new一个代理类的实例A]G --&gt;H[使用代理类实例A] 通过上述流程。我们就获得了一个代理类对象了，调用代理类对应的方法，就会执行我们规定的执行逻辑，实现对被代理类的运行时动态增强和扩展！ 此时，我们再拿出刚开始我们用JDK proxy实现的动态代理代码中的生成代理类的代码：ImplA proxyA = (ImplA)Proxy.newProxyInstance(A.getClass().getClassLoader(), A.getClass().getInterfaces(), myHandler) 每个参数的作用，是不是就很清晰了 问题解答上面动态代理实现流程，我们可以回答上述的第一个代理类为什么可以在运行的时候自动生成呢？如何生成的呢？ 问题了 对于第二个为什么调用代理类的相应的代理方法就可以调用到InvocationHandler实现类的invoke方法呢？和第三个为什么jdk proxy只支持代理有接口实现的类呢？问题，我们需要反编译一下我们通过字节码技术产生的代理类，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344final class $Proxy0 extends Proxy implements ImplA { private static Method m1; private static Method m3; private static Method m2; private static Method m0; static { try { m1 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;equals&quot;, Class.forName(&quot;java.lang.Object&quot;)); m3 = Class.forName(&quot;com.test.ImplA&quot;).getMethod(&quot;methoda&quot;); m2 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;toString&quot;); m0 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;hashCode&quot;); } catch (NoSuchMethodException var2) { // .. } } public $Proxy0(InvocationHandler var1) throws { super(var1); } // 需要被加强的方法methoda public final void methoda() throws { try { super.h.invoke(this, m3, (Object[])null); } catch (RuntimeException | Error var2) { throw var2; } catch (Throwable var3) { throw new UndeclaredThrowableException(var3); } } public final boolean equals(Object var1) throws { // 省略部分代码。。。 return (Boolean)super.h.invoke(this, m1, new Object[]{var1}); } public final String toString() throws { // 省略部分代码。。。 return (String)super.h.invoke(this, m2, (Object[])null); } public final int hashCode() throws { // 省略部分代码。。。 return (Integer)super.h.invoke(this, m0, (Object[])null); }} 上述代码包含几个关键点： 方法为final类型，不可再被继承 代理名称为 $Proxy 代理类前缀 + 递增数字 继承动态代理的核心类Proxy， 实现了我们指定的接口ImplA 一个带参构造方法$Proxy0(InvocationHandler var1) 传入InvocationHandler内部调用了父类的Proxy的构造函数 methoda、toString、hashCode、equals全部调用的传入的InvocationHandler参数的 invoke方法！！！ 现在回答第二个问题为什么调用代理类的相应的代理方法就可以调用到InvocationHandler实现类的invoke方法呢？ 显而易见，代理类内部的代理方法全部显式调用的InvocationHandler实现类的invoke方法 第三个问题为什么jdk proxy只支持代理有接口实现的类呢？ 因为代理类在使用JDK proxy方式生成代理类时，默认继承Proxy类，又因为java语言是单继承不支持多继承，那怎样才能标识我要代理什么类型的类或是代理什么方法呢？ 接口呗，java支持接口的多继承，多少个都ok~ 好了，上述将动态代理的使用方式 和 实现原理统一过了一遍，也回答了几个容易疑惑的问题，下面我们简单说下动态代理在现实的java框架大家庭中的一些典型应用 动态代理的应用spring aop ： 这可以说是spring框架中最典型的应用了，通过动态代理在运行时产生代理类，完成对被代理类的增强和功能附加RPC框架的实现 ： 远程过程调用，RPC使得调用远程方法和调用本地方法一样，这是怎么搞的呢？服务方对外放出服务的接口api，调用方拿到接口api，通过动态代理的方式生成一个代理类，代理类的处理类的invoke方法可以通过websocket连接远程服务器调用对应的远程接口； 这样我们再用代理对象进行调用对应方法时时，就像调用本地方法一样了mybatis框架中 ： mapper.xml中编写sql语句，mapper.java接口写上对应的方法签名；我们直接调用mapper.java中的方法就可以执行对应的sql语句，有没有想过为什么？ 框架使用动态代理创建一个mapper.java的代理对象，代理对象的处理类invoke中执行sql，就ok了 总结代理分为静态代理和动态代理，动态代理的两种实现方式：JDK Proxy和CGLib，动态代理的核心反射机制，通过反射在运行时获取被代理类字节码和处理类字节码，动态代理代理类的生成通过重组字节码的方式。 原创不易，有收获的话，关注、点赞、评论三连支持，我最大的动力~ 关于博文有任何问题请不吝评论，感谢 参考：JDK源码，https://www.jianshu.com/p/861223789d53","link":"/article/%E4%BB%80%E4%B9%88%E6%98%AF%E9%9D%99%E6%80%81%E4%BB%A3%E7%90%86%E5%92%8C%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%EF%BC%9F.html"},{"title":"图解Janusgraph系列-分布式id生成策略分析","text":"本文介绍JanusGraph的分布式id生成逻辑； 大家好，我是洋仔，JanusGraph图解系列文章，实时更新~ 图数据库文章总目录： 整理所有图相关文章，请移步(超链)：图数据库系列-文章总目录 地址：https://liyangyang.blog.csdn.net/article/details/111031257 **源码分析相关可查看github（求star~~）**： https://github.com/YYDreamer/janusgraph 下述流程高清大图地址：https://www.processon.com/view/link/5f471b2e7d9c086b9903b629 版本：JanusGraph-0.5.2 转载文章请保留以下声明： 作者：洋仔聊编程微信公众号：匠心Java原文地址：https://liyangyang.blog.csdn.net/ 正文在介绍JanusGraph的分布式ID生成策略之前，我们来简单分析一下分布式ID应该满足哪些特征？ 全局唯一：必须保证ID是分布式环境中全局性唯一的，这是基本要求 高性能：高可用低延时，ID生成响应快；否则可能会成为业务瓶颈 高可用：提供分布式id的生成的服务要保证高可用，不能随随便便就挂掉了，会对业务产生影响 趋势递增：主要看业务场景，类似于图存储中节点的唯一id就尽量保持趋势递增；但是如果类似于电商订单就尽量不要趋势递增，因为趋势递增会被恶意估算出当天的订单量和成交量，泄漏公司信息 接入方便：如果是中间件，要秉着拿来即用的设计原则，在系统设计和实现上要尽可能的简单 一：常用分布式id生成策略当前常用的分布式id的生成策略主要分为以下四种： UUID 数据库+号段模式（优化：数据库+号段+双buffer） 基于Redis实现 雪花算法（SnowFlake） 还有一些其他的比如：基于数据库自增id、数据库多主模式等，这些在小并发的情况下可以使用，大并发的情况下就不太ok了 市面上有一些生成分布式id的开源组件，包括滴滴基于数据库+号段实现的TinyID 、百度基于SnowFlake的Uidgenerator、美团支持号段和SnowFlake的Leaf等 那么，在JanusGraph中分布式id的生成是采用的什么方式呢？ 二：JanusGraph的分布式id策略在JanusGraph中，分布式id的生成采用的是数据库+号段+双buffer优化的模式； 下面我们来具体分析一下： 分布式id生成使用的数据库就是JanusGraph当前使用的第三方存储后端，这里我们以使用的存储后端Hbase为例； JanusGraph分布式id生成所需元数据存储位置： 在Hbase中有column family 列族的概念； JanusGraph在初始化Hbase表时默认创建了9大列族，用于存储不同的数据， 具体看《图解图库JanusGraph系列-一文知晓图数据底层存储结构》； 其中有一个列族janusgraph_ids简写为i这个列族，主要存储的就是JanusGraph分布式id生成所需要的元数据！ JanusGraph的分布式id的组成结构： 1234// 源码中有一句话体现 /* --- JanusGraphElement id bit format --- * [ 0 | count | partition | ID padding (if any) ] */ 主要分为4部分：0、count、partition、ID padding（每个类型是固定值）； 其实这4部分的顺序在序列化为二进制数据时，顺序会有所改变；这里只是标明了id的组成部分！ 上述部分的partition + count来保证分布式节点的唯一性； partition id：分区id值，JanusGraph默认分了32个逻辑分区；节点分到哪个分区采用的是随机分配; count：每个partition都有对应的一个count范围：0-2的55次幂；JanusGraph每次拉取一部分的范围作为节点的count取值；JanusGraph保证了针对相同的partition，不会重复获取同一个count值！ 保证count在partition维度保持全局唯一性，就保证了生成的最终id的全局唯一性！！ 则分布式id的唯一性保证，就在于count基于partition维度的唯一性！下面我们的分析也是着重在count的获取！ JanusGraph分布式id生成的主要逻辑流程如下图所示：（推荐结合源码分析观看！） 分析过程中有一个概念为id block：指当前获取的号段范围 JanusGraph主要使用``PartitionIDPool 类来存储不同类型的StandardIDPool； 在StandardIDPool`中主要包含两个id Block： current block：当前生成id使用的block next block：double buffer中的另一个已经准备好的block 为什么要有两个block呢？ 主要是如果只有一个block的话，当我们在使用完当前的block时，需要阻塞等待区获取下一个block，这样便会导致分布式id生成较长时间的阻塞等待block的获取； 怎么优化上述问题呢？ double buffer； 除了当前使用的block，我们再存储一个next block；当正在使用的block假设已经使用了50%，触发next block的异步获取，如上图的蓝色部分所示； 这样当current block使用完成后可以直接无延迟的切换到next block如上图中绿色部分所示； 在执行过程中可能会因为一些异常导致节点id获取失败，则会进行重试；重试次数默认为1000次； 1234private static final int MAX_PARTITION_RENEW_ATTEMPTS = 1000;for (int attempt = 0; attempt &lt; MAX_PARTITION_RENEW_ATTEMPTS; attempt++) { // 获取id的过程} ps：上述所说的IDPool和block是基于当前图实例维度共用的！ 三：源码分析在JanusGraph的源码中，主要包含两大部分和其他的一些组件： Graph相关类：用于对节点、属性、边的操作 Transaction相关类：用于在对数据或者Schema进行CURD时，进行事务处理 其他一些：分布式节点id生成类；序列化类；第三方索引操作类等等 Graph和Transaction相关类的类图如下所示： 分布式id涉及到id生成的类图如下所示： 初始数据： 12345678910111213141516@Testpublic void addVertexTest(){ List&lt;Object&gt; godProperties = new ArrayList&lt;&gt;(); godProperties.add(T.label); godProperties.add(&quot;god&quot;); godProperties.add(&quot;name&quot;); godProperties.add(&quot;lyy&quot;); godProperties.add(&quot;age&quot;); godProperties.add(18); JanusGraphVertex godVertex = graph.addVertex(godProperties.toArray()); assertNotNull(godVertex);} 在诸神之图中添加一个name为lyy节点；看下执行流程，注意，此处主要分析的节点的分布式id生成代码！ 1、调用JanusGraphBlueprintsGraph类的AddVertex方法 12345@Overridepublic JanusGraphVertex addVertex(Object... keyValues) { // 添加节点 return getAutoStartTx().addVertex(keyValues);} 2、调用JanusGraphBlueprintsTransaction的addVertex方法 1234567public JanusGraphVertex addVertex(Object... keyValues) { // 。。。省略了其他的处理 // 该处生成节点对象，包含节点的唯一id生成逻辑 final JanusGraphVertex vertex = addVertex(id, label); // 。。。省略了其他的处理 return vertex; } 3、调用StandardJanusGraphTx的addVertex方法 1234567891011@Overridepublic JanusGraphVertex addVertex(Long vertexId, VertexLabel label) { // 。。。省略了其他的处理 if (vertexId != null) { vertex.setId(vertexId); } else if (config.hasAssignIDsImmediately() || label.isPartitioned()) { graph.assignID(vertex,label); // 为节点分配正式的节点id！ } // 。。。省略了其他的处理 return vertex;} 4、调用VertexIDAssigner的assignID(InternalElement element, IDManager.VertexIDType vertexIDType)方法 123456789101112131415161718192021private void assignID(InternalElement element, IDManager.VertexIDType vertexIDType) { // 开始获取节点分布式唯一id // 因为一些异常导致获取节点id失败，进行重试，重试此为默认为1000次 for (int attempt = 0; attempt &lt; MAX_PARTITION_RENEW_ATTEMPTS; attempt++) { // 初始化一个partiiton id long partitionID = -1; // 获取一个partition id // 不同类型的数据，partition id的获取方式也有所不同 if (element instanceof JanusGraphSchemaVertex) { // 为partition id赋值 } try { // 正式分配节点id， 依据partition id 和 节点类型 assignID(element, partitionID, vertexIDType); } catch (IDPoolExhaustedException e) { continue; //try again on a different partition } assert element.hasId(); // 。。。省略了其他代码 }} 5、调用了VertexIDAssigner的assignID(final InternalElement element, final long partitionIDl, final IDManager.VertexIDType userVertexIDType)方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475private void assignID(final InternalElement element, final long partitionIDl, final IDManager.VertexIDType userVertexIDType) { final int partitionID = (int) partitionIDl; // count为分布式id组成中的一部分，占55个字节 // 分布式id的唯一性保证，就在于`count`基于`partition`维度的唯一性 long count; if (element instanceof JanusGraphSchemaVertex) { // schema节点处理 Preconditions.checkArgument(partitionID==IDManager.SCHEMA_PARTITION); count = schemaIdPool.nextID(); } else if (userVertexIDType==IDManager.VertexIDType.PartitionedVertex) { // 配置的热点节点，类似于`makeVertexLabel('product').partition()`的处理 count = partitionVertexIdPool.nextID(); } else { // 普通节点和边类型的处理 // 首先获取当前partition敌营的idPool PartitionIDPool partitionPool = idPools.get(partitionID); // 如果当前分区对应的IDPool为空，则创建一个默认的IDPool，默认size = 0 if (partitionPool == null) { // 在PartitionIDPool中包含多种类型对应的StandardIDPool类型 // StandardIDPool中包含对应的block信息和count信息 partitionPool = new PartitionIDPool(partitionID, idAuthority, idManager, renewTimeoutMS, renewBufferPercentage); // 缓存下来 idPools.putIfAbsent(partitionID,partitionPool); // 从缓存中再重新拿出 partitionPool = idPools.get(partitionID); } // 确保partitionPool不为空 Preconditions.checkNotNull(partitionPool); // 判断当前分区的IDPool是否枯竭；已经被用完 if (partitionPool.isExhausted()) { // 如果被用完，则将该分区id放到对应的缓存中，避免之后获取分区id再获取到该分区id placementStrategy.exhaustedPartition(partitionID); // 抛出IDPool异常， 最外层捕获，然后进行重试获取节点id throw new IDPoolExhaustedException(&quot;Exhausted id pool for partition: &quot; + partitionID); } // 存储当前类型对应的IDPool，因为partitionPool中保存好几个类型的IDPool IDPool idPool; if (element instanceof JanusGraphRelation) { idPool = partitionPool.getPool(PoolType.RELATION); } else { Preconditions.checkArgument(userVertexIDType!=null); idPool = partitionPool.getPool(PoolType.getPoolTypeFor(userVertexIDType)); } try { // 重要！！！！ 依据给定的IDPool获取count值！！！！ // 在此语句中设计 block的初始化 和 double buffer block的处理！ count = idPool.nextID(); partitionPool.accessed(); } catch (IDPoolExhaustedException e) { // 如果该IDPool被用完，抛出IDPool异常， 最外层捕获，然后进行重试获取节点id log.debug(&quot;Pool exhausted for partition id {}&quot;, partitionID); placementStrategy.exhaustedPartition(partitionID); partitionPool.exhaustedIdPool(); throw e; } } // 组装最终的分布式id：[count + partition id + ID padding] long elementId; if (element instanceof InternalRelation) { elementId = idManager.getRelationID(count, partitionID); } else if (element instanceof PropertyKey) { elementId = IDManager.getSchemaId(IDManager.VertexIDType.UserPropertyKey,count); } else if (element instanceof EdgeLabel) { elementId = IDManager.getSchemaId(IDManager.VertexIDType.UserEdgeLabel, count); } else if (element instanceof VertexLabel) { elementId = IDManager.getSchemaId(IDManager.VertexIDType.VertexLabel, count); } else if (element instanceof JanusGraphSchemaVertex) { elementId = IDManager.getSchemaId(IDManager.VertexIDType.GenericSchemaType,count); } else { elementId = idManager.getVertexID(count, partitionID, userVertexIDType); } Preconditions.checkArgument(elementId &gt;= 0); // 对节点对象赋值其分布式唯一id element.setId(elementId);} 上述代码，我们拿到了对应的IdPool，有两种情况： 第一次获取分布式id时，分区对应的IDPool初始化为默认的size = 0的IDPool 分区对应的IDPool不是初次获取 这两种情况的处理，都在代码count = idPool.nextID()的StandardIDPool类中的nextID()方法中被处理！ 在分析该代码之前，我们需要知道 PartitionIDPool 和StandardIDPool的关系： 每个partition都有一个对应的PartitionIDPool extends EnumMap&lt;PoolType,IDPool&gt; 是一个枚举map类型； 每一个PartitionIDPool 都有对应的不同类型的StandardIDPool： NORMAL_VERTEX：用于vertex id的分配 UNMODIFIABLE_VERTEX：用于schema label id的分配 RELATION：用于edge id的分配 在StandardIDPool中包含多个字段，分别代表不同的含义，抽取几个重要的字段进行介绍： 12345678910111213141516private static final int RENEW_ID_COUNT = 100; private final long idUpperBound; // Block的最大值，默认为2的55次幂private final int partition; // 当前pool对应的分区private final int idNamespace; // 标识pool为那种类型的pool，上述的三种类型NORMAL_VERTEX、UNMODIFIABLE_VERTEX、RELATION；值为当前枚举值在枚举中的位置private final Duration renewTimeout;// 重新获取block的超时时间private final double renewBufferPercentage;// 双buffer中，当第一个buffer block使用的百分比，到达配置的百分比则触发other buffer block的获取private IDBlock currentBlock; // 当前的blockprivate long currentIndex; // 标识当前block使用到那一个位置private long renewBlockIndex; // 依据currentBlock.numIds()*renewBufferPercentage来获取这个值，主要用于在当前的block在消费到某个index的时候触发获取下一个buffer blockprivate volatile IDBlock nextBlock;// 双buffer中的另外一个blockprivate final ThreadPoolExecutor exec;// 异步获取双buffer的线程池 6、调用了StandardIDPool类中的nextID方法 经过上述分析，我们知道，分布式唯一id的唯一性是由在partition维度下的count的值的唯一性来保证的； 上述代码通过调用IDPool的nextId来获取count值； 下述代码就是获取count的逻辑； 123456789101112131415161718192021222324252627282930313233@Overridepublic synchronized long nextID() { // currentIndex标识当前的index小于current block的最大值 assert currentIndex &lt;= currentBlock.numIds(); // 此处涉及两种情况： // 1、分区对应的IDPool是第一次被初始化；则currentIndex = 0； currentBlock.numIds() = 0； // 2、分区对应的该IDPool不是第一次，但是此次的index正好使用到了current block的最后一个count if (currentIndex == currentBlock.numIds()) { try { // 将current block赋值为next block // next block置空 并计算renewBlockIndex nextBlock(); } catch (InterruptedException e) { throw new JanusGraphException(&quot;Could not renew id block due to interruption&quot;, e); } } // 在使用current block的过程中，当current index == renewBlockIndex时，触发double buffer next block的异步获取！！！！ if (currentIndex == renewBlockIndex) { // 异步获取next block startIDBlockGetter(); } // 生成最终的count long returnId = currentBlock.getId(currentIndex); // current index + 1 currentIndex++; if (returnId &gt;= idUpperBound) throw new IDPoolExhaustedException(&quot;Reached id upper bound of &quot; + idUpperBound); log.trace(&quot;partition({})-namespace({}) Returned id: {}&quot;, partition, idNamespace, returnId); // 返回最终获取的分区维度的全局唯一count return returnId;} 上述代码中进行了两次判断： currentIndex == currentBlock.numIds()： 第一次生成分布式id：此处判断即为 0==0；然后生成新的block 非第一次生成分布式id：等于情况下标识当前的block已经使用完了，需要切换为next block currentIndex == renewBlockIndex renew index：标识index使用多少后开始获取下一个double buffer 的next block；有一个默认值100，主要为了兼容第一次分布式id的生成；相等则会触发异步获取下一个next block 下面我们分别对nextBlock();逻辑和startIDBlockGetter();进行分析； 7、调用了StandardIDPool类中的nextBlock方法 12345678910111213141516171819202122232425private synchronized void nextBlock() throws InterruptedException { // 在分区对应的IDPool第一次使用时，double buffer的nextBlock为空 if (null == nextBlock &amp;&amp; null == idBlockFuture) { // 异步启动 获取id block startIDBlockGetter(); } // 也是在分区对应的IDPool第一次使用时，因为上述为异步获取，所以在执行到这一步时nextBlock可能还没拿到 // 所以需要阻塞等待block的获取 if (null == nextBlock) { waitForIDBlockGetter(); } // 将当前使用block指向next block currentBlock = nextBlock; // index清零 currentIndex = 0; // nextBlock置空 nextBlock = null; // renewBlockIndex用于双buffer中，当第一个buffer block使用的百分比，到达配置的百分比则触发other buffer block的获取 // 值current block 对应的count数量 - （值current block 对应的count数量 * 为renewBufferPercentage配置的剩余空间百分比） // 在使用current block的时候，当current index == renewBlockIndex时，触发double buffer next block的异步获取！！！！ renewBlockIndex = Math.max(0,currentBlock.numIds()-Math.max(RENEW_ID_COUNT, Math.round(currentBlock.numIds()*renewBufferPercentage)));} 主要是做了三件事： 1、block是否为空，为空的话则异步获取一个block 2、nextBlock不为空的情况下：next赋值到current、next置空、index置零 3、计算获取下一个nextBlock的触发index renewBlockIndex值 8、调用了StandardIDPool类中的startIDBlockGetter方法 12345678910private synchronized void startIDBlockGetter() { Preconditions.checkArgument(idBlockFuture == null, idBlockFuture); if (closed) return; //Don't renew anymore if closed //Renew buffer log.debug(&quot;Starting id block renewal thread upon {}&quot;, currentIndex); // 创建一个线程对象，包含给定的权限控制类、分区、命名空间、超时时间 idBlockGetter = new IDBlockGetter(idAuthority, partition, idNamespace, renewTimeout); // 提交获取double buffer的线程任务，异步执行 idBlockFuture = exec.submit(idBlockGetter);} 其中创建一个线程任务，提交到线程池exec进行异步执行； 下面看下，线程类的call方法主要是调用了 idAuthority.getIDBlock方法，这个方法主要是基于Hbase来获取还未使用的block； 12345678910111213141516/** * 获取double buffer block的线程类 */private static class IDBlockGetter implements Callable&lt;IDBlock&gt; { // 省略部分代码 @Override public IDBlock call() { Stopwatch running = Stopwatch.createStarted(); try { // 此处调用idAuthority 调用HBase进行占用获取Block IDBlock idBlock = idAuthority.getIDBlock(partition, idNamespace, renewTimeout); return idBlock; } catch (BackendException e) {} }} 9、调用ConsistentKeyIDAuthority类的getIDBlock方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495@Overridepublic synchronized IDBlock getIDBlock(final int partition, final int idNamespace, Duration timeout) throws BackendException { // 开始时间 final Timer methodTime = times.getTimer().start(); // 获取当前命名空间配置的blockSize，默认值10000；可自定义配置 final long blockSize = getBlockSize(idNamespace); // 获取当前命名空间配置的最大id值idUpperBound；值为：2的55次幂大小 final long idUpperBound = getIdUpperBound(idNamespace); // uniqueIdBitWidth标识uniqueId占用的位数；uniqueId为了兼容“关闭分布式id唯一性保障”的开关情况，uniqueIdBitWidth默认值=4 // 值：64-1(默认0)-5（分区占用位数）-3（ID Padding占用位数）-4（uniqueIdBitWidth） = 51；标识block中的上限为2的51次幂大小 final int maxAvailableBits = (VariableLong.unsignedBitLength(idUpperBound)-1)-uniqueIdBitWidth; // 标识block中的上限为2的51次幂大小 final long idBlockUpperBound = (1L &lt;&lt;maxAvailableBits); // UniquePID用尽的UniquePID集合，默认情况下，randomUniqueIDLimit = 0； final List&lt;Integer&gt; exhaustedUniquePIDs = new ArrayList&lt;&gt;(randomUniqueIDLimit); // 默认0.3秒 用于处理TemporaryBackendException异常情况（后端存储出现问题）下：阻塞一断时间，然后进行重试 Duration backoffMS = idApplicationWaitMS; // 从开始获取IDBlock开始，持续超时时间（默认2分钟）内重试获取IDBlock while (methodTime.elapsed().compareTo(timeout) &lt; 0) { final int uniquePID = getUniquePartitionID(); // 获取uniquePID，默认情况下“开启分布式id唯一性控制”，值 = 0； 当“关闭分布式id唯一性控制”时为一个随机值 final StaticBuffer partitionKey = getPartitionKey(partition,idNamespace,uniquePID); // 依据partition + idNamespace + uniquePID组装一个RowKey try { long nextStart = getCurrentID(partitionKey); // 从Hbase中获取当前partition对应的IDPool中被分配的最大值，用来作为当前申请新的block的开始值 if (idBlockUpperBound - blockSize &lt;= nextStart) { // 确保还未被分配的id池中的id个数，大于等于blockSize // 相应处理 } long nextEnd = nextStart + blockSize; // 获取当前想要获取block的最大值 StaticBuffer target = null; // attempt to write our claim on the next id block boolean success = false; try { Timer writeTimer = times.getTimer().start(); // ===开始：开始进行插入自身的block需求到Hbase target = getBlockApplication(nextEnd, writeTimer.getStartTime()); // 组装对应的Column: -nextEnd + 当前时间戳 + uid（唯一标识当前图实例） final StaticBuffer finalTarget = target; // copy for the inner class BackendOperation.execute(txh -&gt; { // 异步插入当前生成的RowKey 和 Column idStore.mutate(partitionKey, Collections.singletonList(StaticArrayEntry.of(finalTarget)), KeyColumnValueStore.NO_DELETIONS, txh); return true; },this,times); writeTimer.stop(); // ===结束：插入完成 final boolean distributed = manager.getFeatures().isDistributed(); Duration writeElapsed = writeTimer.elapsed(); // ===获取方才插入的时间耗时 if (idApplicationWaitMS.compareTo(writeElapsed) &lt; 0 &amp;&amp; distributed) { // 判断是否超过配置的超时时间，超过则报错TemporaryBackendException，然后等待一断时间进行重试 throw new TemporaryBackendException(&quot;Wrote claim for id block [&quot; + nextStart + &quot;, &quot; + nextEnd + &quot;) in &quot; + (writeElapsed) + &quot; =&gt; too slow, threshold is: &quot; + idApplicationWaitMS); } else { assert 0 != target.length(); final StaticBuffer[] slice = getBlockSlice(nextEnd); // 组装下述基于上述Rowkey的Column的查找范围：(-nextEnd + 0 : 0nextEnd + 最大值) final List&lt;Entry&gt; blocks = BackendOperation.execute( // 异步获取指定Rowkey和指定Column区间的值 (BackendOperation.Transactional&lt;List&lt;Entry&gt;&gt;) txh -&gt; idStore.getSlice(new KeySliceQuery(partitionKey, slice[0], slice[1]), txh),this,times); if (blocks == null) throw new TemporaryBackendException(&quot;Could not read from storage&quot;); if (blocks.isEmpty()) throw new PermanentBackendException(&quot;It seems there is a race-condition in the block application. &quot; + &quot;If you have multiple JanusGraph instances running on one physical machine, ensure that they have unique machine idAuthorities&quot;); if (target.equals(blocks.get(0).getColumnAs(StaticBuffer.STATIC_FACTORY))) { // 如果获取的集合中，当前的图实例插入的数据是第一条，则表示获取block; 如果不是第一条，则获取Block失败 // 组装IDBlock对象 ConsistentKeyIDBlock idBlock = new ConsistentKeyIDBlock(nextStart,blockSize,uniqueIdBitWidth,uniquePID); if (log.isDebugEnabled()) { idBlock, partition, idNamespace, uid); } success = true; return idBlock; // 返回 } else { } } } finally { if (!success &amp;&amp; null != target) { // 在获取Block失败后，删除当前的插入； 如果没有失败，则保留当前的插入，在hbase中标识该Block已经被占用 //Delete claim to not pollute id space for (int attempt = 0; attempt &lt; ROLLBACK_ATTEMPTS; attempt++) { // 回滚：删除当前插入，尝试次数5次 } } } } catch (UniqueIDExhaustedException e) { // No need to increment the backoff wait time or to sleep log.warn(e.getMessage()); } catch (TemporaryBackendException e) { backoffMS = Durations.min(backoffMS.multipliedBy(2), idApplicationWaitMS.multipliedBy(32)); sleepAndConvertInterrupts(backoffMS); \\ } } throw new TemporaryLockingException();} 主要的逻辑就是： 组装Rowkey：partition + idNameSpace+unquePId 组装Column：-nextEnd+now time+uid 将RowKey+Column插入Hbase 获取的上述组装的RowKey 基于(-nextEnd + 0 : -nextEnd + max)范围的所有Column集合 判断集合的第一个Column是不是当前插入的Column，是的话则占用block成功，不是的话则占用失败，删除刚才占用并进行重试 最终：异步获取到了唯一占用的Block，然后生成对应的唯一count，组装最后的唯一id 整体的调用流程如下： 四：其他类型的id生成上述我们主要依据生成节点id（vertex id）的过程来进行分析 在JanusGraph中还包含edge id、property id、schema label id等几种的分布式id生成 所有类型的分布式id的生成主要思想和逻辑都几乎相同，只是一些具体的逻辑可能有所不同，我们理解了vertex id的分布式id生成流程，其他的也可以理解了。 1、property id的生成在JanusGraph中的property的分布式唯一id的生成，整体逻辑和vertex id的生成逻辑大体相同； property id的 生成和 vertex id有两点不同： ID的组成部分： 在vertex id中组成部分包含count+partition+ID Padding； 而在property id中没有ID Padding部分，其组成为count + partition 123long id = (count&lt;&lt;partitionBits)+partition;if (type!=null) id = type.addPadding(id); // 此时，type = nullreturn id; partition id的获取方式：在生成vertex id时，partition id是随机获取的；而在生成property id时，partition id是获取的当前节点对应的partition id，如果节点获取不到分区id，则随机生成一个； 12345678910111213if (element instanceof InternalRelation) { // 属性 + 边 InternalRelation relation = (InternalRelation)element; if (attempt &lt; relation.getLen()) { InternalVertex incident = relation.getVertex(attempt); Preconditions.checkArgument(incident.hasId()); if (!IDManager.VertexIDType.PartitionedVertex.is(incident.longId()) || relation.isProperty()) { // 获取对应节点已有的partition id partitionID = getPartitionID(incident); } else { continue; } } else { // 如果对应的节点都没有，则随机获取一个partition id partitionID = placementStrategy.getPartition(element); } 2、Edge id的生成在JanusGraph中的edge的分布式唯一id的生成，整体逻辑和vertex id的生成逻辑大体相同； edge id的 生成和 vertex id有两点不同： ID的组成部分： 在vertex id中组成部分包含count+partition+ID Padding； 而在edge id中没有ID Padding部分，其组成为count + partition，代码同property id的生成代码 partition id的获取方式：在生成vertex id时，partition id是随机获取的；而在生成edge id时，partition id是获取的当前source vertex 或者 target vertex对应的partition id，如果节点获取不到分区id，则随机生成一个，代码同property id的生成代码； 3、Schema相关id的生成在JanusGraph中的schema相关id的分布式唯一id的生成，整体逻辑和vertex id的生成逻辑大体相同； schema相关id的生成分为四种：PropertyKey、EdgeLabel、VertexLabel、JanusGraphSchemaVertex ID的组成部分： 在vertex id中组成部分包含count+partition+ID Padding； 在schema对应的id生成，这四种产生的id对应的结构都是一样的：count + 对应类型的固定后缀 1return (count &lt;&lt; offset()) | suffix(); partition id的获取方式：在生成vertex id时，partition id是随机获取的；而在生成schema id时，partition id是默认的partition id = 0； 1234public static final int SCHEMA_PARTITION = 0;if (element instanceof JanusGraphSchemaVertex) { partitionID = IDManager.SCHEMA_PARTITION; // 默认分区} 总结 本文总结了JanusGraph的分布式唯一id的生成逻辑，也进行的源码分析； 下一篇，JanusGraph的锁机制分析，包含本地锁和分布式锁相关的分析，我是“洋仔”，我们下期见~","link":"/article/%E5%9B%BE%E8%A7%A3Janusgraph%E7%B3%BB%E5%88%97-%E5%88%86%E5%B8%83%E5%BC%8Fid%E7%94%9F%E6%88%90%E7%AD%96%E7%95%A5%E5%88%86%E6%9E%90.html"}],"tags":[{"name":"MySql","slug":"MySql","link":"/tags/MySql/"},{"name":"Spring系列","slug":"Spring系列","link":"/tags/Spring%E7%B3%BB%E5%88%97/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"中间件","slug":"中间件","link":"/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"分布式","slug":"分布式","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"JanusGraph","slug":"JanusGraph","link":"/tags/JanusGraph/"},{"name":"内存泄漏","slug":"内存泄漏","link":"/tags/%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"操作系统","slug":"操作系统","link":"/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"线程池","slug":"线程池","link":"/tags/%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"name":"二进制","slug":"二进制","link":"/tags/%E4%BA%8C%E8%BF%9B%E5%88%B6/"},{"name":"idea","slug":"idea","link":"/tags/idea/"},{"name":"maven","slug":"maven","link":"/tags/maven/"},{"name":"高并发","slug":"高并发","link":"/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/"},{"name":"代理模式","slug":"代理模式","link":"/tags/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/"},{"name":"动静态代理","slug":"动静态代理","link":"/tags/%E5%8A%A8%E9%9D%99%E6%80%81%E4%BB%A3%E7%90%86/"}],"categories":[{"name":"MySql","slug":"MySql","link":"/categories/MySql/"},{"name":"Spring系列","slug":"Spring系列","link":"/categories/Spring%E7%B3%BB%E5%88%97/"},{"name":"开发工具","slug":"开发工具","link":"/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"},{"name":"中间件","slug":"中间件","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"分布式","slug":"分布式","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"图数据库","slug":"图数据库","link":"/categories/%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"问题解决","slug":"问题解决","link":"/categories/%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/"},{"name":"操作系统","slug":"操作系统","link":"/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"算法","slug":"算法","link":"/categories/%E7%AE%97%E6%B3%95/"},{"name":"Java基础","slug":"Java基础","link":"/categories/Java%E5%9F%BA%E7%A1%80/"},{"name":"高并发","slug":"高并发","link":"/categories/%E9%AB%98%E5%B9%B6%E5%8F%91/"}]}